# Sora 2 Crushes TikTok, Claude 4.5 Fizzles, DeepSeek innovates attention and GLM 4.6 Takes the Crown! üî•

**Date:** October 03, 2025  
**Duration:** 1:39:59  
**Link:** [https://sub.thursdai.news/p/thursdai-oct-2-sora-2-the-new-tiktok](https://sub.thursdai.news/p/thursdai-oct-2-sora-2-the-new-tiktok)

---

## Description

Hey everyone, Alex here (yes the real me if you‚Äôre reading this) 

The weeks are getting crazier, but what OpenAI pulled this week, with a whole new social media app attached to their latest AI breakthroughs is definitely breathtaking! Sora2 released and instantly became a viral sensation, shooting to the top 3 free iOS spot on AppStore, with millions of videos watched, and remixed. 

On weeks like these, even huge releases like Claude 4.5 are taking the backseat, but we still covered them! 

For listeners of the pod, the second half of the show was very visual heavy, so it may be worth it watching the YT video attached in a comment if you want to fully experience the Sora revolution with us! (and if you want a SORA invite but don‚Äôt have one yet, more on that below) 

ThursdAI - if you find this valuable, please support us by subscribing! 

Sora 2 - the AI video model that signifies a new era of social media

Look, you‚Äôve probably already heard about the SORA-2 release, but in case you haven‚Äôt, OpenAI released a whole new model, but attached it to a new, AI powered social media experiment in the form of a very addictive TikTok style feed. Besides being hyper-realistic, and producing sounds and true to source voice-overs, Sora2 asks you to create your own ‚ÄúCameo‚Äù by taking a quick video, and then allows you to be featured in your own (and your friends) videos. 

This makes a significant break from the previously ‚Äúslop‚Äù based meta Vibes, becuase, well, everyone loves seeing themselves as the stars of the show! 

Cameos are a stroke of genius, and what‚Äôs more, one can allow everyone to use their Cameo, which is what Sam Altman did at launch, making everyone Cameo him, and turning him, almost instantly into one of the most meme-able (and approachable) people on the planet! 

Sam sharing away his likeness like this for the sake of the app achieved a few things, it added trust in the safety features, made it instantly viral and showed folks they shouldn‚Äôt be afraid of adding their own likeness. 

Vibes based feed and remixing

Sora 2 is also unique in that, it‚Äôs the first social media with UGC (user generated content) where content can ONLY be generated, and all SORA content is created within the app. It‚Äôs not possible to upload pictures that have people to create the posts, and you can only create posts with other folks if you have access to their Cameos, or by Remixing existing creations. 

Remixing is also a way to let users ‚Äúparticipate‚Äù in the creation process, by adding their own twist and vibes! 

Speaking of Vibes, while the SORA app has an algorithmic For You page, they have a completely novel and new way to interact with the algorithm, by using their Pick a Mood feature, where you can describe which type of content you want to see, or not see, with natural language! 

I believe that this feature will come to all social media platforms later, as it‚Äôs such a game changer. Want only content in a specific language? or content that doesn‚Äôt have Sam Altman in it? Just ask! 

Content that makes you feel good

The most interesting thing is about the type of content is, there‚Äôs no sexualisation (because all content is moderated by OpenAI strong filters), and no gore etc. OpenAI has clearly been thinking about teenagers and have added parent controls, things like being able to turn of the For You page completely etc to the mix. 

Additionally, SORA seems to be a very funny model, and I mean this literally. You can ask the video generation for a joke and you‚Äôll often get a funny one. The scene setup, the dialogue, the things it does even unprompted are genuinely entertaining. 

AI + Product = Profit? 

OpenAI shows that they are one of the worlds best product labs in the world, not just a foundational AI lab. Most AI advancements are tied to products, and in this case, the whole experience is so polished, it‚Äôs hard to accept that it‚Äôs a brand new app from a company that didn‚Äôt do social before. There‚Äôs very little buggy behavior, videos are loaded up quick, there‚Äôs even DMs! I‚Äôm thoroughly impressed and am immersing myself in the SORA sphere. Please give me a follow there and feel free to use my Cameo by tagging [@altryne](https://sora.chatgpt.com/profile/altryne) in there. I love seeing how folks have used my Cameo, it makes me laugh üòÇ 

The copyright question is.. wild

Remember last year when I asked Sam why Advanced Voice Mode couldn‚Äôt sing Happy Birthday? He said they didn‚Äôt have classifiers to detect IP violations. Well, apparently that‚Äôs not a concern anymore because SORA 2 will happily generate perfect South Park episodes, Rick and Morty scenes, and Pokemon battles. They‚Äôre not even pretending they didn‚Äôt train on this stuff. You can even generate videos with any dead famous person (I‚Äôve had zoom meetings with Michael Jackson and 2Pac, JFK and Mister Rogers) 

Our friend Ryan Carson already used it to create a YouTube short ad for his startup in two minutes. What would have cost $100K and three months now takes six generations and you‚Äôre done. This is the real game-changer for businesses.

Getting invited

EDIT: If you‚Äôre reading this on Friday, try the code `FRIYAY` and let me know in comments if it worked for you üôè

I wish I would have invites for all of you, but all invited users have 4 other folks they can invite, so we shared a bunch of invites during the live show, and asked folks to come back and invite other listeners, this went on for half an hour so I bet we‚Äôve got quite a few of you in! If you‚Äôre still looking for an invite, you can visit the [thread on X](thursdai.news/sora) and see who claimed and invite and ask them for one, tell them you‚Äôre also a ThursdAI listener, they hopefully will return the favor! 

Alternatively, OpenAI employees often post codes with a huge invite ratio, so follow [@GabrielPeterss4](https://x.com/GabrielPeterss4) who often posts codes and you can get in there fairly quick, and if you‚Äôre not in the US, I heard a VPN works well. Just don‚Äôt forget to follow me on there as well üòâ

A Week with OpenAI Pulse: The Real Agentic Future is Here

Listen to me, this may be a hot take. I think OpenAI Pulse is a bigger news story than Sora. I‚Äôve told you about Pulse last week, but today on the show I was able to share my weeks worth of experience, and honestly, it‚Äôs now the first thing I look at when I wake up in the morning after brushing my teeth! 

While Sora is changing media, Pulse is changing how we interact with AI on a fundamental level. Released to Pro subscribers for now, Pulse is an agentic, personalized feed that works for you behind the scenes. Every morning, it delivers a briefing based on your interests, your past conversations, your calendar‚Äîeverything. It‚Äôs the first asynchronous AI agent I‚Äôve used that feels truly proactive.

You don‚Äôt have to trigger it. It just works. It knew I had a flight to Atlanta and gave me tips. I told it I was interested in Halloween ideas for my kids, and now it‚Äôs feeding me suggestions. Most impressively, this week it surfaced a new open-source video model, Kandinsky 5.0, that I hadn‚Äôt seen anywhere on X or my usual news feeds. An agent found something new and relevant for my show, without me even asking.

This is it. This is the life-changing-level of helpfulness we‚Äôve all been waiting for from AI. Personalized, proactive agents are the future, and Pulse is the first taste of it that feels real. I cannot wait for my next Pulse every morning.

**This Week‚Äôs Buzz: The AI Build-Out is NOT a Bubble**

This show is powered by Weights & Biases from CoreWeave, and this week that‚Äôs more relevant than ever. I just got back from a company-wide offsite where we got a glimpse into the future of AI infrastructure, and folks, the scale is mind-boggling.

CoreWeave, our parent company, is one of the key players providing the GPU infrastructure that powers companies like OpenAI and Meta. And the commitments being made are astronomical. In the past few months, CoreWeave has locked in a **$22.4B deal with OpenAI**, a **$14.2B pact with Meta**, and a **$6.3B ‚Äúbackstop‚Äù guarantee with NVIDIA** that runs through 2032.

If you hear anyone talking about an ‚ÄúAI bubble,‚Äù show them these numbers. These are multi-year, multi-billion dollar commitments to build the foundational compute layer for the next decade of AI. The demand is real, and it‚Äôs accelerating. And the best part? As a Weights & Biases user, you have access to this same best-in-class infrastructure that runs OpenAI through our inference services. Try [wandb.me/inference](wandb.me/inference)**, **and let me know if you need a bit of a credit boost! 

Claude Sonnet 4.5: The New Coding King Has a Few Quirks

On any other week, Anthropic‚Äôs release of **Claude Sonnet 4.5 **would‚Äôve been the headline news. They‚Äôre positioning it as the new best model for coding and complex agents, and the benchmarks are seriously impressive. It matches or beats their previous top-tier model, Opus 4.1, on many difficult evals, all while keeping the same affordable price as the previous Sonnet.

One of the most significant jumps is on the OS World benchmark, which tests an agent‚Äôs ability to use a computer‚Äîopening files, manipulating windows, and interacting with applications. Sonnet 4.5 scored a whopping 61.4%, a massive leap from Opus 4.1‚Äôs 44%. This clearly signals that Anthropic is doubling down on building agents that can act as real digital assistants.

However, the real-world experience has been a bit of a mixed bag. My co-host Ryan Carson, whose company Amp switched over to 4.5 right away, noted some regressions and strange errors, saying they‚Äôre even considering switching back to the previous version until the rough edges are smoothed out. Nisten also found it could be more susceptible to ‚Äúslop catalysts‚Äù in prompting. It seems that while it‚Äôs incredibly powerful, it might require some re-prompting and adjustments to get the best, most stable results. The jury‚Äôs still out, but it‚Äôs a potent new tool in the developer‚Äôs arsenal.

Open Source LLMs: DeepSeek‚Äôs Attention Revolution

Despite the massive news from the big companies, open source still brought the heat this week, with one release in particular representing a fundamental breakthrough.

DeepSeek released **V3.2 Experimental**, and the big news is DSA, or DeepSeek Sparse Attention. For those who don‚Äôt know, one of the biggest bottlenecks in LLMs is the ‚Äúquadratic attention problem‚Äù‚Äîas you double the context length, the computation and memory required quadruple. This makes very long contexts incredibly expensive. DeepSeek‚Äôs new architecture makes the cost curve nearly flat, allowing for massive context at a fraction of the cost, all while maintaining the same SOTA performance as their previous model.

This is one of those ‚Äúunhobbling moments,‚Äù like the invention of RoPE or GRPO, that moves the entire field forward. Everyone will be able to implement this, making all open-source models faster and more efficient. It‚Äôs a huge deal.

We also saw major releases from [Z.ai](Z.ai) with **GLM-4.6**, an advanced agentic model with a 200K context window that‚Äôs getting incredibly close to Claude‚Äôs performance, and a surprise from **ServiceNow SLAM Labs**, who dropped **Apriel-1.5-15B**, a frontier-level multimodal model that‚Äôs fully open source. It‚Äôs amazing to see a huge enterprise company contributing to the open-source ecosystem at this level.

Multimodal Madness: Audio, Video, and Image Models updates

The torrent of releases continued across all modalities this week, a bit overshadowed by SORA but definitely still happened (all links in the TL;DR section)

In voice and audio, our friends at **Hume AI launched Octave 2**, their next-gen text-to-speech model that‚Äôs faster, cheaper, and now fluent in over 11 languages. We also saw **LFM2-Audio from Liquid AI**, an incredibly efficient 1.5B parameter end-to-end audio model with sub-100ms latency.

In video, the open-source community answered Sora 2 with **Kandinsky 5.0**, a new 2B parameter text-to-video model that is claiming the #1 spot in open source and looks incredibly promising. And as I mentioned on the show, I wouldn‚Äôt have even known about it if it weren‚Äôt for my new personal AI agent, Pulse!

Finally, in AI art, Tencent dropped a monster: **HunyuanImage 3.0**, a massive 80-billion-parameter open-source text-to-image model. The scale of these open-source releases is just breathtaking.

Agentic browsing for all is here

Just as I was wrapping up the show, Perplexity has decided to let everyone in to use their Comet Agentic browser. I strongly recommend it, as I switched to it lately and it‚Äôs great! 

I‚Äôm using it right now to run some agents, it can click stuff, scroll through stuff, collect info across tabs, it‚Äôs really great. Give it a spin, really, it‚Äôs worth getting into the habit of agentic browsing! 

Many of you were asking me for invites before, well, it‚Äôs free access now, [download it here](https://comet.perplexity.ai/) (not sponsored, I just really like it) 

Phew, ok, this was a WILD week, and I‚Äôm itching to get back to creating and seeing all the folks who used my Cameo on SORA, which you can see too btw if you hit the Cameo button here ([https://sora.chatgpt.com/profile/altryne](https://sora.chatgpt.com/profile/altryne)) 

Next week is OpenAI‚Äôs Dev Day, and for the third year in a row we‚Äôre going to cover it, so follow us on social media and tune in Monday 8:30am Pacific. We‚Äôll be live streaming from the location and re-streaming the keynote with Sam so don‚Äôt miss it! 

TL;DR and Show Notes

**Hosts and Guests**:

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co Hosts - [@WolframRvnwlf](http://x.com/@WolframRvnwlf) [@yampeleg](x.com/@yampeleg) [@nisten](http://x.com/@nisten) [@ldjconfirmed](http://x.com/@ldjconfirmed) [@ryancarson](https://x.com/ryancarson/status/1957809743679906246)

**Big CO LLMs + APIs**:

* OpenAI releases SORA2 + a new social media app ([X](https://x.com/altryne/status/1973568567489798144), [Blog](https://openai.com/index/sora-2/), [App download](https://apps.apple.com/us/app/sora-by-openai/id6744034028))

* Anthropic releases Claude Sonnet 4.5 - same price as 4.1 - leading coding model ([X](https://x.com/claudeai/status/1972706807345725773))

* OpenAI launches Instant Checkout & Agentic Commerce Protocol ([X](https://x.com), [Protocol](https://agenticcommerce.dev))

**Open Source LLMs**:

* DeepSeek V3.2 Exp: Sparse Attention, Cost Drop ([X](https://x.com/deepseek_ai/status/1972604768309871061), [Evals](https://twitter.com/ArtificialAnlys/status/1973230103854456993), [HF](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp))

* Apriel-1.5-15B-Thinker by ServiceNow SLAM Labs ([X](https://twitter.com/ServiceNowRSRCH/status/1973100536280027586), [HF](https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker), [Arxiv](https://arxiv.org/abs/2508.10948))

* [Z.ai](Z.ai) GLM-4.6: advanced Agentic flagship model ([X](https://x.com/Zai_org/status/1973034639708344767), [Blog](https://z.ai/blog/glm-4.6), [HF](https://huggingface.co/zai-org/GLM-4.6))

**This weeks Buzz**:

* CoreWeave locks **$22.4B OpenAI**, a **$6.3B NVIDIA ‚Äúbackstop‚Äù**, and a **$14.2B Meta** compute pact ([X](https://x.com/CoreWeave/status/1971218329713938942))

**Voice & Audio**:

* Hume AI launches Octave 2 ([X](https://twitter.com/hume_ai/status/1973450822840152455), [Blog](https://hume.ai/blog/octave2))

* LFM2-Audio: End-to-end audio foundation model ([X](https://x.com/LiquidAI_/status/1973372092230836405), [Blog](https://www.liquid.ai/blog/lfm2-audio-an-end-to-end-audio-foundation-model), [HF](https://huggingface.co/LiquidAI/LFM2-Audio-1.5B))

**Vision & Video**:

* Kandinsky 5.0 T2V Lite: #1 open-source text-to-video ([Blog](https://ai-forever.github.io/Kandinsky-5/), [GitHub](https://github.com/ai-forever/Kandinsky-5), [HF](https://huggingface.co/collections/ai-forever/kandinsky-50-t2v-lite-68d71892d2cc9b02177e5ae5), [Try It](https://t.me/kandinsky_access_bot))

**AI Art & Diffusion & 3D**:

* HunyuanImage 3.0: 80B Open-Source Text-to-Image by Tencent ([X](https://twitter.com/TencentHunyuan/status/1972130405160833334), [HF](https://huggingface.co/tencent/HunyuanImage-3.0), [Github](https://github.com/Tencent-Hunyuan)) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-oct-2-sora-2-the-new-tiktok/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-oct-2-sora-2-the-new-tiktok?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE3NTE1MjM4NiwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.vb191_pQaKspBYaenugvHkeIzsAbojabAkiwtBknBXU&utm_campaign=CTA_5).
