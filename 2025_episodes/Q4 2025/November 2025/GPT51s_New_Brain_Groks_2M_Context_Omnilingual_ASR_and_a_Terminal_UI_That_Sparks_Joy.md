# GPT‚Äë5.1‚Äôs New Brain, Grok‚Äôs 2M Context, Omnilingual ASR, and a Terminal UI That Sparks Joy

**Date:** November 13, 2025  
**Duration:** 1:10:20  
**Link:** [https://sub.thursdai.news/p/thursdai-nov-13-gpt-51-ernie-45-vl](https://sub.thursdai.news/p/thursdai-nov-13-gpt-51-ernie-45-vl)

---

## Description

Hey, this is Alex! 

We‚Äôre finally so back! Tons of open source releases, OpenAI updates GPT and a few breakthroughs in audio as well, makes this a very dense week! 

Today on the show, we covered the newly released GPT 5.1 update, a few open source releases like Terminal Bench and Project AELLA (renamed OASSAS), and Baidu‚Äôs Ernie 4.5 VL that shows impressive visual understanding! 

Also, chatted with Paul from 11Labs and Dima Duev from the wandb SDK team, who brought us a delicious demo of LEET, our new TUI for wandb! 

Tons of news coverage, let‚Äôs dive in üëá (as always links and show notes in the end) 

Open Source AI

Let‚Äôs jump directly into Open Source as this week has seen some impressive big company models. 

Terminal-Bench 2.0 -  a harder, highly‚Äëverified coding and terminal benchmark ([X](https://x.com/alexgshaw/status/1986911106108211461), [Blog](https://harborframework.com/), [Leaderboard](https://www.tbench.ai/leaderboard))

We opened with Terminal‚ÄëBench 2.0 plus its new harness, Harbor, because this is the kind of benchmark we‚Äôve all been asking for. 

Terminal‚ÄëBench focuses on agentic coding in a real shell. Version 2.0 is a hard set of 89 terminal tasks, each one painstakingly vetted by humans and LLMs to make sure it‚Äôs solvable and realistic. Think ‚ÄúI checked out master and broke my personal site, help untangle the git mess‚Äù or ‚Äúimplement GPT‚Äë2 code golf with the fewest characters.‚Äù 

On the new leaderboard, top agents like Warp‚Äôs agentic console and Codex CLI + GPT‚Äë5 sit around fifty percent success. That number is exactly what excites me: we‚Äôre nowhere near saturation. When everyone is in the 90‚Äësomething range, tiny 0.1 improvements are basically noise. When the best models are at fifty percent, a five‚Äëpoint jump really means something.

A huge part of our conversation focused on reproducibility. We‚Äôve seen other benchmarks like OSWorld turn out to be unreliable, with different task sets and non‚Äëreproducible results making scores incomparable. Terminal‚ÄëBench addresses this with Harbor, a harness designed to run sandboxed, containerized agent rollouts at scale in a consistent environment. This means results are actually comparable. It‚Äôs a ton of work to build an entire evaluation ecosystem like this, and with over a thousand contributors on their Discord, it‚Äôs a fantastic example of a healthy, community‚Äëdriven effort. This is one to watch! 

**Baidu‚Äôs ERNIE‚Äë4.5‚ÄëVL ‚ÄúThinking‚Äù: a 3B visual reasoner that punches way up **([X](https://x.com/Baidu_Inc/status/1988182106359411178), [HF](https://huggingface.co/ERNIE/ERNIE-4.5-VL-28B-A3B-Thinking), [GitHub](https://github.com/ERNIE/ERNIE-4.5-VL-28B-A3B-Thinking))

Next up, Baidu dropped a really interesting model, ERNIE‚Äë4.5‚ÄëVL‚Äë28B‚ÄëA3B‚ÄëThinking. This is a compact, 3B active‚Äëparameter multimodal reasoning model focused on vision, and it‚Äôs much better than you‚Äôd expect for its size. Baidu‚Äôs own charts show it competing with much larger closed models like Gemini‚Äë2.5‚ÄëPro and GPT‚Äë5‚ÄëHigh on a bunch of visual benchmarks like ChartQA and DocVQA.

During the show, I dropped a fairly complex chart into the demo, and ERNIE‚Äë4.5‚ÄëVL gave me a clean textual summary almost instantly‚Äîit read the chart more cleanly than I could. The model is built to ‚Äúthink with images,‚Äù using dynamic zooming and spatial grounding to analyze fine details. It‚Äôs released under an Apache‚Äë2.0 license, making it a serious candidate for edge devices, education, and any product where you need a cheap but powerful visual brain.

**Open Source Quick Hits: OSSAS, VibeThinker, and Holo Two**

We also covered a few other key open-source releases. Project AELLA was quickly rebranded to OSSAS (Open Source Summaries At Scale), an initiative to make scientific literature machine‚Äëreadable. They‚Äôve released 100k paper summaries, two fine-tuned models for the task, and a 3D visualizer. It‚Äôs a niche but powerful tool if you‚Äôre working with massive amounts of research. ([X](https://x.com/samhogan/status/1988306424309706938), [HF](https://huggingface.co/inference-net))

WeiboAI (from the Chinese social media company) released VibeThinker‚Äë1.5B, a tiny 1.5B‚Äëparameter reasoning model that is making bold claims about beating the 671B DeepSeek R1 on math benchmarks. 

We discussed the high probability of benchmark contamination, especially on tests like AIME24, but even with that caveat, getting strong chain‚Äëof‚Äëthought math out of a 1.5B model is impressive and useful for resource‚Äëconstrained applications.  ([X](https://x.com/WeiboLLM/status/1988109435902832896), [HF](https://huggingface.co/WeiboAI/VibeThinker-1.5B), [Arxiv](https://arxiv.org/abs/2511.06221))

Finally, we had some breaking news mid‚Äëshow: H Company released Holo Two, their next‚Äëgen multimodal agent for controlling desktops, websites, and mobile apps. It‚Äôs a fine‚Äëtune of Qwen3‚ÄëVL and comes in 4B and 8B Apache‚Äë2.0 licensed versions, pushing the open agent ecosystem forward. ([X](https://x.com/hcompany_ai/status/1989013556134638039), [Blog](https://hcompany.ai/blog), [HF](https://huggingface.co/hcompany-ai/Holo2-8B))

ThursdAI - Recaps of the most high signal AI weekly spaces is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.

Big Companies & APIs

**GPT‚Äë5.1: Instant vs Thinking, and a new personality bar**

The biggest headline of the week was OpenAI shipping GPT‚Äë5.1, and this was a hot topic of debate on the show. The update introduces two modes: ‚ÄúInstant‚Äù for fast, low‚Äëcompute answers, and ‚ÄúThinking‚Äù for deeper reasoning on hard problems. OpenAI claims Instant mode uses 57% fewer tokens on easy tasks, while Thinking mode dedicates 71% more compute to difficult ones. This adaptive approach is a smart evolution.

The release also adds a personality dropdown with options like Professional, Friendly, Quirky, and Cynical, aiming for a more ‚Äúwarm‚Äù and customizable experience. 

Yam and I felt this was a step in the right direction, as GPT‚Äë5 could often feel a bit cold and uncommunicative. However, Wolfram had a more disappointing experience, finding that GPT‚Äë5.1 performed significantly worse on his German grammar and typography tasks compared to GPT‚Äë4 or Claude Sonnet 4.5. It‚Äôs a reminder that ‚Äúupgrades‚Äù can be subjective and task‚Äëdependent.

Since the show was recorded, GPT 5.1 is also released in the API and they have published a [prompting guide](https://cookbook.openai.com/examples/gpt-5/gpt-5-1_prompting_guide) and some evals! With some significant jumps across SWE-bench verified and GPQA Diamond! We‚Äôll be testing this model out all week. 

The highlight for this model is the creative writing, it was made public that this model was being tested on OpenRouter as Polaris-alpha and that one tops the eqbench creative writing benchmarks beating Sonnet 4.5 and Gemini! 

**Grok‚Äë4 Fast: 2M context and a native X superpower**

Grok‚Äë4 Fast from xAI apparenly quietly got a substantial upgrade to a 2M‚Äëtoken context window, but the most interesting part is its unique integration with X. The API version has access to internal tools for semantic search over tweets, retrieving top quote tweets, and understanding embedded images and videos. I‚Äôve started using it as a research agent in my show prep, and it feels like having a research assistant living inside X‚Äôs backend‚Äîsomething you simply can‚Äôt replicate with public tools.

I still have my gripes about their ‚Äústealth upgrade‚Äù versioning strategy, which makes rigorous evaluation difficult, but as a practical tool, Grok‚Äë4 Fast is incredibly powerful. It‚Äôs also surprisingly fast and cost‚Äëeffective, holding its own against other top models on benchmarks while offering a superpower that no one else has.

**Google SIMA 2: Embodied Agents in Virtual Worlds**

Google‚Äôs big contribution this week was SIMA 2, DeepMind‚Äôs latest embodied agent for 3D virtual worlds. SIMA lives inside real games like *No Man‚Äôs Sky* and *Goat Simulator*, seeing the screen and controlling the game via keyboard and mouse, using Gemini as its reasoning brain. Demos showed it following complex, sketch‚Äëbased instructions, like finding an object that looks like a drawing of a spaceship and jumping on top of it.

When you combine this with Genie 3‚ÄîGoogle‚Äôs world model that can generate playable environments from a single image‚Äîyou see the bigger picture: agents that learn physics, navigation, and common sense by playing in millions of synthetic worlds. We‚Äôre not there yet, but the pieces are clearly being assembled. We also touched on the latest Gemini Live voice upgrade, which users are reporting feels much more natural and responsive

**More Big Company News: Qwen Deep Research, Code Arena, and Cursor**

We also briefly covered Qwen‚Äôs [new](https://x.com/Alibaba_Qwen/status/1989026687611461705) Deep Research feature, which offers an OpenAI‚Äëstyle research agent inside their ecosystem. LMSYS launched [Blog](https://arena.lmsys.org/blog/code-arena), a fantastic live evaluation platform where models build real web apps agentically, with humans voting on the results. And in the world of funding, the AI‚Äënative code editor Cursor raised a staggering $2.3 billion, a clear sign that AI is becoming the default way developers interact with code.

**This Week‚Äôs Buzz: W&B LEET ‚Äì a terminal UI that sparks joy**

For this week‚Äôs buzz, I brought on Dima Duev from our SDK team at Weights & Biases to show off a side project that has everyone at the company excited: LEET, the Lightweight Experiment Exploration Tool. Imagine you‚Äôre training on an air‚Äëgapped HPC cluster, living entirely in your terminal. How do you monitor your runs? With LEET.

You run your training script in W&B offline mode, and in another terminal, you type wandb beta leet. Your terminal instantly turns into a full TUI dashboard with live metric plots, system stats, and run configs. You can zoom into spikes in your loss curve, filter metrics, and see everything updating in real time, all without a browser or internet connection. It‚Äôs one of those tools that just sparks joy. It ships with the latest wandb SDK (v0.23.0+), so just upgrade and give it a try! 

**Voice & Audio: Scribe v2 Realtime and Omnilingual ASR**

**ElevenLabs Scribe v2 Realtime: ASR built for agents **([X](https://x.com/elevenlabsio/status/1988282248445976987), [Announcement](https://docs.elevenlabs.io), [Demo](https://captions.events/))

We‚Äôve talked a lot on this show about ElevenLabs as ‚Äúthe place you go to make your AI talk.‚Äù This week, they came for the other half of the conversation. Paul Asjes from ElevenLabs joined us to walk through Scribe v2 Realtime, their new low‚Äëlatency speech‚Äëto‚Äëtext model. If you‚Äôre building a voice agent, you need ears, a brain, and a mouth. ElevenLabs already nailed the mouth, and now they‚Äôve built some seriously good ears.

Scribe v2 Realtime is designed to run at around 150 milliseconds median latency, across more than ninety languages. Watching Paul‚Äôs live demo, it felt comfortably real‚Äëtime. When he switched from English to Dutch mid‚Äësentence, the system just followed along without missing a beat. Community benchmarks and our own impressions show it holding its own or beating competitors like Whisper and Deepgram in noisy, accented, and multi‚Äëspeaker scenarios. It‚Äôs also context‚Äëaware enough to handle code, initialisms, and numbers correctly, which is critical for real‚Äëworld agents. This is a production‚Äëready ASR for anyone building live voice experiences.

**Meta‚Äôs drops Omnilingual ASR: 1,600+ languages, many for the first time + a bunch of open source models  ** ([X](https://x.com/AIatMeta/status/1987946571439444361), [Blog](https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition), [Announcement](https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/), [HF](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus))

On the other end of the spectrum, Meta released something that‚Äôs less about ultra‚Äëlow latency and more about sheer linguistic coverage: Omnilingual ASR. This is a family of models and a dataset designed to support speech recognition for more than 1,600 languages, including about 500 that have never had any ASR support before. That alone is a massive contribution.

Technically, it uses a wav2vec 2.0 backbone scaled up to 7B parameters with both CTC and LLM‚Äëstyle decoders. The LLM‚Äëlike architecture allows for in‚Äëcontext learning, so communities can add support for new languages with only a handful of examples. They‚Äôre also releasing the Omnilingual ASR Corpus with data for 350 underserved languages. The models and code are Apache‚Äë2.0, making this a huge step forward for more inclusive speech tech.

**AI Art, Diffusion & 3D**

**Qwen Image Edit + Multi‚ÄëAngle LoRA: moving the camera after the fact  **([X](https://x.com/linoy_tsaban/status/1986090375409533338), [HF](https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles), [Fal](https://x.com/fal/status/1988693046267969804?s=20))

This one was pure fun. A new set of LoRAs for Qwen Image Edit adds direct camera control to still images. A Hugging Face demo lets you upload a photo and use sliders to rotate the camera up to 90 degrees, tilt from a bird‚Äôs‚Äëeye to a worm‚Äôs‚Äëeye view, and adjust the lens. We played with it live on the show with a portrait of Wolfram and a photo of my cat, generating different angles and then interpolating them into a short ‚Äúfly‚Äëaround‚Äù video. It‚Äôs incredibly cool and preserves details surprisingly well, feeling like you have a virtual camera inside a 2D picture.

**NVIDIA ChronoEdit‚Äë14B Upscaler LoRA **([X](https://x.com/HuanLing6/status/1988098676838060246), [HF](https://huggingface.co/NVIDIA/ChronoEdit-14B-Diffusers-Upscaler-LoRA))

Finally, NVIDIA released an upscaler LoRA based on their ChronoEdit‚Äë14B model and merged the pipeline into Hugging Face Diffusers. ChronoEdit reframes image editing as a temporal reasoning task, like generating a tiny video. This makes it good for maintaining consistency in edits and upscales. It‚Äôs a heavy model, requiring ~34GB of VRAM, and for aggressive upscaling, specialized tools might still be better. But for moderate upscales where temporal coherence matters, it‚Äôs a very interesting new tool in the toolbox.

Phew, we made it through this dense week! Looking to next week, I‚Äôll be recoridng the show live from the AI Engieer CODE summit in NY, and we‚Äôll likely see a few good releases from the big G? Maybe? finally? 

As always, if this was helpful, please subscribe to ThursdAI and share it with 2 friends, see you next week ü´° 

TL;DR and Show Notes

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/altryne))

* **Co-Hosts** - [@WolframRvnwlf](http://x.com/WolframRvnwlf), [@yampeleg](https://x.com/yampeleg), [@ldjconfirmed](http://x.com/ldjconfirmed)

* **Guest**: Dima Duev - SDK team Wandb

* **Guest**: Paul Asjes - Eleven Labs ([@paul_asjes](https://x.com/paul_asjes))

* **Open Source LLMs**

* **Terminal-Bench 2.0 and Harbor launch** ([X](https://x.com/alexgshaw/status/1986911106108211461), [Blog](https://harborframework.com/), [Docs](https://harborframework.com/docs/running-tbench), [Announcement](https://www.tbench.ai/leaderboard))

* **Baidu releases ERNIE-4.5-VL-28B-A3B-Thinking** ([X](https://x.com/Baidu_Inc/status/1988182106359411178), [HF](https://huggingface.co/ERNIE/ERNIE-4.5-VL-28B-A3B-Thinking), [GitHub](https://github.com/ERNIE/ERNIE-4.5-VL-28B-A3B-Thinking), [Blog](https://ernie.baidu.com/blog/ernie-4-5-vl-28b-a3b-thinking), [Platform](https://ai.baidu.com/ai-studio))

* **Project AELLA (OSSAS): 100K LLM-generated paper summaries** ([X](https://x.com/samhogan/status/1988306424309706938), [HF](https://huggingface.co/inference-net))

* **WeiboAI‚Äôs VibeThinker-1.5B** ([X](https://x.com/WeiboLLM/status/1988109435902832896), [HF](https://huggingface.co/WeiboAI/VibeThinker-1.5B), [Arxiv](https://arxiv.org/abs/2511.06221), [Announcement](https://venturebeat.com/ai/weibos-new-open-source-ai-model-vibethinker-1-5b-outperforms-deepseek-r1-on))

* **Code Arena ‚Äî live, agentic coding evaluations** ([X](https://x.com/arena/status/1988665193275240616), [Blog](https://arena.lmsys.org/blog/code-arena), [Announcement](https://arena.lmsys.org/))

* **Big CO LLMs + APIs**

* **Grok 4 Fast, Grok Imagine and Nano Banana v1/v2** ([X](https://x.com/chatgpt21/status/1987976808562589946), [X](https://x.com/cowowhite/status/1988213138333069314), [X](https://x.com/RasNas1994/status/1986426297900245106), [X](https://x.com/XFreeze/status/1987396781861212353))

* **OpenAI launches GPT-5.1** ([X](https://x.com/fidjissimo/status/1988683216681889887), [X](https://x.com/sama/status/1988692165686620237))

* **This weeks Buzz**

* **W&B LEET ‚Äî an open-source Terminal UI (TUI) to monitor runs** ([X](https://x.com/wandb/status/1988401253156876418), [Blog](https://app.getbeamer.com/wandb/en/meet-wb-leet-a-new-terminal-ui-for-weights-biases-JXSFhyt2))

* **Voice & Audio**

* **ElevenLabs launches Scribe v2 Realtime** ([X](https://x.com/elevenlabsio/status/1988282248445976987), [Blog](https://elevenlabs.io/agents), [Docs](https://docs.elevenlabs.io))

* **Meta releases Omnilingual ASR for 1,600+ languages** ([X](https://x.com/AIatMeta/status/1987946571439444361), [Blog](https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition), [Paper](https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/), [HF Dataset](https://huggingface.co/datasets/facebook/omnilingual-asr-corpus), [HF Demo](https://huggingface.co/spaces/facebook/omniasr-transcriptions), [GitHub](https://github.com/facebookresearch/omnilingual-asr))

* **Gemini Live conversational upgrade** ([X](https://x.com/carlovarrasi/status/1988691309591425234))

* **AI Art & Diffusion & 3D**

* **Qwen Image Edit + Multi‚ÄëAngle LoRA for camera control** ([X](https://x.com/linoy_tsaban/status/1986090375409533338), [HF](https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-Angles), [Fal](https://x.com/fal/status/1988693046267969804?s=20))

* **NVIDIA releases ChronoEdit-14B Upscaler LoRA** ([X](https://x.com/HuanLing6/status/1988098676838060246), [HF](https://huggingface.co/NVIDIA/ChronoEdit-14B-Diffusers-Upscaler-LoRA), [Docs](https://huggingface.co/docs/diffusers/main/en/api/pipelines/chronoedit)) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-nov-13-gpt-51-ernie-45-vl/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-nov-13-gpt-51-ernie-45-vl?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE3ODgzMTc5NywiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.VgzHd7KhwCSB6lu3AOknChk60OsQoytWW7EhQvzfW_0&utm_campaign=CTA_5).
