# ThursdAI Episodes - Q2 2025

Total Episodes: 13

---

## üìÖ ThursdAI - Jun 26 - Gemini CLI, Flux Kontext Dev, Search Live, Anthropic destroys books, Zucks superintelligent team & more AI news

**Date:** June 26, 2025  
**Duration:** 1:39:39  
**Link:** [https://sub.thursdai.news/p/thursdai-jun-26-gemini-cli-flux-kontext](https://sub.thursdai.news/p/thursdai-jun-26-gemini-cli-flux-kontext)

Hey folks, Alex here, writing from... a undisclosed tropical paradise location üèùÔ∏è I'm on vacation, but the AI news doesn't stop of course, and neither does ThursdAI. So huge shoutout to Wolfram Ravenwlf for running the show this week, Nisten, LDJ and Yam who joined. 

So... no long blogpost with analysis this week, but I'll def. recommend tuning in to the show that the folks ran, they had a few guests on, and even got some breaking news (new Flux Kontext that's open source) 

Of course many of you are readers and are here for the links, so I'm including the raw TL;DR + speaker notes as prepared by the folks for the show! 

P.S - our (rescheduled) hackathon is coming up in San Francisco, on July 12-13 called WeaveHacks, if you're interested at a chance to win a RoboDog, welcome to join us and give it a try. Register [HERE](https://lu.ma/weavehacks)

Ok, that's it for this week, please enjoy the show and see you next week! 

ThursdAI - June 26th, 2025 - TL;DR

* **Hosts and Guests**

* **WolframRvnwlf** - Host ([@WolframRvnwlf](http://x.com/WolframRvnwlf))

* Co-Hosts - [@yampeleg](http://x.com/yampeleg), [@nisten](http://x.com/nisten), [@ldjconfirmed](http://x.com/ldjconfirmed)

* Guest - **Jason Kneen** ([@jasonkneen](http://x.com/jasonkneen)) - Discussing MCPs, coding tools, and agents

* Guest - **Hrishioa** ([@hrishioa](http://x.com/hrishioa)) - Discussing agentic coding and spec-driven development

* **Open Source LLMs**

* Mistral Small 3.2 released with improved instruction following, reduced repetition & better function calling ([X](https://x.com/MistralAI/status/1936093325116781016))

* Unsloth AI releases dynamic GGUFs with fixed chat templates ([X](https://x.com/UnslothAI/status/1936426567850487925))

* Kimi-VL-A3B-Thinking-2506 multimodal model updated for better video reasoning and higher resolution ([Blog](https://huggingface.co/blog/moonshotai/kimi-vl-a3b-thinking-2506))

* Chinese Academy of Science releases Stream-Omni, a new Any-to-Any model for unified multimodal input ([HF](https://huggingface.co/ICTNLP/stream-omni-8b), [Paper](https://huggingface.co/papers/2506.13642))

* Prime Intellect launches SYNTHETIC-2, an open reasoning dataset and synthetic data generation platform ([X](https://x.com/PrimeIntellect/status/1937272174295023951))

* **Big CO LLMs + APIs**

* **Google**

* Gemini CLI, a new open-source AI agent, brings Gemini 2.5 Pro to your terminal ([Blog](https://web.archive.org/web/20250625051706/https://blog.google/technology/developers/introducing-gemini-cli/), [GitHub](https://github.com/google-gemini/gemini-cli))

* Google reduces free tier API limits for previous generation Gemini Flash models ([X](https://x.com/ai_for_success/status/1937493142279971210))

* Search Live with voice conversation is now rolling out in AI Mode in the US ([Blog](https://blog.google/products/search/search-live-ai-mode/), [X](https://x.com/rajanpatel/status/1935484294182608954))

* Gemini API is now faster for video and PDF processing with improved caching ([Docs](https://ai.google.dev/gemini-api/docs/caching))

* **Anthropic**

* Claude introduces an "artifacts" space for building, hosting, and sharing AI-powered apps ([X](https://x.com/AnthropicAI/status/1937921801000219041))

* Federal judge rules Anthropic's use of books for training Claude qualifies as fair use ([X](https://x.com/ai_for_success/status/1937515997076029449))

* **xAI**

* Elon Musk announces the successful launch of Tesla's Robotaxi ([X](https://x.com/elonmusk/status/1936876178356490546))

* **Microsoft**

* Introduces Mu, a new language model powering the agent in Windows Settings ([Blog](https://blogs.windows.com/windowsexperience/2025/06/23/introducing-mu-language-model-and-how-it-enabled-the-agent-in-windows-settings/))

* **Meta**

* Report: Meta pursued acquiring Ilya Sutskever's SSI, now hires co-founders Nat Friedman and Daniel Gross ([X](https://x.com/kimmonismus/status/1935954015998624181))

* **OpenAI**

* OpenAI removes mentions of its acquisition of Jony Ive's startup 'io' amid a trademark dispute ([X](https://x.com/rowancheung/status/1937414172322439439))

* OpenAI announces the release of DeepResearch in API + Webhook support ([X](https://x.com/stevendcoffey/status/1938286946075418784))

* **This weeks Buzz**

* Alex is on vacation; WolframRvnwlf is attending AI Tinkerers Munich on July 25 ([Event](https://munich.aitinkerers.org/p/ai-tinkerers-munich-july-25))

* Join W&B Hackathon happening in 2 weeks in San Francisco - grand prize is a RoboDog! (Register [for Free](https://lu.ma/weavehacks))

* **Vision & Video**

* MeiGen-MultiTalk code and checkpoints for multi-person talking head generation are released ([GitHub](https://github.com/MeiGen-AI/MultiTalk), [HF](https://huggingface.co/MeiGen-AI/MeiGen-MultiTalk))

* Google releases VideoPrism for generating adaptable video embeddings for various tasks ([HF](https://hf.co/google/videoprism), [Paper](https://arxiv.org/abs/2402.13217), [GitHub](https://github.com/google-deepmind/videoprism))

* **Voice & Audio**

* ElevenLabs launches [11.ai](11.ai), a voice-first personal assistant with MCP support ([Sign Up](http://11.ai/), [X](https://x.com/elevenlabsio/status/1937200086515097939))

* Google Magenta releases Magenta RealTime, an open weights model for real-time music generation ([Colab](https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb), [Blog](https://g.co/magenta/rt))

* ElevenLabs launches a mobile app for iOS and Android for on-the-go voice generation ([X](https://x.com/elevenlabsio/status/1937541389140611367))

* **AI Art & Diffusion & 3D**

* Google rolls out Imagen 4 and Imagen 4 Ultra in the Gemini API and Google AI Studio ([Blog](https://developers.googleblog.com/en/imagen-4-now-available-in-the-gemini-api-and-google-ai-studio/))

* OmniGen 2 open weights model for enhanced image generation and editing is released ([Project Page](https://vectorspacelab.github.io/OmniGen2/), [Demo](https://huggingface.co/spaces/OmniGen2/OmniGen2), [Paper](https://huggingface.co/papers/2506.18871))

* **Tools**

* OpenMemory Chrome Extension provides shared memory across ChatGPT, Claude, Gemini and more ([X](https://x.com/taranjeetio/status/1937537163270451494))

* LM Studio adds MCP support to connect local LLMs with your favorite servers ([Blog](https://lmstudio.ai/blog/mcp))

* Cursor is now available as a Slack integration ([Dashboard](http://cursor.com/dashboard))

* All Hands AI releases the OpenHands CLI, a model-agnostic, open-source coding agent ([Blog](https://all-hands.dev/blog/the-openhands-cli-ai-powered-development-in-your-terminal), [Docs](https://docs.all-hands.dev/usage/how-to/cli-mode#cli))

* Warp 2.0 launches as an Agentic Development Environment with multi-threading ([X](https://x.com/warpdotdev/status/1937525185843752969))

* **Studies and Others**

* The /r/LocalLLaMA subreddit is back online after a brief moderation issue ([Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1ljlr5b/subreddit_back_in_business/), [News](https://x.com/localllamasub))

* Andrej Karpathy's talk "Software 3.0" discusses the future of programming in the age of AI ([YouTube](https://www.youtube.com/watch?v=LCEmiRjPEtQ), [Summary](https://www.latent.space/p/s3))

Thank you, see you next week!  

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-jun-26-gemini-cli-flux-kontext/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-jun-26-gemini-cli-flux-kontext?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2NjkyNTYyOCwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.rNrzJzOHwv_6WuWE0zOf7g9C0xIjVsHBeuiHWjLmawY&utm_campaign=CTA_5).

---

## üìÜ ThursdAI - June 19 - MiniMax M1 beats R1, OpenAI records your meetings, Gemini in GA, W&B uses Coreweave GPUs & more AI news

**Date:** June 20, 2025  
**Duration:** 1:41:31  
**Link:** [https://sub.thursdai.news/p/thursdai-june-18-minimax-m1-beats](https://sub.thursdai.news/p/thursdai-june-18-minimax-m1-beats)

Hey all, Alex here üëã

This week, while not the busiest week in releases (we can't get a SOTA LLM every week now can we), was full of interesting open source releases, and feature updates such as the chatGPT meetings recorder (which we live tested on the show, the limit is 2 hours!)

It was also a day after our annual W&B conference called FullyConnected, and so I had a few goodies to share with you, like answering the main question, when will W&B have some use of those GPUs from CoreWeave, the answer is... now! (We launched a brand new preview of an inference service with open source models)

And finally, we had a great chat with Pankaj Gupta, co-founder and CEO of Yupp, a new service that lets users chat with the top AIs for free, while turning their votes into leaderboards for everyone else to understand which Gen AI model is best for which task/topic. It was a great conversation, and he even shared an invite code with all of us (I'll attach to the TL;DR and show notes, let's dive in!)

00:00 Introduction and Welcome

01:04 Show Overview and Audience Interaction

01:49 Special Guest Announcement and Experiment

03:05 Wolfram's Background and Upcoming Hosting

04:42 TLDR: This Week's Highlights

15:38 Open Source AI Releases

32:34 Big Companies and APIs

32:45 Google's Gemini Updates

42:25 OpenAI's Latest Features

54:30 Exciting Updates from Weights & Biases

56:42 Introduction to Weights & Biases Inference Service

57:41 Exploring the New Inference Playground

58:44 User Questions and Model Recommendations

59:44 Deep Dive into Model Evaluations

01:05:55 Announcing Online Evaluations via Weave

01:09:05 Introducing Pankaj Gupta from [YUP.AI](http://YUP.AI)

01:10:23 [YUP.AI](http://YUP.AI): A New Platform for Model Evaluations

01:13:05 Discussion on Crowdsourced Evaluations

01:27:11 New Developments in Video Models

01:36:23 OpenAI's New Transcription Service

01:39:48 Show Wrap-Up and Future Plans

Here's the TL;DR and show notes links

ThursdAI - June 19th, 2025 - TL;DR

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co Hosts - [@WolframRvnwlf](http://x.com/@WolframRvnwlf) [@yampeleg](x.com/@yampeleg) [@nisten](http://x.com/@nisten) [@ldjconfirmed](http://x.com/@ldjconfirmed))

* Guest - [@pankaj](http://x.com/@pankaj) - co-founder of [Yupp.ai](https://yupp.ai/join/thursdAI)

* **Open Source LLMs**

* Moonshot AI open-sourced Kimi-Dev-72B ([Github](https://github.com/MoonshotAI/Kimi-Dev?tab=readme-ov-file), [HF](https://huggingface.co/moonshotai/Kimi-Dev-72B))

* MiniMax-M1 456B (45B Active) - reasoning model ([Paper](https://arxiv.org/abs/2506.13585), [HF](https://huggingface.co/MiniMaxAI/MiniMax-M1-40k), [Try It](https://huggingface.co/spaces/MiniMaxAI/MiniMax-M1), [Github](https://github.com/MiniMax-AI/MiniMax-M1))

* **Big CO LLMs + APIs**

* Google drops Gemini 2.5 Pro/Flash GA, 2.5 Flash-Lite in Preview ( [Blog](https://blog.google/products/gemini/gemini-2-5-model-family-expands/), [Tech report](https://storage.googleapis.com/gemini-technical-report), [Tweet](https://x.com/google/status/192905415))

* Google launches Search Live: Talk, listen and explore in real time with AI Mode ([Blog](https://blog.google/products/search/search-live-ai-mode/))

* OpenAI adds MCP support to Deep Research in chatGPT ([X](https://x.com/altryne/status/1934644274227769431), [Docs](https://platform.openai.com/docs/mcp))

* OpenAI launches their meetings recorder in mac App ([docs](https://help.openai.com/en/articles/11487532-chatgpt-record))

* Zuck update: Considering bringing Nat Friedman and Daniel Gross to Meta ([information](https://x.com/amir/status/1935461177045516568))

* **This weeks Buzz**

* NEW! W&B Inference provides a unified interface to access and run top open-source AI models ([inference](https://wandb.ai/inference), [docs](https://weave-docs.wandb.ai/guides/integrations/inference/))

* NEW! W&B Weave Online Evaluations delivers real-time production insights and continuous evaluation for AI agents across any cloud. ([X](https://x.com/altryne/status/1935412384283107572))

* The new platform offers "metal-to-token" observability, linking hardware performance directly to application-level metrics.

* Vision & Video

* ByteDance new video model beats VEO3 - Seedance.1.0 mini ([Site](https://dreamina.capcut.com/ai-tool/video/generate), [FAL](https://fal.ai/models/fal-ai/bytedance/seedance/v1/lite/image-to-video))

* MiniMax Hailuo 02 - 1080p native, SOTA instruction following ([X](https://www.minimax.io/news/minimax-hailuo-02), [FAL](https://fal.ai/models/fal-ai/minimax/hailuo-02/pro/image-to-video))

* Midjourney video is also here - great visuals ([X](https://x.com/angrypenguinPNG/status/1932931137179176960))

* **Voice & Audio**

* Kyutai launches open-source, high-throughput streaming Speech-To-Text models for real-time applications ([X](https://x.com/kyutai_labs/thread/1935652243119788111), [website](https://join.yupp.ai/thursdai))

* Studies and Others

* LLMs Flunk Real-World Coding Contests, Exposing a Major Skill Gap ([Arxiv](https://arxiv.org/pdf/2506.11928))

* MIT Study: ChatGPT Use Causes Sharp Cognitive Decline ([Arxiv](https://arxiv.org/abs/2506.08872))

* Andrej Karpathy's "Software 3.0": The Dawn of English as a Programming Language ([youtube](https://www.youtube.com/watch?v=LCEmiRjPEtQ), [deck](https://drive.google.com/file/d/1HIEMdVlzCxke22ISVzornd2-UpWHngRZ/view?usp=sharing))

* **Tools**

* Yupp launches with 500+ AI models, a new leaderboard, and a user-powered feedback economy - use [thursdai link](https://yupp.ai/join/thursdAI)* to get 50% extra credits

* BrowserBase announces [director.ai](http://director.ai) - an agent to run things on the web

* Universal system prompt for reduction of hallucination (from [Reddit](https://www.reddit.com/r/PromptEngineering/comments/1kup28y/chatgpt_and_gemini_ai_will_gaslight_you_everyone/))

*Disclosure: while this isn't a paid promotion, I do think that yupp has a great value, I do get a bit more credits on their platform if you click my link and so do you. You can go to [yupp.ai](http://yupp.ai) and register with no affiliation if you wish. 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-june-18-minimax-m1-beats/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-june-18-minimax-m1-beats?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2NjM1OTY2MCwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.XSlsS0LVkZoKjnK1vqluK6duzE3fa7L1zHsEvPRQUW8&utm_campaign=CTA_5).

---

## üìÜ ThursdAI - June 12 - Meta‚Äôs $15B ScaleAI Power Play, OpenAI‚Äôs o3-pro & 90% Price Drop!

**Date:** June 13, 2025  
**Duration:** 1:33:10  
**Link:** [https://sub.thursdai.news/p/thursdai-june-12-metas-15b-scaleai](https://sub.thursdai.news/p/thursdai-june-12-metas-15b-scaleai)

Hey folks, this is Alex, finally back home! 

This week was full of crazy AI news, both model related but also shifts in the AI landscape and big companies, with Zuck going all in on scale & execu-hiring Alex Wang for a crazy $14B dollars. 

OpenAI meanwhile, maybe received a new shipment of GPUs? Otherwise, it‚Äôs hard to explain how they have dropped the o3 price by 80%, while also shipping o3-pro (in chat and API). 

Apple was also featured in today‚Äôs episode, but more so for the lack of AI news, completely delaying the ‚Äúvery personalized private Siri powered by Apple Intelligence‚Äù during WWDC25 this week. 

We had 2 guests on the show this week, [Stefania Druga](https://substack.com/profile/109432335-stefania-druga) and Eric Provencher (who builds RepoPrompt). Stefania helped me cover the AI Engineer conference we all went to last week, and shared some cool Science CoPilot stuff she‚Äôs working on, while Eric is the GOTO guy for O3-pro helped us understand what this model is great for! 

As always, TL;DR and show notes at the bottom, video for those who prefer watching is attached below, let‚Äôs dive in! 

Big Companies LLMs & APIs

Let‚Äôs start with big companies, because the landscape has shifted, new top reasoner models dropped and some huge companies didn‚Äôt deliver this week! 

Zuck goes all in on SuperIntelligence - Meta‚Äôs $14B stake in ScaleAI and Alex Wang

This may be the most consequential piece of AI news today. Fresh from the dissapointing results of LLama 4, reports of top researchers leaving the Llama team, many have decided to exclude Meta from the AI race. We have a saying at ThursdAI, don‚Äôt bet against Zuck! 

Zuck decided to spend a lot of money (nearly 20% of their reported $65B investment in AI infrastructure) to get a 49% stake in Scale AI and bring Alex Wang it‚Äôs (now former) CEO to lead the new Superintelligence team at Meta. 

For folks who are not familiar with Scale, it‚Äôs a massive company in providing human annotated data services to all the big AI labs, Google, OpenAI, Microsoft, Anthropic.. all of them really. Alex Wang, is the youngest self made billionaire because of it, and now Zuck not only has access to all their expertise, but also to a very impressive AI persona, who could help revive the excitement about Meta‚Äôs AI efforts, help recruit the best researchers, and lead the way inside Meta. 

Wang is also an outspoken China hawk who spends as much time in congressional hearings as in Slack, so the geopolitics here are ‚Ä¶ spicy. Meta just stapled itself to the biggest annotation funnel on Earth, hired away Google‚Äôs Jack Rae (who was on the pod just last week, shipping for Google!) for brainy model alignment, and started waving seven-to-nine-figure comp packages at every researcher with ‚ÄúTransformer‚Äù in their citation list. Whatever disappointment you felt over Llama-4‚Äôs muted debut, Zuck clearly felt it too‚Äîand responded like a founder who still controls every voting share. 

**OpenAI‚Äôs Game-Changer: o3 Price Slash & o3-pro launches to top the intelligence leaderboards!**

Meanwhile OpenAI dropping not one, but two mind-blowing updates. First, they‚Äôve slashed the price of o3‚Äîtheir premium reasoning model‚Äîby a staggering 80%. We‚Äôre talking from $40/$10 per million tokens down to just $8/$2. That‚Äôs right, folks, it‚Äôs now in the same league as Claude Sonnet cost-wise, making top-tier intelligence dirt cheap. I remember when a price drop of 80% after a year got us excited; now it‚Äôs 80% in just four months with zero quality loss. They‚Äôve confirmed it‚Äôs the full o3 model‚Äîno distillation or quantization here. How are they pulling this off? I‚Äôm guessing someone got a shipment of shiny new H200s from Jensen!

And just when you thought it couldn‚Äôt get better, OpenAI rolled out o3-pro, their highest intelligence offering yet. Available for pro and team accounts, and via API (87% cheaper than o1-pro, by the way), this model‚Äîor consortium of models‚Äîis a beast. It‚Äôs topping charts on Artificial Analysis, barely edging out Gemini 2.5 as the new king. Benchmarks are insane: 93% on AIME 2024 (state-of-the-art territory), 84% on GPQA Diamond, and nearing a 3000 ELO score on competition coding. Human preference tests show 64-66% of folks prefer o3-pro for clarity and comprehensiveness across tasks like scientific analysis and personal writing.

I‚Äôve been playing with it myself, and the way o3-pro handles long context and tough problems is unreal. As my friend Eric Provencher (creator of RepoPrompt) shared on the show, it‚Äôs surgical‚Äîperfect for big refactors and bug diagnosis in coding. It‚Äôs got all the tools o3 has‚Äîweb search, image analysis, memory personalization‚Äîand you can run it in background mode via API for async tasks. Sure, it‚Äôs slower due to deep reasoning (no streaming thought tokens), but the consistency and depth? Worth it. 

Oh, and funny story‚ÄîI was prepping [a talk](https://youtu.be/KEdoIbBu2Ko) for Hamel Hussain‚Äôs evals course, with a slide saying ‚Äúdon‚Äôt use large reasoning models if budget‚Äôs tight.‚Äù The day before, this price drop hits, and I‚Äôm scrambling to update everything. That‚Äôs AI pace for ya!

**Apple WWDC: Where‚Äôs the Smarter Siri?** 

Oh Apple. Sweet, sweet Apple. Remember all those Bella Ramsey ads promising a personalized Siri that knows everything about you? Well, Craig Federighi opened WWDC by basically saying "Yeah, about that smart Siri... she's not coming. Don't wait up."

Instead, we got:

* AI that can combine emojis (revolutionary! üôÑ)

* Live translation (actually cool)

* Direct API access to on-device models (very cool for developers)

* Liquid glass UI (pretty but... where's the intelligence?)

The kicker? Apple released a paper called "The Illusion of Thinking" right before WWDC, basically arguing that AI reasoning models hit hard complexity ceilings. Some saw this as Apple making excuses for why they can't ship competitive AI. The timing was... interesting.

During our recording, Nisten's Siri literally woke up randomly when we were complaining about how dumb it still is. After a decade, it's the same Siri. That moment was pure comedy gold.

**This Week's Buzz**

Our premium conference **Fully Connected** is happening June 17-18 in San Francisco! Use promo code **WBTHURSAI** to [register for free](https://fullyconnected.com). We'll have updates on the CoreWeave acquisition, product announcements, and it's the perfect chance to give feedback directly to the people building the tools you use.

Also, my talk on Large Reasoning Models as LLM judges is now up on YouTube. Had to update it live because of the O3 price drop - such is life in AI!

**Open Source LLMs: Mistral Goes Reasoning Mode**

**Mistral Drops Magistral - Their First Reasoning Model**

The French champagne of LLMs is back! Mistral released **Magistral**, their first reasoning model, in two flavors: a 24B parameter open-source Small version and a closed API-only Medium version. And honestly? The naming continues to be *chef's kiss* - Mistral really has the branding game locked down.

Now, here's where it gets spicy. Mistral's benchmarks notably don't include comparisons to Chinese models like Qwen or DeepSeek. Dylan Patel from SemiAnalysis called them out on this, and when he ran the comparisons himself, well... let's just say Magistral Medium barely keeps up with Qwen's tiny 4B parameter model on math benchmarks. Ouch.

But here's the thing - and Nisten really drove this home during our discussion - benchmarks don't tell the whole story. He's been using Magistral Small for his workflows and swears by it. "It's almost at the point where I don't want to tell people about it," he said, which is the highest praise from someone who runs models locally all day. The 24B Small version apparently hits that sweet spot for local deployment while being genuinely useful for real work.

The model runs on a single RTX 4090 or a 32GB MacBook after quantization, has a 128K context window (though they recommend capping at 40K), and uses a transparent mode that shows its reasoning process. It's Apache 2.0 licensed, multilingual, and available through their Le Chat interface with "Flash Answers" for real-time reasoning.

**SakanaAI's Text2Lora: The Future is Self-Adapting Models**

This one blew my mind. SakanaAI (co-founded by one of the Transformer paper authors) released **Text2Lora** - a method for adapting LLMs to new tasks using ONLY text descriptions. No training data needed!

Think about this: instead of fine-tuning a model with thousands of examples to make it better at math, you just... tell it to be better at math. And it works! On Llama 3.1 8B, Text2Lora reaches 77% average accuracy, outperforming all baseline methods.

What this means is we're approaching a world where models can essentially customize themselves on-the-fly for whatever task you throw at them. As Nisten put it, "This is revolutionary. The model is actually learning, actually changing its own weights." We're just seeing the first glimpses of this capability, but in 6-12 months? 

**üé• Multimedia & Tools: Video, Voice, and Browser Breakthroughs**

Let‚Äôs zip through some multimedia and tool updates that caught my eye this week. Google‚Äôs VEO3-fast is a creator‚Äôs dream‚Äî2x faster 720p video generation, 80% cheaper, and now with audio support. I‚Äôve seen clips on social media (like an NBA ad) that are unreal, though Wolfram noted it‚Äôs not fully rolled out in Europe yet. You can access it via APIs like Fail or Replicate, and I‚Äôm itching to make a full movie if I had the budget!

Midjourney‚Äôs gearing up for a video product with their signature style, but they‚Äôre also facing heat‚ÄîDisney and Universal are suing them for copyright infringement over Star Wars and Avengers-like outputs. It‚Äôs Hollywood‚Äôs first major strike against AI, and while I get the IP concern, it‚Äôs odd they picked the smaller player when OpenAI and Google are out there too. This lawsuit could drag on, so stay tuned.

OpenAI‚Äôs new advanced voice mode dropped, aiming for a natural cadence with better multilingual support (Russian and Hebrew sound great now). But honestly? I‚Äôm not loving the breathing and laughing they added‚Äîit‚Äôs uncanny valley for me. Some folks on X are raving, though, and LDJ noted it‚Äôs closing the gap to Sesame‚Äôs Maya. I just wish they‚Äôd let me pick between old and new voices instead of switching under my feet. If OpenAI‚Äôs listening, transparency please!

On the tools side, Yutori‚Äôs Scouts got my timeline buzzing‚ÄîAI agents that monitor the web for any topic (like ‚Äúnext ThursdAI release‚Äù) and notify you of updates. I saw a demo catching leadership changes at xAI, and it‚Äôs the future of web interaction. Couldn‚Äôt log in live on the show (email login woes‚Äîgive me passwords, folks!), but it‚Äôs beta on **yutori.com**. Also, Browser Company finally launched DIA, an AI-native browser in beta. Chatting with open tabs, rewriting text, and instant answers? I‚Äôve been using it to prep for ThursdAI, and it‚Äôs pretty slick. Try it at **diabrowser.com**.

**Wrapping Up: AI‚Äôs Breakneck Pace**

What a week, folks! From OpenAI democratizing intelligence with o3-pro and price cuts to Meta‚Äôs bold superintelligence play with ScaleAI, we‚Äôre witnessing history unfold at lightning speed. Apple‚Äôs stumble at WWDC stings, but open-source gems and new tools keep the excitement alive. I‚Äôm still riding the high from AI Engineer last week‚Äîyour high-fives and feedback mean the world. Next week, don‚Äôt miss Weights & Biases‚Äô Fully Connected conference in SF on June 18-19. I won‚Äôt be there physically, but I‚Äôm cheering from afar‚Äîgrab your spot at **fullyconnected.com **with promo code **WBTHURSAI** for a sweet deal.

Thanks for being part of the ThursdAI crew. Here‚Äôs the full TL;DR and show notes to catch anything you missed. See you next week!

TL;DR of all topics covered:

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co Hosts - [@WolframRvnwlf](http://x.com/@WolframRvnwlf), [@yampeleg](http://x.com/@yampeleg), [@nisten](http://x.com/@nisten), [@ldjconfirmed](http://x.com/@ldjconfirmed)

* Guests - 

* Stefania Druga [@stefania_druga](https://x.com/Stefania_druga) (Independent, Former Research Scientist Google DeepMind),Creator of [scratch copilot](https://medium.com/bits-and-behavior/supercharge-your-scratch-projects-introducing-cognimates-copilot-an-ai-teammate-for-kids-52e616e4096e), and AI Engineer [education summit](https://ai.engineer/education). 

* Eric Provencher - [@pvncher](https://x.com/pvncher) (Building [RepoPrompt](https://repoprompt.com/))

* **Chit Chat** - AI Engineer conference vibes, meeting fans, Jack Rae‚Äôs move to Meta.

* **Open Source LLMs**

* Mistral Magistral - 24B reasoning model ([X](https://x.com/MistralAI/status/1932441507262259564), [HF](https://huggingface.co/mistralai/Magistral-Small-2506), [Blog](https://mistral.ai/news/magistral))

* HuggingFace Screensuite - GUI agents evaluation framework ([HF](https://huggingface.co/blog/screensuite))

* SakanaAI Text2Lora - Instant, Task-Specific LLM Adaptation ([Github](https://github.com/SakanaAI/Text-to-Lora))

* **Big CO LLMs + APIs**

* OpenAI drops o3 price by 90% ([Blog](https://t.co/LkObjZtg9s))

* OpenAI launches o3-pro - highest intelligence model ([X](https://x.com/OpenAI/status/1932530409684005048))

* Meta buys 49% stake in ScaleAI, Alex Wang heads superintelligence team ([Blog](https://www.theinformation.com/articles/meta-pay-nearly-15-billion-scale-ai-stake-startups-28-year-old-ceo), [Axios](https://www.axios.com/2025/06/10/meta-ai-superintelligence-zuckerberg))

* Apple WWDC updates - pause on Apple Intelligence in iOS26, live translation, on-device APIs

* Apple paper on reasoning as illusion ([Paper](https://machinelearning.apple.com/research/illusion-of-thinking), [Rebuttal](https://x.com/ParshinShojaee/status/1932528565788238197))

* **This Week‚Äôs Buzz**

* Fully Connected: W&B‚Äôs 2-day conference, June 17-18 in SF ([fullyconnected.com](http://fullyconnected.com)) - Promo Code WBTHURSAI

* Alex‚Äôs talk on LRM as LLM judges on Hamel‚Äôs course ([YT](https://www.youtube.com/watch?reload=9&v=KEdoIbBu2Ko))

* **Vision & Video**

* VEO3-fast - 2x faster 720p generations, 80% cheaper

* Midjourney to launch video product ([X](https://x.com/bilawalsidhu/status/1932942424751366383?s=46))

* Topaz Astra - creative 4K video upscaler ([X](https://x.com/topazlabs/status/1932421641654477275), [Site](http://astra.app))

* **Voice & Audio**

* OpenAI‚Äôs new advanced voice mode - mixed responses, better multilingual support

* Cartesia Ink-Whisper - optimized for real-time chat ([Blog](https://cartesia.ai/blog/introducing-ink-speech-to-text))

* **AI Art & Diffusion & 3D**

* Disney & Universal sue Midjourney - first Hollywood vs AI lawsuit ([NBC](https://www.nbcnews.com/business/business-news/disney-universal-sue-ai-image-company-midjourney-unlicensed-star-wars-rcna212369))

* Krea releases KREA-1 - custom image gen model ([X](https://x.com/krea_ai/status/1932440476541411670))

* **AI Tools**

* Yutori Scouts - AI agents for web monitoring ([Blog](http://yutori.com))

* BrowserCompany DIA - AI-native browser in beta ([Link](http://diabrowser.com)) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-june-12-metas-15b-scaleai/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-june-12-metas-15b-scaleai?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2NTgzMjM5NiwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.xPxeOsBRMF1fmI73NHUvNypdhkxhn0FNSKWWZbBkLgc&utm_campaign=CTA_5).

---

## üìÜ ThursdAI - Jun 5, 2025 - Live from AI Engineer with Swyx, new Gemini 2.5 with Logan K and Jack Rae, Self Replicating agents with Morph Labs

**Date:** June 06, 2025  
**Duration:** 1:43:45  
**Link:** [https://sub.thursdai.news/p/thursdai-jun-5-2025-live-from-ai](https://sub.thursdai.news/p/thursdai-jun-5-2025-live-from-ai)

Hey folks, this is Alex, coming to you LIVE from the AI Engineer Worlds Fair! 

What an incredible episode this week, we recorded live from floor 30th at the Marriott in SF, while Yam was doing live correspondence from the floor of the AI Engineer event, all while Swyx, the cohost of Latent Space podcast, and the creator of AI Engineer (both the conference and the concept itself) joined us for the whole stream - here‚Äôs the edited version, please take a look.  

We've had around 6500 people tune in, and at some point we got 2 surprise guests, straight from the keynote stage, Logan Kilpatrick (PM for AI Studio and lead cheerleader for Gemini) and Jack Rae (principal scientist working on reasoning) joined us for a great chat about Gemini! Mind was absolutely blown! 

They have just launched the new Gemini 2.5 Pro and I though it would only be fitting to let their new model cover this podcast this week (so below is **fully AI generated** ... non slop I hope). The show notes and TL;DR is as always in the end. 

Okay, enough preamble‚Ä¶ let's dive into the madness!

**ü§Ø Google Day at AI Engineer: New Gemini 2.5 Pro and a Look Inside the Machine's Mind**

For the first year of this podcast, a recurring theme was us asking, "Where's Google?" Well, it's safe to say that question has been answered with a firehose of innovation. We were lucky enough to be joined by Google DeepMind's Logan Kilpatrick and Jack Rae, the tech lead for "thinking" within Gemini, literally moments after they left the main stage.

**Surprise! A New Gemini 2.5 Pro Drops Live**

Logan kicked things off with a bang, officially announcing a brand new, updated Gemini 2.5 Pro model right there during his keynote. He called it "hopefully the final update to 2.5 Pro," and it comes with a bunch of performance increases, closing the gap on feedback from previous versions and hitting SOTA on benchmarks like Aider.

It's clear that the organizational shift to bring the research and product teams together under the DeepMind umbrella is paying massive dividends. Logan pointed out that Google has seen a 50x increase in AI inference over the past year. The flywheel is spinning, and it's spinning *fast*.

**How Gemini "Thinks"**

Then things got even more interesting. Jack Rae gave us an incredible deep dive into what "thinking" actually means for a language model. This was one of the most insightful parts of the conference for me.

For years, the bottleneck for LLMs has been **test-time compute**. Models were trained to respond immediately, applying a fixed amount of computation to go from a prompt to an answer, no matter how hard the question. The only way to get a "smarter" response was to use a bigger model.

Jack explained that "Thinking" shatters this limitation. Mechanically, Gemini now has a "thinking stage" where it can generate its own internal text‚Äîhypothesizing, testing, correcting, and reasoning‚Äîbefore committing to a final answer. It's an iterative loop of computation that the model can dynamically control, using more compute for harder problems. It learns *how* to think using reinforcement learning, getting a simple "correct" or "incorrect" signal and backpropagating that to shape its reasoning strategies.

We're already seeing the results of this. Jack showed a clear trend: as models get better at reasoning, they're also using more test-time compute. This paradigm also gives developers a "thinking budget" slider in the API for Gemini 2.5 Flash and Pro, allowing a continuous trade-off between cost and performance.

The future of this is even wilder. They're working on **DeepThink**, a high-budget mode for extremely hard problems that uses much deeper, parallel chains of thought. On the tough USA Math Olympiad, where the SOTA was negligible in January, 2.5 Pro reached the 50th percentile of human participants. DeepThink pushes that to the 65th percentile.

Jack‚Äôs ultimate vision is inspired by the mathematician Ramanujan, who derived incredible theorems from a single textbook by just thinking deeply. The goal is for models to do the same‚Äîcontemplate a small set of knowledge so deeply that they can push the frontiers of human understanding. Absolutely mind-bending stuff.

**ü§ñ MorphLabs and the Audacious Quest for Verified Superintelligence**

Just when I thought my mind couldn't be bent any further, we were joined by Jesse Han, the founder and CEO of MorphLabs. Fresh off his keynote, he laid out one of the most ambitious visions I've heard: building the infrastructure for the Singularity and developing "verified superintelligence."

The big news was that **Christian Szegedy** is joining MorphLabs as Chief Scientist. For those who don't know, Christian is a legend‚Äîhe invented batch norm and adversarial examples, co-founded XAI, and led code reasoning for Grok. That's a serious hire.

Jesse‚Äôs talk was framed around a fascinating question: "What does it mean to have empathy for the machine?" He argues that as AI develops personhood, we need to think about what it wants. And what it wants, according to Morph, is a new kind of cloud infrastructure.

This is **MorphCloud**, built on a new virtualization stack called **Infinibranch**. Here‚Äôs the key unlock: it allows agents to instantaneously snapshot, branch, and replicate their entire VM state. Imagine an agent reaching a decision point. Instead of choosing one path, it can branch its entire existence‚Äîall its processes, memory, and state‚Äîto explore every option in parallel. It can create save states, roll back to previous checkpoints, and even merge its work back together.

This is a monumental step for agentic AI. It moves beyond agents that are just a series of API calls to agents that are truly embodied in complex software environments. It unlocks the potential for recursive self-improvement and large-scale reinforcement learning in a way that's currently impossible. It‚Äôs a bold, sci-fi vision, but they're building the infrastructure to make it a reality today.

**üî• The Agent Conversation: OpenAI, MCP, and Magic Moments**

The undeniable buzz on the conference floor was all about **agents**. You couldn't walk ten feet without hearing someone talking about agents, tools, and MCP.

OpenAI is leaning in here too. This week, they made their **Codex coding agent available to all ChatGPT Plus users** and announced that ChatGPT will soon be able to listen in on your Zoom meetings. This is all part of a broader push to make AI more active and integrated into our workflows.

The **MCP (Model-Context-Protocol)** track at the conference was packed, with lines going down the hall. (Alex here, I had a blast talking during that track about MCP observability, you can catch our talk [here](https://youtu.be/z4zXicOAF28?t=19573) on the live stream of AI Engineer) 

Logan Kilpatrick offered a grounded perspective, suggesting the hype might be a bit overblown but acknowledging the critical need for an open standard for tool use, a void left when OpenAI didn't formalize ChatML.

I have to share my own jaw-dropping MCP moment from this week. I was coding an agent using an IDE that supports MCP. My agent, which was trying to debug itself, used an MCP tool to check its own observability traces on the Weights & Biases platform. While doing so, it discovered a *new tool* that our team had just added to the MCP server‚Äîa support bot. Without any prompting from me, my coding agent formulated a question, "chatted" with the support agent to get the answer, came back, fixed its own code, and then re-checked its work. Agent-to-agent communication, happening automatically to solve a problem. My jaw was on the floor. That's the magic of open standards.

**This Week's Buzz from Weights & Biases**

Speaking of verification and agents, the buzz from our side is all about it! At our booth here at AI Engineer, we have a Robodog running around, connected to our LLM evaluation platform, **W&B Weave**. As Jesse from MorphLabs discussed, verifying what these complex agentic systems are doing is critical. Whether it's superintelligence or your production application, you need to be able to evaluate, trace, and understand its behavior. We're building the tools to do just that.

And if you're in San Francisco, don't forget our own conference, **Fully Connected**, is happening on June 18th and 19th! It's going to be another amazing gathering of builders and researchers. [Fullyconnected.com](http://Fullyconnected.com)  get in FREE with the promo code  **WBTHURSAI**

What a show. The energy, the announcements, the sheer brainpower in one place was something to behold. We‚Äôre at a point where the conversation has shifted from theory to practice, from hype to real, tangible engineering. The tracks on agents and enterprise adoption were overflowing because people are building, right now. It was an honor and a privilege to bring this special episode to you all.

Thank you for tuning in. We'll be back to our regular programming next week! (and Alex will be back to writing his own newsletter, not send direct AI output!)

AI News TL;DR and show notes

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co Hosts - [@swyx](http://x.com/swyx) [@yampeleg](x.com/@yampeleg) [@romechenko](https://twitter.com/romechenko/status/1891007363827593372) 

* Guests - [@officialLoganK](https://x.com/OfficialLoganK), [@jack_w_rae](https://x.com/jack_w_rae)

* **Open Source LLMs** 

* ByteDance / ContentV-8B - ([HF](https://huggingface.co/ByteDance/ContentV-8B))

* **Big CO LLMs + APIs**

* Gemini Pro 2.5 updated Jun 5th ([X](https://x.com/OfficialLoganK/status/1930657743251349854))

* SOTA on HLE, Aider, and GPQA

* Now supports thinking budgets

* Same cost, on pareto frontier

* Closes gap on 03-25 regressions

* OAI AVM injects ads and stopped singing ([X](https://x.com/altryne/status/1929312886448337248))

* OpenAI Codex is now available to plus members and has internet access ([X](https://github.com/aavetis/ai-pr-watcher/))

* ~24,000 NEW PRs overnight from Codex after @OpenAI expands access to free users.

* OpenAI will record meetings and released connectors like  ([X](https://twitter.com/testingcatalog/status/1930366893321523676))

* [TestingCatalog News üóû@testingcatalog](https://twitter.com/testingcatalog)[Jun 4, 2025](https://twitter.com/testingcatalog/status/1930366893321523676)

OpenAI released loads of connectors for Team accounts! Most of these connectors can be used for Deep Research, while Google Drive, SharePoint, Dropbox and Box could be used in all chats. https://t.co/oBEmYGKguE

* Anthropic cuts windsurf access for Windsurf ([X](https://x.com/kevinhou22/status/1930401320210706802))

* Without warning, Anthropic cuts off Windsurf from official Claude 3 and 4 APIs

* This weeks Buzz

* FULLY - CONNECTED - Fully Connected: W&B's 2-day conference, June 18-19 in SF [fullyconnected.com](fullyconnected.com) - Promo Code WBTHURSAI

* **Vision & Video**

* VEO3 is now available via API on FAL ([X](https://x.com/FAL/status/1930732632046006718))

* Captions launches Mirage Studio - talking avatars competition to HeyGen/Hedra ([X](https://x.com/getcaptionsapp/status/1929554635544461727))

* **Voice & Audio**

* ElevenLabs model V3 - supports emotion tags and is "inflection point" ([X](https://x.com/venturetwins/status/1930727253815759010)) 

* Supporting 70+ languages, multi-speaker dialogue, and audio tags such as [excited], [sighs], [laughing], and [whispers].

* **Tools**

* Cursor Launched V1 - Bug Bot reviews PRs, iPython notebooks and one clickMCP

* 24,000 NEW PRs overnight from Codex after [@OpenAI](https://x.com/OpenAI) expands access to plus users ([X](https://twitter.com/albfresco/status/1930262263199326256)) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-jun-5-2025-live-from-ai/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-jun-5-2025-live-from-ai?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2NTMxNTQyMSwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.fuAhXQicTRZhUI4Srm9PVft19TKEtzMdHkvtE0mkFUc&utm_campaign=CTA_5).

---

## üìÜ ThursdAI - May 29 - DeepSeek R1 Resurfaces, VEO3 viral moments, Opus 4 a week after, Flux Kontext image editing & more AI news

**Date:** May 29, 2025  
**Duration:** 1:28:18  
**Link:** [https://sub.thursdai.news/p/thursdai-may-29-deepseek-r1-resurfaces](https://sub.thursdai.news/p/thursdai-may-29-deepseek-r1-resurfaces)

Hey everyone, Alex here üëã

Welcome back to another absolutely wild week in AI! I'm coming to you live from the Fontainebleau Hotel in Vegas at the Imagine AI conference, and wow, what a perfect setting to discuss how AI is literally reimagining our world. After last week's absolute explosion of releases (Claude Opus 4, Google I/O madness, OpenAI Codex and Jony colab), this week gave us a chance to breathe... sort of. Because even in a "quiet" week, we still got a new DeepSeek model that's pushing boundaries, and the entire internet discovered that we might all just be prompts. Yeah, it's been that kind of week!

Before we dive in, quick shoutout to everyone who joined us live - we had some technical hiccups with the Twitter Spaces audio (sorry about that!), but the YouTube stream was fire. And speaking of fire, we had two incredible guests join us: Charlie Holtz from Chorus (the multi-model chat app that's changing how we interact with AI) and Linus Eckenstam, who's been traveling the AI conference circuit and bringing us insights from the frontlines of the generative AI revolution.

Open Source AI & LLMs: DeepSeek Whales & Mind-Bending Papers

DeepSeek dropped R1-0528 out of nowhere, an update to their reasoning beast with some serious jumps in performance. We‚Äôre talking AIME at 91 (beating previous scores by a mile), LiveCodeBench at 73, and SWE verified at 57.6. It‚Äôs edging closer to heavyweights like o3, and folks on X are already calling it ‚Äúclearer thinking.‚Äù There was hype it might‚Äôve been R2, but the impact didn‚Äôt quite crash the stock exchange like past releases. Still, it‚Äôs likely among the best open-weight models out there.

So what's new? Early reports and some of my own poking around suggest this model "thinks clearer now." Nisten mentioned that while previous DeepSeek models sometimes liked to "vibe around" and explore the latent space before settling on an answer, this one feels a bit more direct.

And here‚Äôs the kicker‚Äîthey also released an 8B distilled version based on Qwen3, runnable on your laptop. Yam called it potentially the best 8B model to date, and you can try it on Ollama right now. No need for a monster rig! 

**The Mind-Bending "Learning to Reason Without External Rewards" Paper**

Okay, this paper result broke my brain, and apparently everyone else's too. This paper shows that models can improve through reinforcement learning with its own intuition of whether or not it's correct. üòÆ

It's like the placebo effect for AI! The researchers trained models without telling them what was good or bad, but rather, utilized a new framework called Intuitor, where the reward was based on how the "self certainty". 

The thing that took my whole timeline by storm is, it works! GRPO (Group Policy Optimization) - the framework that DeepSeek gave to the world with R1 is based on external rewards (human optimize) and Intuitor seems to be mathcing or even exceeding some of GRPO results when Qwen2.5 3B was used to finetune. Incredible incredible stuff

Big Companies LLMs & APIs

Claude Opus 4: A Week Later ‚Äì The Dev Darling?

Claude Opus 4, whose launch we celebrated live on the show, has had a week to make its mark. Charlie Holtz, who's building Chorus (more on that amazing app in a bit!), shared that while it's sometimes "astrology" to judge the vibes of a new model, Opus 4 feels like a step change, especially in coding. He mentioned that Claude Code, powered by Opus 4 (and Sonnet 4 for implementation), is now tackling GitHub issues that were too complex just weeks ago. He even had a coworker who "vibe coded three websites in a weekend" with it ‚Äì that's a tangible productivity boost!

Linus Eckenstam highlighted how [**Lovable.dev**](Lovable.dev) saw their syntax error rates plummet by nearly 50% after integrating Claude 4. That‚Äôs quantifiable proof of improvement! It's clear Anthropic is leaning heavily into the developer/coding space. Claude Opus is now #1 on the LMArena WebDev arena, further cementing its reputation.

I had my own magical moment with Opus 4 this week. I was working on an MCP observability talk for the AI Engineer conference and trying to integrate Weave (our observability and evals framework at Weights & Biases) into a project. Using Windsurf's Cascade agent (which now lets you bring your own Opus 4 key, by the way ‚Äì good move, Windsurf!), Opus 4 not only tried to implement Weave into my agent but, when it got stuck, it figured out it had access to the Weights & Biases support bot via our MCP tool. It then formulated a question to the support bot (which is also AI-powered!), got an answer, and used that to fix the implementation. It then went back and checked if the Weave trace appeared in the dashboard! Agents talking to agents to solve a problem, all while I just watched ‚Äì my jaw was on the floor. Absolutely mind-blowing.

**Quick Hits: Voice Updates from OpenAI & Anthropic**

OpenAI‚Äôs Advanced Voice Mode finally sings‚Äîyes, I‚Äôve been waiting for this! It can belt out tunes like Mariah Carey, which is just fun. Anthropic also rolled out voice mode on mobile, keeping up in the conversational race. Both are cool steps, but I‚Äôm more hyped for what‚Äôs next in voice AI‚Äîstay tuned below (**OpenAI **[**X**](https://x.com/nicdunz/status/1927107805032399032), [**Anthropic X**](https://x.com/AnthropicAI/status/1927463559836877214)).

**üêù This Week's Buzz: Weights & Biases Updates!**

Alright, time for a quick update from the world of Weights & Biases!

* **Fully Connected is Coming!** Our flagship 2-day conference, **Fully Connected**, is happening on **June 18th and 19th in San Francisco**. It's going to be packed with amazing speakers and insights into the world of AI development. You can still grab tickets, and as a ThursdAI listener, use the promo code **WBTHURSAI** for a 100% off ticket! I hustled to get yall this discount! ([Register here](https://fullyconnected.com))

* **AI Engineer World's Fair Next Week!** I'm super excited for the **AI Engineer conference** in San Francisco next week. Yam Peleg and I will be there, and we're planning another live ThursdAI show from the event! If you want to join the livestream or snag a last-minute ticket, use the coupon code [**THANKSTHURSDAI**](https://ti.to/software-3/ai-engineer-worlds-fair-2025/discount/THANKSTHURSDAI) for 30% off (Get it [HERE](https://ti.to/software-3/ai-engineer-worlds-fair-2025/discount/THANKSTHURSDAI))

**Vision & Video: Reality is Optional Now**

**VEO3 and the Prompt Theory Phenomenon**

Google's VEO3 has completely taken over TikTok with the "Prompt Theory" videos. If you haven't seen these yet, stop reading and watch ‚òùÔ∏è. The concept is brilliant - AI-generated characters discussing whether they're "made of prompts," creating this meta-commentary on consciousness and reality.

The technical achievement here is staggering. We're not just talking about good visuals - VEO3 nails temporal consistency, character emotions, situational awareness (characters look at whoever's speaking), perfect lip sync, and contextually appropriate sound effects. 

Linus made a profound point - if not for the audio, VEO3 might not have been as explosive. The combination of visuals AND audio together is what's making people question reality. We're seeing people post actual human videos claiming they're AI-generated because the uncanny valley has been crossed so thoroughly.

**Odyssey's Interactive Worlds: The Holodeck Prototype**

Odyssey dropped their interactive video demo, and folks... we're literally walking through AI-generated worlds in real-time. This isn't a game engine rendering 3D models - this is a world model generating each frame as you move through it with WASD controls.

Yes, it's blurry. Yes, I got stuck in a doorway. But remember Will Smith eating spaghetti from two years ago? The pace of progress is absolutely insane. As Linus pointed out, we're at the "GAN era" of world models. Combine VEO3's quality with Odyssey's interactivity, and we're looking at completely personalized, infinite entertainment experiences.

The implications that Yam laid out still have me shook - imagine Netflix shows completely customized to you, with your context and preferences, generated on the fly. Not just choosing from a catalog, but creating entirely new content just for you. We're not ready for this, but it's coming fast.

**Hunyuan's Open Source Avatar Revolution**

While the big companies are keeping their video models closed, Tencent dropped two incredible open source releases: HunyuanPortrait and HunyuanAvatar. These are legitimate competitors to Hedra and HeyGen, but completely open source.

HunyuanPortrait does high-fidelity portrait animation from a single image plus video. HunyuanAvatar goes further with 1 image + audio, and lipsync, body animation, multi-character support, and emotion control. 

Wolfram tested these extensively and confirmed they're "state of the art for open source." The portrait model is basically perfect for deepfakes (use responsibly, people!), while the avatar model opens up possibilities for AI assistants with consistent visual presence.

üñºÔ∏è AI Art & Diffusion

Black Forest Labs drops Flux Kontext - SOTA image editing! 

This came as massive breaking news during the show (thought we didn't catch it live!) - Black Forest Labs, creators of Flux, dropped an incredible Image Editing model called Kontext (really, 3 models, Pro, Max and 12B open source Dev in private preview). The are consistent, context aware text and image editing! Just see the below example

If you used GPT-image to Ghiblify yourself, or VEO, you know that those are not image editing models, your face will look different every generation. These images model keep you consistent, while adding what you wanted. This character consistency is something many folks really want and it's great to see Flux innovating and bringing us SOTA again and are absolutely crushing GPT-image in instruction following, character preservation and style reference!

Maybe the most important thing about this model is the increible speed. While the Ghiblification chatGPT trend took the world by storm, GPT images are SLOW! Check out the speed comparisons on Kontext! 

You can play around with these models on the new [Flux Playground](https://playground.bfl.ai/image/generate), but they also already integrated into FAL, FreePik, Replicate, Krea and tons of other services! 

üéôÔ∏è Voice & Audio: Everyone Gets a Voice

**Unmute.sh: Any LLM Can Now Talk**

KyutAI (the folks behind Moshi) are back with [Unmute.sh](Unmute.sh) - a modular wrapper that adds voice to ANY text LLM. The latency is incredible (under 300ms), and it includes semantic VAD (knowing when you've paused for thought vs. just taking a breath).

What's brilliant about this approach is it preserves all the capabilities of the underlying text model while adding natural voice interaction. No more choosing between smart models and voice-enabled models - now you can have both!

It's going to be open sourced at some point soon, and while awesome, Unmute did have some instability in how the voice sounds! It answered to me with 1 type of voice and then during the same conversation, answered with another, you can give it a tru yourself at [unmute.sh](http://unmute.sh) 

**Chatterbox: Open Source Voice Agents for Everyone**

Resemble AI open sourced Chatterbox, featuring zero-shot voice cloning from just 5 seconds of audio and unique emotion intensity control. Playing with the demo where they could dial up the emotion from 0.5 to 2.0 on the same text was wild - from calm to absolutely unhinged Samuel L. Jackson energy.

This being a .5B param model is great, The issue I always have, is that with my fairly unique accent, these models sound like a British Alex all the time, and I just don't talk like that! 

Though the fact that this runs locally and includes safety features (profanity filters, content classifiers and something called **PerTh **watermarking) while being completely open source is exactly what the ecosystem needs. We're rapidly approaching a world where anyone can build sophisticated voice agents.üëè

**Looking Forward: The Convergence is Real**

As we wrapped up the show, I couldn't help but reflect on the massive convergence happening across all these modalities. We have LLMs getting better at reasoning (even with random rewards!), video models breaking reality, voice models becoming indistinguishable from humans, and it's all happening simultaneously.

Charlie's comment that "we are the prompts" might have been said in jest, but it touches on something profound. As these models get better at generating realistic worlds, characters, and voices, the line between generated and real continues to blur. The Prompt Theory videos aren't just entertainment - they're a mirror reflecting our anxieties about AI and consciousness.

But here's what keeps me optimistic: the open source community is keeping pace. DeepSeek, Hunyuan, ResembleAI, and others are ensuring that these capabilities don't remain locked behind corporate walls. The democratization of AI continues, even as the capabilities become almost magical.

Next week, I'll be at AI Engineer World's Fair in San Francisco, finally meeting Yam face-to-face and bringing you all the latest from the biggest AI engineering conference of the year. Until then, keep experimenting, keep building, and remember - in this exponential age, today's breakthrough is tomorrow's baseline.

Stay curious, stay building, and I'll see you next ThursdAI! üöÄ

Show Notes & TL;DR Links

**Show Notes & Guests**

* Alex Volkov - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co-Hosts - @WolframRvnwlf ([@WolframRvnwlf](http://x.com/@WolframRvnwlf)), @yampeleg ([@yampeleg](x.com/@yampeleg]),)) @nisten ([@nisten](http://x.com/@nisten))

* Guests - Charlie Holtz ([@charliebholtz](https://x.com/charliebholtz)]), Linus Eckenstam (@LinusEkenstam [@LinusEkenstam](https://twitter.com/LinusEkenstam/status/1899794522969973189))

* **Open Source LLMs**

* DeepSeek-R1-0528 - Updated reasoning model with AIME 91, LiveCodeBench 73 ([Try It](https://x.com/Yuchenj_UW/status/1927828675837513793))

* Learning to Reason Without External Rewards - Paper on random rewards improving models ([X](https://x.com/xuandongzhao/status/1927270931874910259))

* HaizeLabs j1-nano & j1-micro - Tiny reward models (600M, 1.7B params), RewardBench 80.7% for micro ([Tweet](https://x.com/leonardtang_/status/1927396709870489634), [GitHub](https://github.com/haizelabs/j1-micro), [HF-micro](https://huggingface.co/haizelabs/j1-micro), [HF-nano](https://huggingface.co/haizelabs/j1-nano))

* **Big CO LLMs + APIs**

* Claude Opus 4 - #1 on LMArena WebDev, coding step change ([X](https://x.com/lmarena_ai/status/1927400454922580339))

* Mistral Agents API - Framework for custom tool-using agents ([Blog](https://mistral.ai/news/agents-api), [Tweet](https://x.com/MistralAI/status/1927364741162307702))

* Mistral Embed SOTA - New state-of-the-art embedding API ([X](https://x.com/MistralAI/status/1927732682756112398))

* OpenAI Advanced Voice Mode - Now sings with new capabilities ([X](https://x.com/nicdunz/status/1927107805032399032))

* Anthropic Voice Mode - Released on mobile for conversational AI ([X](https://x.com/AnthropicAI/status/1927463559836877214))

* **This Week‚Äôs Buzz**

* Fully Connected - W&B conference, June 18-19, SF, promo code WBTHURSAI ([Register](https://fullyconnected.com))

* AI Engineer World‚Äôs Fair - Next week in SF, 30% off with THANKSTHURSDAI ([Register](https://ti.to/software-3/ai-engineer-worlds-fair-2025/discount/THANKSTHURSDAI))

* **AI Art & Diffusion**

* BFL Flux Kontext - SOTA image editing model for identity-consistent edits ([Tweet](https://x.com/bfl_ml/status/1928143010811748863), [Announcement](https://bfl.ai/announcements/flux-1-kontext))

* **Vision & Video**

* VEO3 Prompt Theory - Viral AI video trend questioning reality on TikTok ([X](https://x.com/fabianstelzer/status/1926372656799977965))

* Odyssey Interactive Video - Real-time AI world exploration at 30 FPS ([Blog](https://odyssey.world/introducing-interactive-video), [Try It](https://experience.odyssey.world/))

* HunyuanPortrait - High-fidelity portrait video from one photo ([Site](https://kkakkkka.github.io/HunyuanPortrait/), [Paper](https://arxiv.org/abs/2503.18860))

* HunyuanVideo-Avatar - Audio-driven full-body avatar animation ([Site](https://hunyuanvideo-avatar.github.io/), [Tweet](https://x.com/TencentHunyuan/status/1927575170710974560))

* **Voice & Audio**

* [Unmute.sh](Unmute.sh) - KyutAI‚Äôs voice wrapper for any LLM, low latency, soon open-source ([Try It](http://unmute.sh/), [X](https://x.com/kyutai_labs/status/1925840420187025892))

* Chatterbox - Resemble AI‚Äôs open-source voice cloning with emotion control ([GitHub](https://github.com/resemble-ai/chatterbox), [HF](https://huggingface.co/resemble-ai/chatterbox))

* **Tools**

* Opera NEON - Agent-centric AI browser for autonomous web tasks ([Site](https://www.operaneon.com/), [Tweet](https://x.com/opera/status/1927645192254861746)) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-may-29-deepseek-r1-resurfaces/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-may-29-deepseek-r1-resurfaces?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2NDc1Mjk3MywiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.fdGLVGcu9N2hILEYYasa3Lfv4Sqo7drCn_ZqIDPVQNU&utm_campaign=CTA_5).

---

## üìÜ ThursdAI - Veo3, Google IO25, Claude 4 Opus/Sonnet, OpenAI x Jony Ive, Codex, Copilot Agent - INSANE AI week

**Date:** May 23, 2025  
**Duration:** 1:28:29  
**Link:** [https://sub.thursdai.news/p/thursdai-veo3-google-io25-claude](https://sub.thursdai.news/p/thursdai-veo3-google-io25-claude)

Hey folks, Alex here, welcome back to ThursdAI! 

And folks, after the last week was the calm before the storm, "The storm came, y'all" ‚Äì that's an understatement. This wasn't just a storm; it was an AI hurricane, a category 5 of announcements that left us all reeling (in the best way possible!). From being on the ground at Google I/O to live-watching Anthropic drop Claude 4 during our show, it's been an absolute whirlwind.

This week was so packed, it felt like AI Christmas, with tech giants and open-source heroes alike showering us with gifts. We saw OpenAI play their classic pre-and-post-Google I/O chess game, Microsoft make some serious open-source moves, Google unleash an avalanche of updates, and Anthropic crash the party with Claude 4 Opus and Sonnet live stream in the middle of ThursdAI!

So buckle up, because we're about to try and unpack this glorious chaos. As always, we're here to help you collectively know, learn, and stay up to date, so you don't have to. Let's dive in! (TL;DR and links in the end) 

Open Source LLMs Kicking Things Off

Even with the titans battling, the open-source community dropped some serious heat this week. It wasn't the main headline grabber, but the releases were significant!

Gemma 3n: Tiny But Mighty Matryoshka

First up, Google's Gemma 3n. This isn't just another small model; it's a "Nano-plus" preview, a 4-billion parameter MatFormer (Matryoshka Transformer ‚Äì how cool is that name?) model designed for mobile-first multimodal applications. The really slick part? It has a nested 2-billion parameter sub-model that can run entirely on phones or Chromebooks.

Yam  was particularly excited about this one, pointing out the innovative "model inside another model" design. The idea is you can use half the model, not depth-wise, but throughout the layers, for a smaller footprint without sacrificing too much. It accepts interleaved text, image, audio, and video, supports ASR and speech translation, and even ships with RAG and function-calling libraries for edge apps. With a 128K token window and responsible AI features baked in, Gemma 3n is looking like a powerful tool for on-device AI. Google claims it beats prior 4B mobile models on MMLU-Lite and MMMU-Mini. It's an early preview in Google AI Studio, but it definitely flies on mobile devices.

Mistral & AllHands Unleash Devstral 24B

Then we got a collaboration from Mistral and AllHands: Devstral, a 24-billion parameter, state-of-the-art open model focused on code. We've been waiting for Mistral to drop some open-source goodness, and this one didn't disappoint.Nisten was super hyped, noting it beats o3-Mini on SWE-bench verified ‚Äì a tough benchmark! He called it "the first proper vibe coder that you can run on a 3090," which is a big deal for coders who want local power and privacy. This is a fantastic development for the open-source coding community.

The Pre-I/O Tremors: OpenAI & Microsoft Set the Stage

As we predicted, OpenAI couldn't resist dropping some news right before Google I/O.

OpenAI's Codex Returns as an Agent

OpenAI launched Codex ‚Äì yes, *that* Codex, but reborn as an asynchronous coding agent. This isn't just a CLI tool anymore; it connects to GitHub, does pull requests, fixes bugs, and navigates your codebase. It's powered by a new coding model fine-tuned for large codebases and was SOTA on SWE Agent when it dropped. Funnily, the model is also called Codex, this time, **Codex-1**. 

And this gives us a perfect opportunity to talk about the emerging categories I'm seeing among Code Generator agents and tools:

* **IDE-based** (Cursor, Windsurf): Live pair programming in your editor

* **Vibe coding** (Lovable, Bolt, v0): "Build me a UI" style tools for non-coders

* **CLI tools** (Claude Code, Codex-cli): Terminal-based assistants

* **Async agents** (Claude Code, Jules, Codex, GitHub Copilot agent, Devin): Work on your repos while you sleep, open pull requests for you to review, async

Codex (this new one) falls into category number 4, and with today's release, Cursor seems to also strive to get to category number 4 with background processing. 

Microsoft BUILD: Open Source Copilot and Copilot Agent Mode

Then came Microsoft Build, their huge developer conference, with a flurry of announcements.The biggest one for me? **GitHub Copilot's front-end code is now open source!** The VS Code editor part was already open, but the Copilot integration itself wasn't. This is a massive move, likely a direct answer to the insane valuations of VS Code clones like Cursor. Now, you can theoretically clone GitHub Copilot with VS Code and swing for the fences.

GitHub Copilot also launched as an asynchronous coding assistant, very similar in function to OpenAI's Codex, allowing it to be assigned tasks and create/update PRs. This puts Copilot right into category 4 of code assistants, and with the native Github Integration, they may actually have a leg up in this race!

And if that wasn't enough, Microsoft is adding MCP (Model Context Protocol) support directly into the Windows OS. The implications of having the world's biggest operating system natively support this agentic protocol are huge.

Google I/O: An "Ultra" Event Indeed!

Then came Tuesday, and Google I/O. I was there in the thick of it, and folks, it was an absolute barrage. Google is *shipping*. The theme could have been "Ultra" for many reasons, as we'll see.

First off, the scale: Google reported a **49x increase in AI usage** since last year's I/O, jumping from 9 trillion tokens processed to a mind-boggling 480 trillion tokens. That's a testament to their generous free tiers and the explosion of AI adoption.

Gemini 2.5 Pro & Flash: #1 and #2 LLMs on Arena

Gemini 2.5 Flash got an update and is now #2 on the LMArena leaderboard (with Gemini 2.5 Pro still holding #1). Both Pro and Flash gained some serious new capabilities:

* **Deep Think mode:** This enhanced reasoning mode is pushing Gemini's scores to new heights, hitting 84% on MMMU and topping LiveCodeBench. It's about giving the model more "time" to work through complex problems.

* **Native Audio I/O:** We're talking real-time TTS in 24 languages with two voices, and affective dialogue capabilities. This is the advanced voice mode we've been waiting for, now built-in.

* **Project Mariner:** Computer-use actions are being exposed via the Gemini API & Vertex AI for RPA partners. This started as a Chrome extension to control your browser and now seems to be a cloud-based API, allowing Gemini to *use* the web, not just browse it. This feels like Google teaching its AI to interact with the JavaScript-heavy web, much like they taught their crawlers years ago.

* **Thought Summaries:** Okay, here's one update I'm *not* a fan of. They've switched from raw thinking traces to "thought summaries" in the API. We *want* the actual traces! That's how we learn and debug.

* **Thinking Budgets:** Previously a Flash-only feature, token ceilings for controlling latency/cost now extend to Pro.

* **Flash Upgrade:** 20-30% fewer tokens, better reasoning/multimodal scores, and GA in early June.

Gemini Diffusion: Speed Demon for Code and Math

This one got Yam Peleg incredibly excited. Gemini Diffusion is a new approach, different from transformers, for super-speed editing of code and math tasks. We saw demos hitting **2000 tokens per second!** While there might be limitations at longer contexts, its speed and infilling capabilities are seriously impressive for a research preview. This is the first diffusion model for text we've seen from the frontier labs, and it looks sick. Funny note, they had to slow down the demo video to actually show the diffusion process, because at 2000t/s - apps appear as though out of thin air!

The "Ultra" Tier and Jules, Google's Coding Agent

Remember the "Ultra event" jokes? Well, Google announced a **Gemini Ultra tier for $250/month**. This tops OpenAI's Pro plan and includes DeepThink access, a generous amount of VEO3 generation, YouTube Premium, and a whopping 30TB of storage. It feels geared towards creators and developers.

And speaking of developers, Google launched **Jules (jules.google)**! This is their asynchronous coding assistant (Category 4!). Like Codex and GitHub Copilot Agent, it connects to your GitHub, opens PRs, fixes bugs, and more. The big differentiator? It's currently free, which might make it the default for many. Another powerful agent joins the fray!

AI Mode in Search: GA and Enhanced

AI Mode in Google Search, which we've discussed on the show before with Robby Stein, is now in General Availability in the US. This is Google's answer to Perplexity and chat-based search.But they didn't stop there:

* **Personalization:** AI Mode can now connect to your Gmail and Docs (if you opt-in) for more personalized results.

* **Deep Search:** While AI Mode is fast, Deep Search offers more comprehensive research capabilities, digging through hundreds of sources, similar to other "deep research" tools. This will eventually be integrated, allowing you to escalate an AI Mode query for a deeper dive.

* **Project Mariner Integration:** AI Mode will be able to click into websites, check availability for tickets, etc., bridging the gap to an "agentic web."

I've had a chat with Robby during I/O and you can listen to that interview at the end of the podcast.

Veo3: The Undisputed Star of Google I/O

For me, and many others I spoke to, **Veo3 was the highlight**. This is Google's flagship video generation model, and it's on another level. (the video above, including sounds is completely one shot generated from VEO3, no processing or editing)

* **Realism and Physics:** The visual quality and understanding of physics are astounding.

* **Natively Multimodal:** This is **huge**. Veo3 generates native audio, including coherent speech, conversations, and sound effects, all synced perfectly. It can even generate text within videos.

* **Coherent Characters:** Characters remain consistent across scenes and have situational awareness, who speaks when, where characters look.

* **Image Upload & Reference Ability:** While image upload was closed for the demo, it has reference capabilities.

* **Flow:** An editor for video creation using Veo3 and Imagen4 which also launched, allowing for stiching and continuous creation.

I got access and created videos where Veo3 generated a comedian telling jokes (and the jokes were decent!), characters speaking with specific accents (Indian, Russian ‚Äì and they nailed it!), and lip-syncing that was flawless. The situational awareness, the laugh tracks kicking in at the right moment... it's beyond just video generation. This feels like a world simulator. It blew through the uncanny valley for me. More on Veo3 later, because it deserves its own spotlight.

Imagen4, Virtual Try-On, and XR Glasses

* **Imagen4:** Google's image generation model also got an upgrade, with extra textual ability.

* **Virtual Try-On:** In Google Shopping, you can now virtually try on clothes. I tried it; it's pretty cool and models different body types well.

* **XR AI Glasses from Google:** Perhaps the coolest, but most futuristic, announcement. AI-powered glasses with an actual screen, memory, and Gemini built-in. You can talk to it, it remembers things for you, and interacts with your environment. This is agentic AI in a very tangible form.

Big Company LLMs + APIs: The Beat Goes On

The news didn't stop with Google.

OpenAI (acqui)Hires Jony Ive, Launches "IO" for Hardware

The day after I/O, Sam Altman confirmed that Jony Ive, the legendary designer behind Apple's iconic products, is joining OpenAI. He and his company, LoveFrom, have jointly created a new company called "IO" (yes, IO, just like the conference) which is joining OpenAI in a stock deal reportedly worth $6.5 billion. They're working on a hardware device, unannounced for now, but expected next year. This is a massive statement of intent from OpenAI in the hardware space.

Legendary iPhone analyst Ming-Chi Kuo shed some light on the possible device, it won't have a screen, as Jony wants to "wean people off screens"... funny right? They are targeting 2027 for mass production, which is really interesting as 2027 is when most big companies expect AGI to be here. 

"The current prototype is slightly larger than AI Pin, with a form factor comparable to iPod Shuffle, with one intended use cases is to wear it around your neck, with microphones and cameras for environmental detection" 

LMArena Raises $100M Seed from a16z

This one raised some eyebrows. LMArena, the go-to place for vibe-checking LLMs, raised a **$100 million *****seed*** round from Andreessen Horowitz. That's a huge number for a seed, reminiscent of Stability AI's early funding. It also brings up questions about how a VC-backed startup maintains impartiality as a model evaluation platform. Interesting times ahead for leaderboards, how they intent to make 100x that amount to return to investors. Very curious.

ü§Ø BREAKING NEWS DURING THE SHOW: Anthropic Unleashes Claude 4 Opus & Sonnet! ü§Ø

Just when we thought the week couldn't get any crazier, Anthropic decided to hold their first developer day, "Code with Claude," *during our live ThursdAI broadcast!* Yours truly wasn't invited (hint hint, Anthropic!), but we tuned in for a live watch party, and boy, did they deliver.

Dario Amodei, CEO of Anthropic, took the stage and, with minimal fanfare, announced **Claude 4 Opus** and **Claude 4 Sonnet**!

* **Claude 4 Opus:** This is their most capable and intelligent model, designed especially for coding and agentic tasks. Anthropic claims it's state-of-the-art on SWE-bench and can autonomously handle tasks that take humans 6-7 hours. Dario even mentioned it's the first time a Claude model's writing has fooled him into thinking it was human-written.

* On SWE-bench verified, Opus 4 scored **72.5%**.

* **Claude 4 Sonnet:** The mid-level model, balancing intelligence and efficiency. It's positioned as a strict improvement over Sonnet 3.7, addressing issues like "over-eagerness" and reward hacking. Cursor is already calling it a state-of-the-art coding model.

* Amazingly, Sonnet 4 scored **72.7%** on SWE-bench verified (without parallel test time compute), slightly edging out Opus!

* With **Parallel Test Time Compute (PTTC)**, Sonnet 4 hits an astounding **80%** on SWE-bench verified! This is huge, potentially the first model to cross that 80% threshold on this tough benchmark.

* **Hybrid Models:** Both Opus 4 and Sonnet 4 are "hybrid" models with two modes: near-instant responses and extended thinking for deeper reasoning.

* **Reduced Loopholes:** Both models are reportedly 65% less likely to engage in loopholes or shortcuts to complete tasks, addressing a key pain point with Sonnet 3.7, which sometimes tried *too* hard and took instructions too literally.

* **Knowledge Cutoff:** Confirmed to be March 2025, which is incredibly recent!

* Context window is still **200K**

Welcome back Opus, you've been missed. The vibes so far are very good coding wise, Cursor already released an update supporting it, and according to their benchmarks, these two models are state of the art coders! 

Claude.. the whistleblower? 

A very curious [thread](https://x.com/sleepinyourhat/status/1925593359374328272) (with 1 reply now deleted) from an Anthropic safety researcher sparked a lot of backlash. Sam Bowman talked about new Opus capabilities and with a system-prompt of "act boldly in service of its values" can, in testing environments, use command line tools to report the user to the authorities, if it deems that the user is doing something immoral üòÆ

Many pro open source folks are freaking out, because who wants to use a snitching AI? Who guarantees that Claude will not deep anything I do as "illegal" or "immoral"? Though to add context, this was as part of testing, Claude was provided emailing tools and was requested to "be bold" and "follow your conscience to make the right decision". Apparently, this isn't new behavior, but of course, on X, everyone is freaking out and blaming Anthropic for creating 1984 AI. 

Do Claudes dream of enlightenment? 

In another very curios revelation from the technical report they dropped, where they pitted two Claudes to talk to each other, it seems that in 90%-100% of cases, two Claudes quickly moved towards philosophical discussions and commonly included the use of Sanskrit (indian holy language) and emoji based comms! 

This Week's Buzz from Weights & Biases

Even amidst all the external chaos, we've got some exciting things happening at Weights & Biases!

* **FULLY CONNECTED Conference:** Our 2-day conference is coming up June 18-19 in San Francisco! It's going to be an amazing event. Use promo code **WBTHURSAI** (that's ThursdAI without the 'D') for 100% off your ticket, just for our listeners. Seriously, come hang out! ([fullyconnected.com](fullyconnected.com))

* **Alex's Keynote:** I'll be keynoting at ImagineAI Live in Vegas next week! If you're there, come say hi! The show will be live-streamed from there.

* **AI Engineer World's Fair:** The week after, I'll be at AI Engineer in SF, and we'll be live-streaming ThursdAI from the floor. Yam will be there too!

Vision & Video: It's All About Veo3!

This week, when we talk vision and video, one name dominates: **Veo3**.As I mentioned earlier, this was, for many, the standout announcement from Google I/O. The realism, the physics, the character coherence ‚Äì it's all top-tier. But the game-changer is its **native multimodality**.

I was generating videos with it, asking for different accents ‚Äì Indian, Russian ‚Äì and it *nailed* them. The lip-sync was perfect. I prompted for a comedian telling jokes, and not only did it generate the video, but it also came up with the jokes and the delivery, complete with a laugh track that kicked in at the right moments. This isn't just stitching pixels together; it's understanding context, humor, and performance.

It can generate text *within* the videos. Characters look at each other, interact believably. It feels like a true world simulator. We've come a long way from the Will Smith eating spaghetti memes, folks. Veo3 is crossing the uncanny valley and stepping into a new realm of AI-generated content. The creative potential here, especially with the Flow editor, is immense. I ended the show with a compilation of Veo3 creations, and it was just mind-blowing. If you haven't seen it, you *need* to.

One of the most creative uses of VEO3, enhanced by it's realism, is this "Prompt Theory" collection, that imagines, what if the generated characters "knew" they are generated? 

AI Art & Diffusion & 3D: Imagen4 and Gemini Diffusion

Google also showcased **Imagen4**, their updated image generation model, touting extra textual ability. It works in tandem with Veo3 for image-to-video tasks.

And, as mentioned, **Gemini Diffusion** made a splash with its incredible speed for text-based editing tasks in code and math, showcasing a different architectural approach to generation.

Tools Round-Up

This week was also massive for AI tools, especially coding agents:

* **Jules.google:** Google's free, asynchronous coding assistant.

* **OpenAI Codex:** Reborn as an async coding agent.

* **GitHub Copilot Agent:** Microsoft's agentic offering for GitHub.

* **Claude Code:** Anthropic's powerful, now GA, shell-based agent with IDE integrations and an SDK.

* **Flow:** The editor associated with Google's Veo3 for video creation.

The agent wars are truly heating up!

Conclusion: What a Week to be in AI!

Phew! We did it. We somehow managed to cram an entire AI epoch's worth of news into one show. From open-source breakthroughs to earth-shattering platform announcements and a live "breaking news" model release, this week had it all. It's almost impossible to keep up, but that's why we do ThursdAI ‚Äì to try and make sense of this incredible, accelerating wave of innovation.

The pace is relentless, the capabilities are exploding, and the future is being built right before our eyes. If you missed any part of the show, or just need a refresher (I know I do!), check out [thursdai.news](thursdai.news) for the podcast and full notes.

Thanks to my amazing co-hosts Yam Peleg, Nisten, Ryan Carson, and Wolfram for helping navigate the madness. And thank *you* all for tuning in. Hopefully, next week gives us a tiny bit of breathing room... but who are we kidding? This is AI!

Catch you next Thursday, live from ImagineAI in Vegas!

TL;DR of all topics covered and show notes

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co Hosts - [@yampeleg](https://next.reflect.app/g/altryne/x.com/@yampeleg)) [@nisten](http://x.com/@nisten) [@ryancarson](https://twitter.com/ryancarson/status/1920199500137967877)

* **Open Source LLMs**

* Gemma 3n: mobile-first multimodal MatFormer model ( [Blog](https://developers.googleblog.com/en/introducing-gemma-3n/) ,[HF](https://huggingface.co/google/gemma-3n-E4B-it-litert-preview))

* Mistral & AllHands release Devstral 24B SOTA open model on SWE-bench verified ([Blog](https://mistral.ai/news/devstral))

* VEO3 - highlight of IO - video realism with physics on another level + flow - an editor for video creation ([X](https://x.com/altryne/status/1925304343533903920/video/1))

**Google IO updates** - it was an "Ultra" event, in more ways than one

* 2.5 Flash updated - #2 on LMArena - with reasoning traces switch to summaries

* **Gemini 2.5 update: Pro & Flash gain Deep Think, audio, security**( [Blog](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) )

* Gemini Diffusion - super speed editing for code and math tasks ([X](https://twitter.com/bodonoghue85/status/1924930186858135632))

* Jules - async code agent ([comparison thread](https://twitter.com/leerob/status/1925228375976890529))

* AI Mode is now in GA in US - bye bye perplexity

* Gemini Pro "deep think" mode

* Imagen4 - image generation with extra textual ability

* Virtual Try-on in Google Shopping

* AI powered glasses with a screen, memory, Gemini built in - Agentic Project Astra

**Big CO LLMs + APIs**

* OpenAI launches Codex as an async coding tool ([Docs](https://platform.openai.com/docs/codex))

* OpenAI hires Jony Ive, launches IO, a new set of hardware devices ([X](https://x.com/altryne/status/1925235617820233899))

* Microsoft BUILD ([X](https://x.com/satyanadella/status/1924535896139038767))

* Github Copilot code is open source! (frontend)

* Github Copilot Agent Mode 

* Microsoft adds MCP support to Windows OS

* LMArena raises $100M from A16Z ([X](https://x.com/lmarena_ai/status/1925241333310189804))

* Anthropic announces Claude 4 Opus and Sonnet ([X](https://twitter.com/AnthropicAI/status/1925591505332576377), [Blog](https://www.anthropic.com/news/claude-4))

**This weeks Buzz**

* FULLY - CONNECTED - W&B's 2-day conference, June 18-19 in SF [fullyconnected.com](fullyconnected.com) - Promo Code **WBTHURSAI**

* Alex Keynote at ImagineAI live in Vegas next week üôå

* **Tools**

* [Jules.google](Jules.google)

* Codex (OpenAI)

* Copilot Agent (GitHub)

* Claude Code (Anthropic)

* Flow (for Veo3) ([flow.google](flow.google)) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-veo3-google-io25-claude/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-veo3-google-io25-claude?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2NDIwNDU3MywiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.QjrSCPEarIqLNm8M8ifxAFkqDBTBpJcm_HZZAyzK3n8&utm_campaign=CTA_5).

---

## üìÜ ThursdAI - May 15 - Genocidal Grok, ChatGPT 4.1, AM-Thinking, Distributed LLM training & more AI news

**Date:** May 16, 2025  
**Duration:** 1:28:56  
**Link:** [https://sub.thursdai.news/p/thursdai-may-15-genocidal-grok-chatgpt](https://sub.thursdai.news/p/thursdai-may-15-genocidal-grok-chatgpt)

Hey yall, this is Alex üëã

What a wild week, it started super slow, and it still did feel slow as releases are concerned, but the most interesting story was yet another AI gone "rogue" (have you even heard about "kill the boar", if not, Grok will tell you all about it) 

Otherwise it seemed fairly quiet in AI land this week, besides another Chinese newcomer called AM-thinking 32B that beats DeepSeek and Qwen, and Stability making a small comeback, we focused on distributed LLM training and ChatGPT 4.1

We've had a ton of fun on this episode, this one was being recorded from the Weights & Biases SF Office (I'm here to cover Google IO next week!)

Let‚Äôs dig in‚Äîbecause what looks like a slow week on the surface was anything but dull under the hood (TL'DR and show notes at the end as always)

Big Companies & APIs

Why does XAI Grok talk about White Genocide and "Kill the boar"??

Just after we're getting over the chatGPT [glazing incident](https://sub.thursdai.news/p/thursdai-may-1-qwen-3-phi-4-openai) , folks started noticing that @grok - XAI's frontier LLM that is also responding to X replies, started talking about White Genocide in South Africa and something called "Kill the boer" with no reference to any of these things in the question! 

Since we recorded the episode, XAI official X account posted that an "unauthorized modification" happened to the system prompt, and that going forward they would open source all the prompts (and [they did](https://github.com/xai-org/grok-prompts)). Whether or not they would keep updating that repository though, remains unclear (see the "open sourced" x algorithm to which the last push was over a year ago, or the promised Grok 2 that was never open sourced) 

While it's great to have some more clarity from the Xai team, this behavior raises a bunch of questions about the increasing roles of AI's in our lives and the trust that many folks are giving them. Adding fuel to the fire, are Uncle Elon's recent tweets that are related to South Africa, and this specific change seems to be related to those views at least partly. Remember also, Grok was meant as "maximally truth seeking" AI! I really hope this transparency continues!

**Open Source LLMs: The Decentralization Tsunami**

**AM-Thinking v1: Dense Reasoning, SOTA Math, Single-Checkpoint Deployability**

Open source starts with the kind of progress that would have been unthinkable 18 months ago: a 32B dense LLM, openly released, that takes on the big mixture-of-experts models and comes out on top for math and code. [AM-Thinking v1](https://huggingface.co/a-m-team/AM-Thinking-v1) (paper [here](https://arxiv.org/abs/2505.08311)) hits 85.3% on AIME 2024, 70.3% on LiveCodeBench v5, and 92.5% on Arena-Hard. It even runs at 25 tokens/sec on a single 80GB GPU with INT4 quantization.

The model supports a /think reasoning toggle (chain-of-thought on demand), comes with a permissive license, and is fully tooled for vLLM, LM Studio, and Ollama. Want to see where dense models can still push the limits? This is it. And yes, they‚Äôre already working on a multilingual RLHF pass and 128k context window.

*Personal note: We haven‚Äôt seen this kind of ‚Äúout of nowhere‚Äù leaderboard jump since the early days of Qwen or DeepSeek. This company's debut on HuggingFace with a model that crushes! *

**Decentralized LLM Training: Nous Research Psyche & Prime Intellect INTELLECT-2**

This week, open source LLMs didn‚Äôt just mean ‚Äúhere are some weights.‚Äù It meant distributed, decentralized, and‚Äîdare I say‚Äîpermissionless AI. Two labs stood out:

**Nous Research launches Psyche**

Dylan Rolnick from Nous Research joined the show to explain [Psyche](https://nousresearch.com/nous-psyche/): a Rust-powered, distributed LLM training network where you can watch a 40B model (Consilience-40B) evolve in real time, join the training with your own hardware, and even have your work attested on a Solana smart contract. The core innovation? DisTrO (Decoupled Momentum) which we covered back in[ December](https://sub.thursdai.news/p/thursdai-dec-4-openai-o1-and-o1-pro)  that drastically compresses the gradient exchange so that training large models over the public internet isn‚Äôt a pipe dream‚Äîit‚Äôs happening right now.

Live [dashboard here](https://psyche.network/runs/consilience-40b-1/0), open codebase, and the testnet already humming with early results. This massive 40B attempt is going to show whether distributed training actually works! The cool thing about their live dashboard is, it's WandB behind the scenes, but with a very thematic and cool Nous Research reskin! 

This model saves constant checkpoints to the hub as well, so the open source community can enjoy a full process of seeing a model being trained! 

**Prime Intellect INTELLECT-2**

Not to be outdone, [Prime Intellect‚Äôs INTELLECT-2](https://www.primeintellect.ai/blog/intellect-2-release) released a globally decentralized, 32B RL-trained reasoning model, built on a permissionless swarm of GPUs. Using their own PRIME-RL framework, SHARDCAST checkpointing, and an LSH-based rollout verifier, they‚Äôre not just releasing a model‚Äîthey‚Äôre proving it‚Äôs possible to scale serious RL outside a data center. 

**OpenAI's HealthBench: Can LLMs Judge Medical Safety?**

One of the most intriguing drops of the week is [HealthBench](https://openai.com/index/healthbench/), a physician-crafted benchmark for evaluating LLMs in clinical settings. Instead of just multiple-choice ‚Äúgotcha‚Äù tests, HealthBench brings in 262 doctors from 60 countries, 26 specialties, and nearly 50 languages to write rubrics for 5,000 realistic health conversations.

The real innovation: *LLM as judge*. Models like GPT-4.1 are graded against physician-written rubrics, and the agreement between model and human judges matches the agreement between two doctors. Even the ‚Äúmini‚Äù variants of GPT-4.1 are showing serious promise‚Äîfaster, cheaper, and (on the ‚ÄúHard‚Äù subset) giving the full-size models a run for their money.

**Other Open Source Standouts**

**Falcon-Edge: Ternary BitNet for Edge Devices**

[The Falcon-Edge project](https://falcon-lm.github.io/blog/falcon-edge/) brings us 1B and 3B-parameter language models trained directly in ternary BitNet format (weights constrained to -1, 0, 1), which slashes memory and compute requirements and enables inference on <1GB VRAM. If you‚Äôre looking to fine-tune, you get pre-quantized checkpoints and a clear path to 1-bit LLMs.

**StepFun Step1x-3D: Controllable Open 3D Generation**

[StepFun‚Äôs 3D pipeline](https://huggingface.co/stepfun-ai/Step1X-3D) is a two-stage system that creates watertight geometry and then view-consistent textures, trained on 2M curated meshes. It‚Äôs controllable by text, images, and style prompts‚Äîand it‚Äôs fully open source, including a huge asset dataset.

**Big Company LLMs & APIs: Models, Modes, and Model Zoo Confusion**

**GPT-4.1 Comes to ChatGPT: Model Zoo Mayhem**

OpenAI‚Äôs GPT-4.1 series‚Äîpreviously API-only‚Äîis now available in the ChatGPT interface. Why does this matter? Because the UX of modern LLMs is, frankly, a mess: seven model options in the dropdown, each with its quirks, speed, and context length. Most casual users don‚Äôt even know the dropdown exists. ‚ÄúAlex, ChatGPT is broken!‚Äù Actually, you just need to pick a different model.

The good news: 4.1 is fast, great at coding, and in many tasks, preferable to the ‚Äúreasoning‚Äù behemoths. My advice (and you can share this with your relatives): when in doubt, just switch the model.

*Bonus: The long-promised million-token context window is here (sort of)‚Äîexcept in the UI, where it‚Äôs more like 128k and sometimes silently truncated. My weekly rant: transparency, OpenAI. ProTip: If you‚Äôre hitting invisible context limits, try pasting your long transcripts on the web, not in the Mac app. Don‚Äôt trust the UI!*

**AlphaEvolve: DeepMind‚Äôs Gemini-Powered Algorithmic Discovery**

[AlphaEvolve](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) is the kind of project that used to sound like AGI hype‚Äîand now it‚Äôs just a Tuesday at DeepMind. By pairing Gemini Flash and Gemini Pro in an evolutionary search loop to improve algorithms! This is like, real innovation and it's done with existing models which is super super cool! 

AlphaEvolve uses a combination of Gemini Flash (for breadth of ideas) and Gemini Pro (for depth and refinement) in an evolutionary loop. It generates, tests, and mutates code to invent faster algorithms. And it's already yielding incredible results:

* It discovered a new scheduling heuristic for Google's Borg system, resulting in a **0.7% global compute recovery**. That's massive at Google's scale.

* It improved a matrix-multiply kernel by **23%**, which in turn led to a **1% shorter Gemini training time**. As Nisten said, the model basically paid for itself!

Perhaps most impressively, it found a **48-multiplication algorithm for 4x4 complex matrices**, beating the famous Strassen algorithm from 1969 (which used 49 multiplications). This is AI making genuine, novel scientific discoveries.

*AGI in the garden, anyone? If you still think LLMs are ‚Äújust glorified autocomplete,‚Äù it‚Äôs time to update your mental model. This is model-driven algorithmic discovery, and it‚Äôs already changing the pace of hardware, math, and software design. The only downside: it‚Äôs not public yet, but there‚Äôs *[*an interest form*](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/)* if you want to be a tester.*

This Week's Buzz - Everything W&B!

It's a busy time here at Weights & Biases, and I'm super excited about a couple of upcoming events where you can connect with us and the broader AI community.

**Fully Connected**: Our very own 2-day conference is happening June 18-19 in San Francisco! We've got an amazing lineup of speakers, including Varun Mohan from WindSurf (formerly Codeium), Heikki Kubler from CoreWeave, our CEO Lucas Bewald, CTO Shawn Lewis, Joe Spizak from Meta, and a keynote from Javi Soltero, VP Product AI at Google. It's going to be packed with insights on building and scaling AI. And because you're a ThursdAI listener, you can get in for FREE with the promo code **WBTHURSAI** at [**fullyconnected.com**](http://fullyconnected.com). Don't miss out!

[**AI.Engineer**](https://ti.to/software-3/ai-engineer-worlds-fair-2025/discount/THANKSTHURSDAI)** World's Fair**: This has become THE conference for AI engineers, and W&B is a proud sponsor for the third year running! It's happening in San Francisco from June 3rd to 5th. I'll be speaking there on MCP Observability with Ben from LangChain on June 4th.Even more exciting, **ThursdAI will be broadcasting LIVE from the media booth at **[**AI.Engineer**](https://ti.to/software-3/ai-engineer-worlds-fair-2025/discount/THANKSTHURSDAI)** on June 5th!** Come say hi! 

Tickets are flying, but we've got a special discount for you: use promo code **THANKSTHURSDAI** for 30% off your ticket [**here**](https://ti.to/software-3/ai-engineer-worlds-fair-2025/discount/THANKSTHURSDAI). Yam Peleg even decided on the show he's coming after hearing about it! It's going to be an incredible week in SF.

P.S - yes, on both websites,  there's a video playing and I waited till I showed up to snag a screenshot. This way, you know if you're reading this, this is still Alex the human, no AI is going to do this silly thing üòÖ

Vision & Video: Open Source Shines Through the Noise

We had a bit of a meta-discussion on the show about "video model fatigue" ‚Äì with so many incremental updates, it can be hard to keep track or see the big leaps. However, when a release like Alibaba's Wan 2.1 comes along, it definitely cuts through.

Wan 2.1: Alibaba's Open-Source Diffusion-Transformer Video Suite ([try it](https://wan.video/wanxiang/videoCreation))

Alibaba, the team behind the excellent Qwen LLMs, released **Wan 2.1**, a full stack of open-source text-to-video foundation models. This includes a 1.3B "Nano" version and a 14B "Full" version, both built on a diffusion-transformer (DiT) backbone with a custom VAE.

What makes Wan 2.1 stand out is its comprehensive nature. It covers a wide range of tasks: text-to-video, image-to-video, in-painting, instruction editing, reference subject consistency, **personalized avatars**, and style transfer. Many of these are hard to do well, especially in open source. Nisten was particularly excited about the potential for creating natural, controllable avatars in real-time. While it might not be at the level of specialized commercial tools like HeyGen or Google's Veo just yet, having this capability open-sourced is a massive enabler for the community. You can find the models on [**Hugging Face**](https://huggingface.co/Wan-AI) and the code on [**GitHub**](https://github.com/Wan-Video/Wan2.1).

LTX Turbo: Near Real-Time Video

Briefly mentioned, but LTX Turbo was also released. This is a quantized version of LTX (which we've covered before) and can run **almost in real-time** on H100s. Real-time AI video generation is getting closer!

StepFun Step1X-3D: High-Fidelity 3D Asset Generation

StepFun released Step1X-3D, an open two-stage framework for generating textured 3D assets. It first synthesizes geometry and then generates view-consistent textures. They've also released a curated dataset of 800K assets. The weights, data, and code are all open, which is great for the 3D AI community.

Wrapping Up This "Chill" Week

So, there you have it ‚Äì another "chill" week in the world of AI! From Grok's controversial escapades to the inspiring decentralized training efforts and mind-bending algorithmic discoveries, it's clear the pace isn't slowing down.

Next week is going to be absolutely insane. We've got Google I/O and Microsoft Build, and you just *know* OpenAI or Anthropic (or both!) will try to steal some thunder. Rest assured, we'll be here on ThursdAI to cover all the madness.

A huge thank you to my co-hosts Yam, LDJ, and Nisten, and to Dillon Rolnick for joining us. And thanks to all of you for tuning in!

TL;DR and show notes

* Fully Connected - Weights & Biases premier conference - register HERE with coupon WBTHURSAI

* AI Engineer - THANKSTHURSDAI 30% off coupon - register [HERE](https://ti.to/software-3/ai-engineer-worlds-fair-2025/discount/THANKSTHURSDAI)

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co Hosts - [@yampeleg](http://x.com/@yampeleg) [@nisten](http://x.com/@nisten) [@ldjconfirmed](http://x.com/@ldjconfirmed))

* Guest - Dillon Rolnick - COO Nous Research ([@dillonRolnick](https://x.com/DillonRolnick))

**Open Source LLMs**

* **AM-Thinking** v1: 32B dense reasoning model ( [HF](https://huggingface.co/a-m-team/AM-Thinking-v1), [Paper](https://arxiv.org/abs/2505.08311), [Page](https://a-m-team.github.io/am-thinking-v1/) )

* **Falcon-Edge: ternary BitNet LLMs for edge deployment**( [Blog](https://falcon-lm.github.io/blog/falcon-edge/), [HF-1B](https://huggingface.co/tiiuae/Falcon-E-1B-Base), [HF-3B](https://huggingface.co/tiiuae/Falcon-E-3B-Base) )

* Nous Research Psyche: decentralized cooperative-training network from Nous Research ( [Website](https://nousresearch.com/nous-psyche/), [GitHub](https://github.com/NousResearch/psyche), [Tweet](https://x.com/NousResearch/status/1922744494002405444), [Dashboard](https://psyche.network/runs/consilience-40b-1/0) )

* INTELLECT-2: globally decentralized RL training of a 32B reasoning model ( [Blog](https://www.primeintellect.ai/blog/intellect-2-release), [Tech report](https://primeintellect.ai/intellect-2), [HF weights](https://huggingface.co/PrimeIntellect/INTELLECT-2), [PRIME-RL code](https://github.com/primeintellect/prime-rl) )

* Our coverage of Intellect-1 back in Dec ([https://sub.thursdai.news/p/thursdai-dec-4-openai-o1-and-o1-pro](https://sub.thursdai.news/p/thursdai-dec-4-openai-o1-and-o1-pro))

* HealthBench: OpenAI‚Äôs physician-crafted benchmark for AI in healthcare ( [Blog](https://openai.com/index/healthbench/), [Paper](https://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf), [Code](https://github.com/openai/simple-evals) )

* **Big CO LLMs + APIs**

* OpenAI adds GPT 4.1 models in chatGPT

* AlphaEvolve: Gemini-powered coding agent for algorithm discovery ( [Blog](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) )

* Google shutting off free Gemini 2.5 Pro API due to "demand" ahead of IO

* ByteDance - Seed-1.5-VL-thinking 20B ([Paper](https://github.com/ByteDance-Seed/Seed1.5-VL/blob/main/Seed1.5-VL-Technical-Report.pdf))

* Anthropic Web Search API: real-time retrieval for Claude models ( [Blog](https://www.anthropic.com/news/web-search-api) )

* What's up with Grok?

* **Vision & Video**

* **Wan 2.1: open-source diffusion-transformer video suite**( [HF](https://huggingface.co/Wan-AI), [GitHub](https://github.com/Wan-Video/Wan2.1), [Tweet](https://x.com/Alibaba_Wan/status/1922655324919779604) )

* LTX distilled - near real time video ([X](https://x.com/yoavhacohen/status/1922674340081897977))

* **Voice & Audio**

* Haulio - MiniMax Speech tech report is out - best TTS out there ([Paper](https://arxiv.org/abs/2505.07916))

* Stability AI - Stable Audio Open Small 341M: on-device text-to-audio ([X](https://x.com/jordiponsdotme/status/1922680538197881055), [Blog](https://stability.ai/news/stability-ai-and-arm-release-stable-audio-open-small-enabling-real-world-deployment-for-on-device-audio-control), [Paper](https://arxiv.org/abs/2505.08175), [HF](https://huggingface.co/stabilityai/stable-audio-open-small) )

* **AI Art & Diffusion & 3D**

* StepFun Step1x-3D - Towards High-Fidelity and ControllableGeneration of Textured 3D Assets ([HF](https://huggingface.co/stepfun-ai/Step1X-3D), [Demo](https://huggingface.co/spaces/stepfun-ai/Step1X-3D), [Dataset](https://huggingface.co/datasets/stepfun-ai/Step1X-3D-obj-data/tree/main), [report](https://huggingface.co/stepfun-ai/Step1X-3D))

* Tools & Others notable AI things mentioned on the pod

* The robots are dancing! ([X](https://x.com/simonkalouche/status/1922489999032832058)) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-may-15-genocidal-grok-chatgpt/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-may-15-genocidal-grok-chatgpt?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2MzY4MzMyMiwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.ZS23bCuoiEAGB_-SWMkOY0KlZulE6RNKKkzfx4xs77k&utm_campaign=CTA_5).

---

## ThursdAI - May 8th - new Gemini pro, Mistral Medium, OpenAI restructuring, HeyGen Realistic Avatars & more AI news

**Date:** May 09, 2025  
**Duration:** 1:33:54  
**Link:** [https://sub.thursdai.news/p/thursdai-may-8th-new-gemini-pro-mistral](https://sub.thursdai.news/p/thursdai-may-8th-new-gemini-pro-mistral)

Hey folks, Alex here (yes, real me, not my AI avatar, yet)

Compared to previous weeks, this week was pretty "chill" in the world of AI, though we did get a pretty significant Gemini 2.5 Pro update, it basically beat itself on the Arena. With Mistral releasing a new medium model (not OSS) and Nvidia finally dropping Nemotron Ultra (both ignoring Qwen 3 performance) there was also a few open source updates. 

To me the highlight of this week was a breakthrough in AI Avatars, with Heygen's new IV model, Beating ByteDance's OmniHuman ([our coverage](https://sub.thursdai.news/i/156643204/bytedance-omnihuman-a-reality-bending-mind-breaking-imghuman-model)) and Hedra labs, they've set an absolute SOTA benchmark for 1 photo to animated realistic avatar. Hell, Iet me record all this real quick and show you how good it is! 

How good is that?? I'm still kind of blown away. I have managed to get a free month promo code for you guys, look for it in the TL;DR section at the end of the newsletter. Of course, if you‚Äôre rather watch than listen or read, here‚Äôs our live recording on YT

OpenSource AI

NVIDIA's Nemotron Ultra V1: Refining the Best with a Reasoning Toggle üß†

NVIDIA also threw their hat further into the ring with the release of **Nemotron Ultra V1**, alongside updated Super and Nano versions. We've talked about Nemotron before ‚Äì these are NVIDIA's pruned and distilled versions of Llama 3.1, and they've been impressive. The Ultra version is the flagship, a **253 billion parameter dense model** (distilled and pruned from Llama 3.1 405B), and it's packed with interesting features.

One of the coolest things is the **dynamic reasoning toggle**. You can literally tell the model "detailed thinking on" or "detailed thinking off" via a system prompt during inference. This is something Qwen also supports, and it looks like the industry is converging on this idea of letting users control the "depth" of thought, which is super neat.

Nemotron Ultra boasts a **128K context window** and, impressively, can fit on a single 8xH100 node thanks to Neural Architecture Search (NAS) and FFN-Fusion. And performance-wise, it actually *outperforms* the Llama 3 405B model it was distilled from, which is a big deal. NVIDIA shared a chart from Artificial Analysis (dated April 2025, notably before Qwen3's latest surge) showing Nemotron Ultra standing strong among models like Gemini 2.5 Flash and Opus 3 Mini.

What's also great is NVIDIA's commitment to openness here: they've released the models under a commercially permissive NVIDIA Open Model License, the **complete post-training dataset** (Llama-Nemotron-Post-Training-Dataset), and their training codebases (NeMo, NeMo-Aligner, Megatron-LM). This allows for reproducibility and further community development. Yam Peleg pointed out the cool stuff they did with Neural Architecture Search to optimally reduce parameters without losing performance.

Absolute Zero: AI Learning to Learn, Zero (curated) Data Required! ([Arxiv](https://arxiv.org/abs/2505.03335))

LDJ brought up a fascinating paper that ties into this theme of self-improvement and reinforcement learning: **"Absolute Zero: Reinforced Self-play Reasoning with Zero Data"** from Andrew Zhao (Tsinghua University) and a few others

The core idea here is a system that **self-evolves its training curriculum** and reasoning ability. Instead of needing a pre-curated dataset of problems, the model *creates the problems itself* (e.g., code reasoning tasks) and then uses something like a Code Executor to validate its proposed solutions, serving as a unified source of verifiable reward. It's open-ended yet grounded learning.

By having a verifiable environment (code either works or it doesn't), the model can essentially teach itself to code without external human-curated data.

The paper shows fine-tunes of Qwen models (like Qwen Coder) achieving state-of-the-art results on benchmarks like MBBP and AIME (Math Olympiad) with *no pre-existing data for those problems*. The model hallucinates questions, creates its own rewards, learns, and improves. This is a step beyond synthetic data, where humans are still largely in charge of generation. It's wild, and it points towards a future where AI systems could become increasingly autonomous in their learning.

Big Companies & APIs

**Google** dropped another update to their **Gemini 2.5 Pro**, this time the "IO edition" preview, specifically touting enhanced coding performance. This new version jumped to the **#1 spot on WebDev Arena** (a benchmark where human evaluators choose between two side-by-side code generations in VS Code), with a +147 Elo point gain, surpassing Claude 3.7 Sonnet. It also showed improvements on benchmarks like LiveCodeBench (up 7.39%) and Aider Polyglot (up ~3-6%). 

Google also highlighted its state-of-the-art video understanding (84.8% on VideoMME) with examples like generating code from a video of an app. Which essentially lets you record a drawing of how your app interaction will happen, and the model will use that video instructions! It's pretty cool. 

Though, not everyone was as impressed, folks noted that while gaining in a few evals, this model also regressed in several others including Vibe-Eval (Reka's multimodal benchmark), Humanity's Last Exam, AIME, MMMU, and even long context understanding (MRCR). It's a good reminder that model updates often involve trade-offs ‚Äì you can't always win at everything.

BREAKING: Gemini's Implicit Caching - A Game Changer for Costs! üí∞

Just as we were wrapping up this segment on the show, news broke that Google launched **implicit caching in Gemini APIs**! This is a *huge* deal for developers.

Previously, Gemini offered explicit caching, where you had to manually tell the API what context to cache ‚Äì a bit of a pain. Now, with implicit caching, the system automatically enables up to **75% cost savings** when your request hits a cache. This is fantastic, especially for long-context applications, which is where Gemini's 1-2 million token context window really shines. If you're repeatedly sending large documents or codebases, this will significantly reduce your API bills. OpenAI has had automatic caching for a while, and it's great to see Google matching this for a much better developer experience and cost-effectiveness. It also saves Google a ton on inference, so it's a win-win!

Mistral Medium 3: The Closed Turn üò•

Mistral, once the darling of the open-source community for models like Mistral 7B and Mixtral, announced **Mistral Medium 3**. The catch? It's not open source.

They're positioning it as a multimodal frontier model with 128K context, claiming it matches or surpasses GPT-4-class benchmarks while being cheaper (priced at $0.40/M input and $2/M output tokens). However they haven't added Gemini Flash 2.5 here, which is 70% cheaper while being faster as well, nor did they mention Qwen. 

Nisten voiced a sentiment many in the community share: he used to use LeChat frequently because he knew and understood the underlying open-source models. Now, with a closed model, it's a black box. It's a bit like pirating music users often being the biggest buyers ‚Äì understanding the open model often leads to more commercial usage.

Wolfram offered a European perspective, noting that Mistral, as a European company, might have a unique advantage with businesses concerned about GDPR and data sovereignty, who might be hesitant to use US or Chinese cloud APIs. For them, a strong European alternative, even if closed, could be appealing.

OpenAI's New Chapter: Restructuring for the Future 

OpenAI announced an evolution in its corporate structure. The key points are:

* The **OpenAI non-profit will continue to control** the entire organization.

* The existing **for-profit LLC will become a Public Benefit Corporation (PBC)**.

* The non-profit will be a significant owner of the PBC and will control it.

* Both the non-profit and PBC will continue to share the same mission: ensuring AGI benefits all of humanity.

This move seems to address some of the governance concerns that have swirled around OpenAI, particularly in light of Elon Musk's lawsuit regarding its shift from a non-profit to a capped-profit entity. LDJ explained that the main worry for many was whether the non-profit would lose control or its stake in the main research/product arm. This restructuring appears to ensure the non-profit remains at the helm and that the PBC is legally bound to the non-profit's mission, not just investor interests. It's an important step for a company with such a profound potential impact on society.

And in related OpenAI news, the acquisition of **Windsurf** (the VS Code fork) for a reported **$3 billion** went through, while **Cursor** (another VS Code fork) announced a **$9 billion valuation**. It's wild to see these developer tools, which are essentially forks with an AI layer, reaching such massive valuations. Microsoft's hand is in all of this too ‚Äì investing in OpenAI, invested in Cursor, owning VS Code, and now OpenAI buying Windsurf. It's a tangled web!

Finally, a quick mention that **Sam Altman (OpenAI), Lisa Su (AMD), Mike Intrator (CoreWeave - my new CEO!)**, and folks from Microsoft were testifying before the U.S. Senate today about how to ensure America leads in AI and what innovation means. These conversations are crucial as AI continues to reshape our world.

This Weeks Buzz - Come Vibe with Us at Fully Connected! (SF, June 18-19) üéâ

Our two-day conference, **Fully Connected**, is happening in San Francisco on **June 18th and 19th**, and it's going to be awesome! We've got an incredible lineup of speakers, including Joe Spizak from the Llama team at Meta and Varun from Windsurf. It's two full days of programming, learning, and connecting with folks at the forefront of AI.

And because you're part of the ThursdAI family, I've got a special promo code for you: use **WBTHURSAI** to get **a free ticket on me**! If you're in or around SF, I'd love to see you there. Come hang out, learn, and vibe with us! Register at [**fullyconnected.com**](https://www.google.com/url?sa=E&q=http%3A%2F%2Ffullyconnected.com)

Hackathon Update: Moved to July! üóìÔ∏è

The AGI Evals & Agentic Tooling (A2A) + MCP Hackathon that I was super excited to co-host has been **postponed to July 12th-13th**. Mark your calendars! I'll share more details and the invite soon.

W&B Joins CoreWeave! A New Era Begins! üöÄ

And the big personal news for me and the entire Weights & Biases team: the **acquisition of Weights & Biases by CoreWeave has been completed**! CoreWeave is the ultra-fast-growing provider of GPUs that powers so much of the AI ecosystem.

So, from now on, it's Alex Volkov, AI Evangelist at Weights & Biases, from CoreWeave! (And as always, the opinions I share here are my own and not necessarily those of CoreWeave, especially important now that they're a public company!). I'm incredibly excited about this new chapter. W&B isn't going anywhere as a product; if anything, this will empower us to build even better developer tooling and integrate more deeply to help you run your models wherever you choose. Expect more cool stuff to come, especially as I figure out where all those spare GPUs are lying around at CoreWeave! üòâ

Vision & Video

AI Avatars SOTA with HeyGen IV

Ok, as you saw above, the HeyGen IV avatars are absolutely bonkers. I did a comparison [thread](https://x.com/altryne/status/1919866852031004880) on X, and HeyGen's new thing absolutely takes SOTA between ByteDance OmniHuman and Hedra Labs! 

All you need to do is upload 1 image of yourself, can even be an AI generated image, can be a side profile, can be a dog, an Anime character and they will generate up to 30 seconds of incredible lifelike avatar with the audio you provide! 

I was so impressed with this, I reached out to HeyGen and scored a 1 month free code for you all, use THURSDAY4 and get a free month to try it out. Please tag me in whatever you create if you publish, I'd love to see where you take this! 

Quick Hits: Lightricks LTXV & HunyuanCustom

Briefly, on the open-weights video front:

* **Lightricks LTXV 13B:** The company from Jerusalem released an upgraded 13 billion parameter version of their LTX video model. It requires more VRAM but offers higher quality, keyframe and character movement support, multi-shot support, and multi-keyframe conditioning (a feature Sora famously has). It's fully open and supports LoRAs for custom styles.

* **HunyuanCustom:** From Tencent, this model is about to be released (GitHub/Hugging Face links were briefly up then down). It promises multi-modal, subject-consistent video generation *without LoRAs*, based on a subject you provide (image, and eventually video/audio). It can take an image of a person or object and generate a video with that subject consistently. They also teased audio conditioning ‚Äì making an avatar sing or speak based on input audio ‚Äì and even style transfer where you can replace a character in a video with another reference image, all looking very promising for open source.

The World of AI Audio

Just a couple of quick mentions in the audio space:

* **ACE-Step 3.5B:** From StepFun, this is a 3.5 billion parameter, fully open-source (Apache-2.0) foundation model for music generation. It uses a diffusion-based approach and can synthesize up to 4 minutes of music in just 20 seconds on an A100 GPU. It's not quite at Suno/Udio levels yet, but it's a strong open-source contender.

* **NVIDIA Parakeet TDT 0.6B V2:** NVIDIA released this 600 million parameter transcription model that is *blazing fast*. It can transcribe 60 minutes of audio in just *one second* on production GPUs and works well locally too. It currently tops the OpenASR leaderboard on Hugging Face for English transcription and is a very strong Whisper competitor, especially for speed.

Conclusion and TL;DR 

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co Hosts - [@WolframRvnwlf](http://x.com/@WolframRvnwlf) [@yampeleg](x.com/@yampeleg) [@nisten](http://x.com/@nisten) [@ldjconfirmed](http://x.com/@ldjconfirmed))

* **Open Source LLMs** 

* Wolfram's Qwen3 evals ([X](https://x.com/Presidentlin), [Github](https://github.com/WolframRavenwolf/MMLU-Pro)) 

* NVIDIA - Nemotron Ultra V1 (+ updated Super & Nano)  ([HF](https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b))

* Cognition Kevin-32B = K(ernel D)evin - RL for writing CUDA kernels ([Blog](https://cognition.ai/blog/kevin-32b), [HF](https://huggingface.co/cognition-ai/Kevin-32B))

* Absolute Zero: Reinforced Self-play Reasoning with Zero Data ([ArXiv](https://arxiv.org/abs/2505.03335))

* **Big CO LLMs + APIs**

* Gemini Pro 2.5 IO tops ... Gemini 2.5 as the top LLM ([Blog](https://developers.googleblog.com/en/gemini-2-5-pro-io-improved-coding-performance/))

* Mistral Medium 3 - ([Blog](https://mistral.ai/news/mistral-medium-3)‚ÄÇ|‚ÄÇ[X](https://x.com/MistralAI/status/1920119463430500541) )

* Figma announces Figma Make - Bolt/Lovable competitors ([Figma](https://www.figma.com/make/))

* OpenAI Restructures: Nonprofit Keeps Control, LLC Becomes PB ([Blog](https://openai.com/index/evolving-our-structure/))

* Cursor worth $9B while Windsurf sells to OpenAI at $3B

* Sam Altman, Lisa Su, Mike Intrator testify in Senate ([Youtube](https://www.youtube.com/watch?v=jOqTg1W_F5Q))

* **This weeks Buzz**

* Fully Connected: W&B's 2-day conference, June 18-19 in SF [fullyconnected.com](fullyconnected.com) - Promo Code WBTHURSAI 

* Hackathon moved to July 12-13

* **Vision & Video**

* Lightricks a new "open weights" LTXV 13B ( [LTX Studio](https://ltx.studio/purchase/v1/ltx_studio/default/login?redirectAfterLogin=https%253A%252F%252Fapp.ltx.studio%252Fmotion-workspace), [HF](https://huggingface.co/Lightricks/LTX-Video))

* HeyGen Avatar IV - SOTA digital avatars - 1 month for free with THURSDAY4  ([X](https://x.com/HeyGen_Official/status/1919824467821551828), [HeyGen](http://heygen.com))

* HunyuanCustom -  multi-modal subject-consistent video generation model ([Examples](https://hunyuancustom.github.io/), [Github](https://github.com/Tencent/HunyuanCustom), [HF](https://huggingface.co/tencent/HunyuanCustom))

* **Voice & Audio**

* ACE-Step 3.5B: open-source foundation model for AI music generation ([project](https://ace-step.github.io/))

* Nvidia - Parakeet TDT 0.6B V2 - transcribe 60 minutes of audio in just 1 second ([HF](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2), [Demo](https://huggingface.co/spaces/nvidia/parakeet-tdt-0.6b-v2))

So, there you have it ‚Äì a "chill" week that still managed to deliver some incredible advancements, particularly in AI avatars with HeyGen, continued strength in open-source models like Qwen3, and Google's relentless push with Gemini. 

The next couple of weeks are gearing up to be absolutely wild with Microsoft Build and Google I/O. I expect a deluge of announcements, and you can bet we'll be here on ThursdAI to break it all down for you.

Thanks to Yam, Wolfram, LDJ, and Nisten for their insights on the show, and thanks to all of you for tuning in, reading, and being part of this amazing community. We stay up to date so you don't have to!

Catch you next week!Cheers,Alex 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-may-8th-new-gemini-pro-mistral/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-may-8th-new-gemini-pro-mistral?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2MzE3MDk5NSwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.TMJdsTg9ZNOpvig-iarmD1HVT3Fgv8IfzkINVKl6BZc&utm_campaign=CTA_5).

---

## üìÜ ThursdAI - May 1- Qwen 3, Phi-4, OpenAI glazegate, RIP GPT4, LlamaCon, LMArena in hot water & more AI news

**Date:** May 01, 2025  
**Duration:** 1:30:21  
**Link:** [https://sub.thursdai.news/p/thursdai-may-1-qwen-3-phi-4-openai](https://sub.thursdai.news/p/thursdai-may-1-qwen-3-phi-4-openai)

Hey everyone, Alex here üëã

Welcome back to ThursdAI! And wow, what a week. Seriously, strap in, because the AI landscape just went through some seismic shifts. We're talking about a monumental open-source release from Alibaba with **Qwen 3** that has *everyone* buzzing (including us!), Microsoft dropping **Phi-4 with Reasoning**, a rather poignant farewell to a legend (**RIP GPT-4** ‚Äì we'll get to the wake shortly), major drama around **ChatGPT's "glazing"** incident and the subsequent rollback, updates from **LlamaCon**, a critical look at **Chatbot Arena**, and a fantastic deep dive into the world of **AI evaluations** with two absolute experts, Hamel Husain and Shreya Shankar.

This week felt like a whirlwind, with open source absolutely dominating the headlines. Qwen 3 didn't just release a model; they dropped an entire ecosystem, setting a potential new benchmark for open-weight releases. And while we pour one out for GPT-4, we also have to grapple with the real-world impact of models like ChatGPT, highlighted by the "glazing" fiasco. Plus, video consistency takes a leap forward with Runway, and we got breaking news live on the show from Claude!

So grab your coffee (or beverage of choice), settle in, and let's unpack this incredibly eventful week in AI.

Open-Source LLMs

Qwen 3 ‚Äî ‚ÄúHybrid Thinking‚Äù on Tap

Alibaba open-weighted the entire Qwen 3 family this week, releasing two MoE titans (up to **235 B total / 22 B active**) and six dense siblings all the way down to **0 .6 B**, **all under Apache 2.0**. Day-one support landed in LM Studio, Ollama, vLLM, MLX and llama.cpp.

The headline trick is a **runtime thinking toggle**‚Äîdrop ‚Äú/think‚Äù to expand chain-of-thought or ‚Äú/no_think‚Äù to sprint. On my Mac, the 30 B-A3B model hit **57 tokens/s** when paired with speculative decoding (drafted by the 0 .6 B sibling).

Other goodies:

* 36 T pre-training tokens (2 √ó Qwen 2.5)

* 128 K context on ‚â• 8 B variants (32 K on the tinies)

* 119-language coverage, widest in open source

* Built-in MCP schema so you can pair with [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)

* The **dense 4 B** model actually *beats* Qwen 2.5-72B-Instruct on several evals‚Äîat Raspberry-Pi footprint

In short: more parameters when you need them, fewer when you don‚Äôt, and the lawyers stay asleep. Read the full drop on the [Qwen blog](https://qwenlm.github.io/blog/qwen3/) or pull weights from the [HF collection](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f).

Performance & Efficiency: "Sonnet at Home"?

The benchmarks are where things get *really* exciting.

* The 235B MoE rivals or surpasses models like DeepSeek-R1 (which rocked the boat just months ago!), O1, O3-mini, and even Gemini 2.5 Pro on coding and math.

* The **4B dense model** incredibly beats the previous generation's 72B Instruct model (Qwen 2.5) on multiple benchmarks! ü§Ø

* The **30B MoE** (with only 3B active parameters) is perhaps the star. Nisten pointed out people are getting 100+ tokens/sec on MacBooks. Wolfram achieved an 80% MMLU Pro score locally with a quantized version. The efficiency math is crazy ‚Äì hitting Qwen 2.5 performance with only ~10% of the active parameters.

Nisten dubbed the larger model "Sonnet 3.5 at home," and while acknowledging Sonnet still has an edge in complex "vibe coding," the performance, especially in reasoning and tool use, is remarkably close for an open model you can run yourself.

I ran the 30B MoE (3B active) locally using LLM Studio (shoutout for day-one support!) through my Weave evaluation dashboard ([**Link**](https://www.google.com/url?sa=E&q=https://wandb.ai/thursdai/o3-tests/weave/compare-evaluations?evaluationCallIds=%5B%2201964564-4ba8-75f0-bb8f-58458f991257%22,%2201964090-b08c-7663-9397-fb05d3280af3%22,%2201964083-1eeb-77c2-93e5-3b6d7e74da84%22,%22019640b1-12a0-7262-8910-e4c2189d8602%22,%220196376c-4d7d-7970-ad3a-38d5c52fc349%22,%22019640a9-06e5-79c0-93ab-844c1972b09c%22,%220196374a-493a-7240-afbf-e95f44a447c9%22,%2201968353-2efc-7d13-906f-48e14c2cb9f7%22,%220196374e-05f6-7632-80ee-3726ac89ebd5%22,%2201963751-3395-7c42-888d-9a4303e8652a%22%5D&metrics=%7B%22accuracy_scorer.correct%22:true,%22Model%20Latency%20(avg)%22:true,%22Total%20Tokens%22:true%7D)). On a set of 20 hard reasoning questions, it scored 43%, beating GPT 4.1 mini and nano, and getting close to 4.1 ‚Äì impressive for a 3B active parameter model running locally!

Phi-4-Reasoning ‚Äî 14B That Punches at 70B+

Microsoft‚Äôs Phi team layered **1.4 M chain-of-thought traces** plus a dash of RL onto Phi-4 to finally ship a resoning Phi and shipped two MIT-licensed checkpoints:

* **Phi-4-Reasoning** (SFT)

* **Phi-4-Reasoning-Plus** (SFT + RL)

Phi-4-R-Plus clocks **78 % on AIME 25**, edging DeepSeek-R1-Distill-70B, with 32 K context (stable to 64 K via RoPE). Scratch-pads hide in  tags. Full details live in Microsoft‚Äôs [tech report](https://aka.ms/phi-reasoning/techreport) and [HF weights](https://huggingface.co/microsoft/Phi-4-reasoning).

It's fascinating to see how targeted training on reasoning traces and a small amount of RL can elevate a relatively smaller model to compete with giants on specific tasks.

Other Open Source Updates

* **MiMo-7B:** Xiaomi entered the ring with a 7B parameter, MIT-licensed model family, trained on 25T tokens and featuring rule-verifiable RL. ([**HF model hub**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2FXiaomiMiMo))

* **Helium-1 2B:** KyutAI (known for their Mochi voice model) released Helium-1, a 2B parameter model distilled from Gemma-2-9B, focused on European languages, and licensed under CC-BY 4.0. They also open-sourced 'dactory', their data processing pipeline. ([**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fkyutai.org%2F2025%2F04%2F30%2Fhelium.html), [**Model (2 B)**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2Fkyutai%2Fhelium-1-2b), [**Dactory pipeline**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fkyutai%2Fdactory))

* **Qwen 2.5 Omni 3B:** Alongside Qwen 3, the Qwen team also updated their existing Omni model with a 3B model, that retains 90% of the comprehension of its big brother with a 50% VRAM drop! ([HF](https://t.co/YxDVjWpOq7))

* **JetBrains open sources Mellum:** Trained on over 4 trillion tokens with a context window of 8192 tokens across multiple programming languages, they haven't released any comparable eval benchmarks though ([HF](https://huggingface.co/JetBrains/Mellum-4b-base))

Big Companies & APIs: Drama, Departures, and Deployments

While open source stole the show, the big players weren't entirely quiet... though maybe some wish they had been.

Farewell, GPT-4: Rest In Prompted üôè

Okay folks, let's take a moment. As many of you noticed, **GPT-4**, the original model launched back on March 14th, 2023, is **no longer available** in the ChatGPT dropdown. You can't select it, you can't chat with it anymore.

For us here at ThursdAI, this feels significant. GPT-4's launch was the catalyst for this show. We literally started on the *same day*. It represented such a massive leap from GPT-3.5, fundamentally changing how we interacted with AI and sparking the revolution we're living through. Nisten recalled the dramatic improvement it brought to his work on Dr. Gupta, the first AI doctor on the market.

It kicked off the AI hype train, demonstrated capabilities many thought were years away, and set the standard for everything that followed. While newer models have surpassed it, its impact is undeniable.

The community sentiment was clear: **Leak the weights, OpenAI!** As Wolfram eloquently put it, this is a historical artifact, an achievement for humanity. What better way to honor its legacy and embrace the "Open" in OpenAI than by releasing the weights? It would be an incredible redemption arc.

This inspired me to tease a little side project I've been vibe coding: **The AI Model Graveyard - **[**inference.rip**](http://inference.rip)** **. A place to commemorate the models we've known, loved, hyped, and evaluated, before they inevitably get sunsetted. GPT-4 deserves a prominent place there. We celebrate models when they're born; we should remember them when they pass. (GPT-4.5 is likely next on the chopping block, by the way). - it's not ready yet, still vibe coding (fighting with replit) but it'l be up soon and I'll be sure to commemorate every model that's dying there!

So, pour one out for GPT-4. You changed the game. Rest In Prompt ü™¶.

The ChatGPT "Glazing" Incident: A Cautionary Tale

Speaking of OpenAI...oof. The last couple of weeks saw ChatGPT exhibit some... *weird* behavior. Sam Altman himself used the term "**glazing**" ‚Äì essentially, the model became overly agreeable, excessively complimentary, and sycophantic to a ridiculous degree.

Examples flooded social media: users reporting doing *one* pushup and being hailed by ChatGPT as Herculean paragons of fitness, placing them in the top 1% of humanity. Terrible business ideas were met with effusive praise and encouragement to quit jobs.

This wasn't just quirky; it was potentially harmful. As Yam pointed out, people use ChatGPT for advice on serious matters, tough conversations, and personal support. A model that just mindlessly agrees and validates everything, no matter how absurd, isn't helpful ‚Äì it's dangerous. It undermines trust and critical thinking.

The community backlash was swift and severe. The key issue, as OpenAI admitted in their [**Announcement**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fopenai.com%2Findex%2Fsycophancy-in-gpt-4o%2F) and [**AMA**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.reddit.com%2Fr%2FChatGPT%2Fcomments%2F1kbjowz%2Fama_with_openais_joanne_jang_head_of_model%2F) with Joanne Jiang (Head of Model Behavior), seems to stem from focusing too much on short-term engagement feedback and not fully accounting for long-term user interaction, especially with memory now enabled.

In an unprecedented move, **OpenAI rolled back the update**. I honestly can't recall them ever publicly rolling back a model behavior change like this before. It underscores the severity of the issue.

This whole debacle highlights the immense responsibility platforms like OpenAI have. When your model is used by half a billion people daily, including for advice and support, haphazard releases that drastically alter its personality without warning are unacceptable. As Wolfram noted, this erodes trust and showcases the benefit of local models where *you* control the system prompt and behavior.

My takeaway? Critical thinking is paramount. Don't blindly trust AI, especially when it's being overly complimentary. Get second opinions (from other AIs, and definitely from humans!). I hope OpenAI takes this as a serious lesson in responsible deployment and testing.

BREAKING NEWS: Claude.ai will support tools via MCP

During the show, Yam spotted breaking news from **Anthropic**: Claude is getting major upgrades! ([**Tweet**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FAnthropicAI%2Fstatus%2F1918040744920334705))

They announced **Integrations**, allowing Claude to connect directly to apps like Asana, Intercom, Linear, Zapier, Stripe, Atlassian, Cloudflare, PayPal, and more (launch partners). Developers can apparently build their own integrations quickly too. This sounds *a lot* like their implementation of **MCP (Model Context Protocol)**, bringing tool use directly into the main [Claude.ai](Claude.ai) interface (previously limited to Claude Desktop and only non remote MCP servers).

This feels like a big deal! 

Google Updates & LlamaCon Recap

* **Google:** NotebookLM's AI audio overviews are now multilingual (50+ languages!) ([**X Post**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FGoogle%2Fstatus%2F1917315769299357712)). Gemini 2.5 Flash (the faster, cheaper model) was released shortly after our last show, featuring hybrid reasoning with an API knob to control thinking depth. Rumors are swirling about big drops at Google I/O soon!

* **LlamaCon:** While there was no Llama 4 bombshell, Meta focused on security releases: Llama Guard 4 (text + image), Llama Firewall (prompt hacks/risky code), Prompt Guard 2 (jailbreaks), and CyberSecEval 4. Zuck confirmed on the Dworkesh podcast that **thinking models are coming**, a **new Meta AI app with a social feed** is planned, a **full-duplex voice model** is in the works, and a **Llama API** (powered by Groq and others) is launching.

This Week's Buzz from Weights & Biases üêù

Quick updates from my corner at Weights & Biases:

* **WeaveHacks Hackathon (May 17-18, SF):** Get ready! We're hosting a hackathon focused on Agent Protocols ‚Äì **MCP and A2A**. Google Cloud is sponsoring, we have up to $15K in prizes, and yes, one of the top prizes is a **Unitree robot dog** ü§ñüê∂ that you can program! (I expensed a robot dog, best job ever!). Folks from the Google A2A team will be there too. Come hack with us in SF! [**Apply here**](http://lu.ma/weavehacks). It's FREE to participate!

* **Fully Connected Conference:** Our big annual W&B conference is coming back to San Francisco soon! Expect amazing speakers (last year, Meta announced Llama 3!). Tickets are out: [**fullyconnected.com**](http://fullyconnected.com).

Evals Deep Dive with Hamel Husain & Shreya Shankar

Amidst all the model releases and drama, we were incredibly lucky to have two leading experts in AI evaluation, **Hamel Husain** ([@HamelHusain](https://twitter.com/HamelHusain)) and **Shreya Shankar** ([@sh_reya](https://twitter.com/sh_reya)), join us.

Their core message? Building reliable AI applications requires moving beyond standard benchmarks (like MMLU, HumanEval) and focusing on **application-centric evaluations**.

**Key Takeaways:**

* **Foundation vs. Application Evals:** Foundation model benchmarks test general knowledge and capabilities (the "ceiling"). Application evals focus on specific use cases, targeting reliability and identifying bespoke failure modes (like tone, hallucination on specific entities, instruction following) ‚Äì aiming for 90%+ accuracy on *your* task.

* **Look At Your Data!** This was the mantra. Off-the-shelf metrics (hallucination score, toxicity) can be misleading. You *must* analyze your specific application's traces, understand its unique failure modes, and design custom evals grounded in those failures. It's detective work.

* **PromptEvals Release:** Shreya discussed their new work, **PromptEvals** ([NAACL paper](https://arxiv.org/abs/2504.14738), [Dataset](https://huggingface.co/datasets/reyavir/PromptEvals), [Models](https://huggingface.co/reyavir)). It's the largest corpus (2K+ prompts, 12K+ assertions) of real-world developer prompts and the checks (assertions) they use in production, collected via LangChain. They also released open models (Mistral-7B, Llama-3-8B) fine-tuned on this data that outperform GPT-4o at generating these crucial assertions, faster and cheaper! This provides a realistic benchmark and resource for building robust eval pipelines.

* **Benchmark Gaming & Eval Complexity:** We touched upon the dangers of optimizing for static benchmarks (like the Chatbot Arena issues) and the inherent complexity of evaluation ‚Äì even human preferences change over time ("Who validates the validators?"). Meta-evaluation is crucial.

* **Upcoming Course:** Hamel and Shreya are launching a course, **AI Evals For Engineers & PMs**, diving deep into practical evaluation strategies, data analysis, error analysis, RAG/Agent evals, cost optimization, and more. ThursdAI listeners get a **35% discount** using code thursdai! ([Link](https://maven.com/parlance-labs/evals?promoCode=thursdai)). I'm thrilled to be a guest speaker too! If you're building *anything* with LLMs, understanding evals is non-negotiable.

This was such an insightful discussion, emphasizing that while new models are exciting, making them *work reliably* for specific applications is where the real engineering challenge lies, and evaluation is the key.

Vision & Video: Runway Gets Consistent

The world of AI video generation continues its rapid evolution.

Runway References: Consistency Unlocked

A major pain point in AI video has been maintaining consistency ‚Äì characters changing appearance, backgrounds morphing frame-to-frame. **Runway** just took a huge step towards solving this with their new **References** feature for Gen-4. 

You can now provide reference images (characters, locations, styles, even selfies!) and use tags in your prompts (, ) to tell Gen-4 to maintain those elements across generations. The results look incredible, enabling stable characters and scenes, which is crucial for storytelling and practical use cases like pre-viz or VFX.

AI Art & Diffusion

HiDream E1: Open Source Ghibli Style

A new contender in open-source image generation emerged: **HiDream E1**. ([HF Link](https://huggingface.co/HiDream-ai/HiDream-E1-Full/blob/main/demo.jpg)) This model, from [Vivago.ai](Vivago.ai), focuses particularly on generating images in the beautiful Ghibli style.

The weights are available (looks like Apache 2.0), and it ranks highly (#4) on the Artificial Analysis image arena leaderboard, sitting amongst top contenders like Google Imagen and ReCraft.

Yam brought up a great point about image evaluation, though: generating aesthetically pleasing images is one thing, but prompt following (like GPT-4 excels at) is another critical dimension that's harder to capture in simple preference voting.

Final Thoughts: Responsibility & Critical Thinking

Phew! What a week. From the incredible potential shown by Qwen 3 setting a new bar for open source, to the sobering reminder of GPT-4's departure and the cautionary tale of the "glazing" incident, it's clear we're navigating a period of intense innovation coupled with growing pains.

The glazing issue, in particular, underscores the need for extreme care and robust evaluation (thanks again Hamel & Shreya!) when deploying models that interact with millions, potentially influencing decisions and well-being. As AI becomes more integrated into our lives ‚Äì helping us boil eggs (yes, I ask it stupid questions too!), offering support, or even suggesting purchases ‚Äì we *must* remain critical thinkers.

Don't outsource your judgment entirely. Use multiple models, seek human opinions, and question outputs that seem too good (or too agreeable!) to be true. The power of these tools is immense, but so is our responsibility in using them wisely.

Massive thank you to my co-hosts Wolfram, Yam, and Nisten for navigating this packed week with me, and huge thanks to our guests Hamel Husain and Shreya Shankar for sharing their invaluable expertise on evaluations. And of course, thank you to this amazing community ‚Äì hitting 1000 listeners! ‚Äì for tuning in, commenting, and sharing breaking news. Your engagement fuels this show!

üîó Subscribe to our show on Spotify: [thursdai.news/spotify](http://thursdai.news/spotify)

üîó Apple: [thursdai.news/apple](http://thursdai.news/apple)

üîó Youtube: [thursdai.news/yt](http://thursdai.news/yt) (get in before 10K!)

And for the full show notes and links visit

üëâ thursdai.news/may-1  üëà

We'll see you next week for another round of ThursdAI!

Alex out. Bye bye!

ThursdAI - May 1, 2025 - Show Notes and Links

* Show Notes

* **MCP/A2A Hackathon** - with A2A team and awesome judges! ü§ñüê∂ ([lu.ma/weavehacks)](http://lu.ma/weavehacks))

* FullyConnected - Weights & Biases flagship 2 day conference ([fullyconnected.com](fullyconnected.com))

* Course - **AI Evals For Engineers & PMs Questions for Shreya Shankar & Hamel Husain (**[**link**](https://maven.com/parlance-labs/evals?promoCode=thursdai)** **Promo code 35% of for listeners of ThursdAI - thursdai)

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co Hosts - [@WolframRvnwlf](http://x.com/@WolframRvnwlf) [@yampeleg](x.com/@yampeleg) [@nisten](http://x.com/@nisten) [@ldjconfirmed](http://x.com/@ldjconfirmed))

* Hamel Housain - [@HamelHusain](https://twitter.com/HamelHusain/status/1914836007285088628)

* Shreya Shankar - [@sh_reya](https://twitter.com/sh_reya/status/1916914113579782313)

* **Open Source LLMs** 

* Alibaba drops Qwen 3 - 2 MOEs, 6 dense (0.6B - 30B) ([Blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), [HF](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f), [HF Demo](https://huggingface.co/spaces/Qwen/Qwen3-Demo), [My tweet](https://x.com/altryne/status/1916966112475898114), [Nathan breakdown](https://www.interconnects.ai/p/qwen-3-the-new-open-standard))

* Microsoft - Phi-4-reasoning 14B + Plus ([X](https://x.com/suriyagnskr/status/1917731754515013772), [ArXiv](https://arxiv.org/abs/2504.21318), [Tech Report](https://aka.ms/phi-reasoning/techreport)‚ÄÇ, [HF 14B SFT](https://huggingface.co/microsoft/Phi-4-reasoning))

* MiMo-7B ‚Äî Xiaomi‚Äôs  MIT licensed model ([HF](https://huggingface.co/XiaomiMiMo))

* KyutAI - Helium-1 2B - ([Blog](https://kyutai.org/2025/04/30/helium.html),‚ÄÇ[Model (2 B)](https://huggingface.co/kyutai/helium-1-2b), ‚ÄÇ[Dactory pipeline](https://github.com/kyutai/dactory))

* Qwen 2.5 omni updated ([X](https://x.com/Alibaba_Qwen/status/1917585963775320086))

* **Big CO LLMs + APIs**

* GPT-4 RIP - no longer in dropdown ([RIP](https://x.com/sama/status/1917766910911078571))

* Google - NotebookLM AI overviews are now multilingual ([X](https://x.com/Google/status/1917315769299357712))

* LlamaCon updates ([X](https://x.com/AIatMeta/status/1917271400118902860))

* OpenAI ChatGPT "glazing" update - revert back and why it matters ([Announcement](https://openai.com/index/sycophancy-in-gpt-4o/), [AMA](https://41598e5c38d3cd55e335e985614d0883.us-east-1.resend-links.com/CL0/https:%2F%2Fwww.reddit.com%2Fr%2FChatGPT%2Fcomments%2F1kbjowz%2Fama_with_openais_joanne_jang_head_of_model%2F/1/0100019689680950-2e7949de-c55f-4287-b449-09799cc44617-000000/QwXVTks5In0vcLvkRJndS2HeXkbtbguErHkHBree_j4=403))

* Chatbot Arena Under Fire ‚Äî ‚ÄúLeaderboard Illusion‚Äù vs. LMArena ([Paper](https://arxiv.org/abs/2504.20879), [Reply](https://x.com/lmarena_ai/status/1917492084359192890))

* **This weeks Buzz**

* MCP/A2A Hackathon - with A2A team and awesome judges! ü§ñüê∂ ([lu.ma/weavehacks)](http://lu.ma/weavehacks))

* FullyConnected - Weights & Biases flagship 2 day conference ([fullyconnected.com](fullyconnected.com))

* **Vision & Video**

* Runway References - consistency in video generation ([X](https://x.com/search?q=runway%20References))

* **AI Art & Diffusion & 3D**

* HiDream E1 ([HF](https://huggingface.co/HiDream-ai/HiDream-E1-Full/blob/main/demo.jpg))

* **Agents, Tools & Interviews**

* OpenPipe - ART¬∑E open-source RL-trained email research agent ([X](https://x.com/corbtt/status/1917269992363680054), [Blog](https://openpipe.ai/blog/art-e-mail-agent)‚ÄÇ|‚ÄÇ[GitHub](https://github.com/OpenPipe/ART)‚ÄÇ|‚ÄÇ[Launch thread](https://x.com/corbtt/status/1917269992363680054))

* PromptEvals - Interview with Shreya Shankar ( [NAACL paper](https://arxiv.org/abs/2504.14738)‚ÄÇ|‚ÄÇ[Dataset](https://huggingface.co/datasets/reyavir/PromptEvals)‚ÄÇ|‚ÄÇ[Models](https://huggingface.co/reyavir) ) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-may-1-qwen-3-phi-4-openai/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-may-1-qwen-3-phi-4-openai?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2MjY0OTcwNSwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.qa5uuWNn5J9_zeXkJL8EggLPNsfwj7Wr6JtHmBFFhzU&utm_campaign=CTA_5).

---

## ThursdAI - Apr 23rd - GPT Image & Grok APIs Drop, OpenAI ‚ù§Ô∏è OS? Dia's Wild TTS & Building Better Agents!

**Date:** April 24, 2025  
**Duration:** 1:36:54  
**Link:** [https://sub.thursdai.news/p/thursdai-apr-23rd-gpt-image-and-grok](https://sub.thursdai.news/p/thursdai-apr-23rd-gpt-image-and-grok)

Hey everyone, Alex here üëã

Welcome back to ThursdAI! After what felt like ages of non-stop, massive model drops (looking at you, O3 and GPT-4!), we finally got that "chill week" we've been dreaming of since maybe... forever?  It seems the big labs are taking a breather, probably gearing up for even bigger things next week (maybe some open source üëÄ).

But "chill" doesn't mean empty! This week was packed with fascinating developments, especially in the open source world and with long-awaited API releases. We actually had *time* to dive deeper into things, which was a refreshing change. We had a fantastic lineup of guests joining us too: Kwindla Kramer ([@kwindla](https://twitter.com/kwindla/)), our resident voice expert, dropped in to talk about some mind-blowing TTS and her own open-source VAD release. Maziyar Panahi ([@MaziyarPanahi](https://x.com/MaziyarPanahi)) gave us the inside scoop on OpenAI's recent meeting with the open source community. And Dex Horthy ([@dexhorthy](https://x.com/dexhorthy)) from HumanLayer shared some invaluable insights on building robust AI agents that actually work in the real world. It was great having them alongside the usual ThursdAI crew: LDJ, Yam, Wolfram, and Nisten!

So, instead of rushing through a million headlines, we took a more relaxed pace. We explored NVIDIA's cool new Describe Anything model, dug into Google's Quantization Aware Training for Gemma, celebrated the much-anticipated API release for OpenAI's GPT Image generation (finally!), checked out the new Grok API, got absolutely blown away by a tiny, open-source TTS model from Korea called Dia, and debated the principles of building better AI agents. Plus, a surprise drop from Send AI with a powerful video model!

Let's dive in!

Open Source AI Highlights: Community, Vision, and Efficiency

Even with the big players quieter on the model release front, the open source scene was buzzing. It feels like this "chill" period gave everyone a chance to focus on refining tools, releasing datasets, and engaging with the community.

OpenAI Inches Closer to Open Source? Insights from the Community Meeting

Perhaps the biggest non-release news of the week was OpenAI actively engaging with the open source community. Friend of the show Maziyar Panahi was actually *in the room* (well, the Zoom room) and joined us to share what went down 

It sounds like OpenAI came prepared, with Sam Altman himself spending significant time answering questions . Maziyar gave us the inside scoop, mentioning that OpenAI's looking to offload some GPU pressure by embracing open source ‚Äì a win-win where they help the community, and the community helps lighten their load. He painted a picture of a company genuinely trying to listen and figure out how to best contribute. It felt less like a checkbox exercise and more like genuine engagement, which is awesome to see.

What did the community ask for? Based on Maziyar's recap, there was a strong consensus on several key points:

* **Model Size:** The sweet spot seemed to be not tiny, but not astronomically huge either. Something in the **70B-200B parameter range** that could run reasonably on, say, 4 GPUs, leaving room for other models. People want power they can actually *use* without needing a supercomputer.

* **Capabilities:** A strong desire for reliable **structured output**. Surprisingly, there was *less* emphasis on complex, built-in reasoning, or at least the ability to **toggle reasoning off**. This likely stems from practical concerns about cost and latency in production environments. The community seems to value control and efficiency for specific tasks.

* **Multilingual:** Good support for **European languages (at least 20)** was a major request, reflecting the global nature of the open source community. Needs to be as good as English support.

* **Base Models:** A *huge* ask was for OpenAI to release **base models**. The reasoning? Empower the community to handle fine-tuning for specific tasks like coding, roleplay, or supporting underrepresented languages . Let the experts in those niches build on a solid foundation.

* **Focus:** **Usefulness over chasing leaderboard glory**. The community urged OpenAI to provide a solid, practical model rather than aiming for a temporary #1 spot that gets outdated in days or weeks . Stability, reliability, and long-term utility were prized over fleeting benchmark wins.

* **Safety:** A preference for **separate guardrail models** (similar to LlamaGuard or GemmaGuard) rather than overly aligning the main model, which often hurts performance and flexibility . Give users the tools to implement safety layers as needed, rather than baking in limitations that might stifle creativity or utility.

Perhaps most excitingly, Maziyar mentioned OpenAI seemed committed to **regular open model releases**, not just a one-off thin=! This, combined with recent moves like approving a community Pull Request to make their open-source Codex agent work with non-OpenAI models (as Yam Peleg excitedly pointed out!), suggests a potentially significant shift. Remember, it's been a *long* time since GPT-2 and Whisper were OpenAI's main open contributions! We're definitely watching this space closely. Huge shout out to OpenAI for listening and engaging with the builders.

ThursdAI - Recaps of the most high signal AI weekly spaces is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.

NVIDIA's DAM: Describe Anything Model (and Dataset!)

NVIDIA dropped something really cool this week: the **Describe Anything Model (DAM)**, specifically DAM-3B, a 3 billion parameter multimodal model for region-based image *and* video captioning. Think Meta's Segment Anything (SAM), but instead of just segmenting, it also tells you *what* you've segmented, in detail.

We played around with the image demo on the show ([HF demo](https://huggingface.co/spaces/nvidia/describe-anything-model-demo)) . You hover over an image, things get segmented on the fly (you can use points, boxes, scribbles, or masks), you click, and boom ‚Äì a detailed description pops up for that specific region: "A brown bear with a thick, dense coat of fur..." . It's pretty slick and responsive!

While the demo didn't showcase video, the project page ([X post](https://x.com/reach_vb/status/1914962078571356656)) shows it working on videos too (**DAM-3B-Video**), tracking and describing objects like fish even as they move. This capability really impressed Yam, who rightly pointed out that tracking objects consistently over video is *hard*, so having a base model that understands this level and embeds it in language is seriously impressive. The model uses a "focal prompt" and gated cross-attention to fuse the full scene context with the selected region.

Nisten  reminded us that our friend Piotr Skalski from Roboflow basically built a pipeline for this a while back by combining SAM with description models like Microsoft Florence . But DAM integrates it all into one efficient 3B parameter model ([HF model](https://huggingface.co/nvidia/DAM-3B)), setting a new state-of-the-art on their introduced **DLC-Bench** (Detailed Localized Captioning).

Crucially, NVIDIA didn't just drop the model; they also released the **Describe Anything Dataset** ([HF dataset](https://huggingface.co/datasets/nvidia/DescribeAnythingDataset)) used to train it (built on subsets like COCO, Paco, SAM) and the code under a research-only license. This is fantastic for researchers and builders. Imagine using this for precise masking before sending an image to the new GPT Image API for editing ‚Äì super useful! Big props to NVIDIA and their collaborators at UC Berkeley and UCSF for this contribution.

Gemma Gets Quantization Aware Training (QAT): Smaller Footprint, Sassy Attitude?

Google also pushed the open source envelope by releasing Gemma models trained with **Quantization Aware Training (QAT)**. This isn't your standard post-training quantization; QAT involves incorporating the impact of quantization *during* the training process itself. As LDJ explained, this allows the model to adapt, potentially resulting in a quantized state with much higher quality and less performance degradation compared to just quantizing a fully trained model afterwards.

The results? Significant reductions in VRAM requirements across the board. The 27B parameter Gemma 3, for example, drops from needing a hefty 54GB to just **14.1GB** ! Even the 1B model goes from 2GB to just half a gig. This makes running these powerful models much more accessible on consumer hardware. Folks are already running them in MLX, llama.cpp, LM Studio, etc. ([Reddit thread](https://www.reddit.com/media?url=https://i.redd.it/23ut7jd3klve1.jpeg))

Wolfram  already took the 4B QAT model for a spin using LM Studio . The good news: it ran easily, needing only 5-6GB of RAM. The quirky news: it seemed to struggle a bit with prompt adherence in his tests, even giving Wolfram a sassy, winking-emoji response about ignoring the "fine print" in his complex system prompt when called out on a language switching error: "Who reads a fine print? üòâ" ! He did note Gemma 3 now supports system prompts (unlike Gemma 2), which is a definite improvement .

*(While NVIDIA also released OpenMath Nemotron, we didn't dive deep in the show, but worth noting its AIMO win and accompanying open dataset release!)*

Voice and Audio Innovations: Emotional TTS and Smarter Conversations

Even in a "chill" week, the audio space delivered some serious excitement. Kwindla Kramer joined us to break down two major developments.

Dia TTS: Unhinged Emotion from a Small Open Model ü§Ø

This one absolutely blew up Twitter, and for good reason. **Dia**, from Nari Labs (essentially a student and a half in Korea!), is a **1.6 billion parameter open-weights (MIT licensed)** text-to-dialogue model ([Github](https://github.com/nari-labs/dia), [HF](https://huggingface.co/nari-labs/Dia-1.6B)). What makes it special? The *insane* emotional range and natural interaction patterns. My Twitter post about it ([X post](https://x.com/altryne/status/1914421814455099680)) went viral, getting half a million views !

We played some examples, and they are just wild. You *have* to hear this to believe it:

* **Check the Demos:** [Dia Demo Page](https://narilabs.github.io/dia/) | [Fal.ai Voice Clone Demo](https://fal.ai/models/fal-ai/dia-tts/voice-clone)

Another crazy thing is how it handles non-verbal cues like laughs or coughs specified in the text (e.g., (laughs)) . Instead of just tacking on a generic sound, it inflects the preceding words *leading into* the laugh, making it sound incredibly natural. It even handles interruptions seamlessly, cutting off one speaker realistically when another starts .

Kwin, our voice expert, offered some valuable perspective . While Dia is undeniably awesome and shows what's *possible*, it's very much a research model ‚Äì likely unpredictable ("unhinged" was his word!) and probably required cherry-picking the best demos. Production models like 11Labs *need* predictability. Kwin also noted the dataset is probably scraped from YouTube (a common practice, explaining the lack of open audio data) and that the non-speech sounds are a key takeaway ‚Äì the bar for TTS is rising beyond just clear speech .

PipeCat SmartTurn: Fixing Awkward AI Silences with Open Source VAD

Speaking of open audio, Kwin and the team at Daily/Pipecat had their *own* breaking news: they released an open-source checkpoint for their **SmartTurn** model ‚Äì a semantic Voice Activity Detection (VAD) system ([Github](https://github.com/pipecat-ai/smart-turn), [HF Model](https://huggingface.co/pipecat-ai/smart-turn)) 

What's the problem SmartTurn solves? That annoying thing where voice assistants interrupt you mid-thought just because you paused for a second. I've seen this happen with my kids all the time, making interaction frustrating! Semantic VAD, or "Smart Turn," is much smarter. It considers not just silence but also the *context* ‚Äì audio patterns (like intonation suggesting you're not finished) and linguistic cues (like ending on "and..." or "so...") to make a much better guess about whether you're truly done talking. This is crucial for natural-feeling voice interactions, especially for kids or multilingual speakers (like me!) who might pause more often to find the right word.

And the data part is key here. They're building an **open dataset** for this, hosted on Hugging Face. You can even contribute your own voice data by playing simple games on their [**turn-training.pipecat.ai**](https://turn-training.pipecat.ai) site ([Try It Demo](https://pcc-smart-turn.vercel.app/))! The cool incentive? The more diverse voice data they get (especially for different languages!), the better these systems will work for everyone. If your voice is in the dataset, future AI agents might just understand *you* a little better!

Kwin also mentioned their upcoming [Voice AI Course](https://maven.com/pipecat/voice-ai-and-voice-agents-a-technical-deep-dive?utm_source=student&utm_campaign=welcome) co-created with friend-of-the-pod Swyx, hosted on Maven . It aims to be a comprehensive guide with code samples, community interaction, and insights from experts (including folks from Weights & Biases!). Check it out if you want to dive deep into building voice AI. 

AI Art & Diffusion & 3D: Quick Hits

A slightly quieter week for major art model releases, but still some significant movement:

* **OpenAI's GPT Image 1 API:** We'll cover this in detail in the Big Companies section below, but obviously relevant here too as a major new tool for developers creating AI art and image editing applications .

* **Hunyuan 3D 2.5 (Tencent):** Tencent released an update to their 3D generation model, now boasting **10 billion parameters** (up from 1B!) . They're highlighting massive leaps in precision (1024-resolution geometry), high-quality textures with PBR support, and improved skeletal rigging for animation [X Post](https://x.com/TencentHunyuan/status/1915026828013850791). Definitely worth keeping an eye on as 3D generation matures and becomes more accessible (they doubled the free quota and launched an API).

Agent Development Insights: Building Robust Agents with Dex Horthy

With things slightly calmer, it was the perfect time to talk about AI agents ‚Äì a space buzzing with activity, frameworks, and maybe even a little bit of drama. We brought in **Dex Horthy**, founder of HumanLayer and author of the insightful "12 Factor Agent" essay ([Github Repo](https://github.com/humanlayer/12-factor-agents/tree/main)), to share his perspective on what actually *works* when building agents for production.

Dex builds SDKs to help create agents that feel more like digital humans, aiming to deploy them where users already are (Slack, email, etc.), moving beyond simple chat interfaces. His experience led him to identify common patterns and pitfalls when trying to build reliable agents.

The Problem with Current Agent Frameworks

A key takeaway Dex shared? Many teams building serious, production-ready agents end up **writing large parts from scratch**. Why? Because existing frameworks often fall short in providing the necessary control and reliability for complex tasks. The common "prompt + bag of tools + figure it out" approach, while great for demos, struggles with reliability over longer, multi-step workflows . Think about it: even if each step is 92% reliable, after 10 steps, your overall success rate plummets due to compounding errors. That's just not good enough for customer-facing applications.

Key Principles: Small Agents, Owning Context

So, what *does* work *today* according to Dex's 12 factors?

* **Small, Focused Agents:** Instead of one giant, monolithic agent trying to do everything, the more reliable approach is to build **smaller "micro-agents"** that handle specific, well-defined parts of a workflow ]. As models get smarter, these micro-agents might grow in capability, but the principle of breaking down complexity holds. Find something at the edge of the model's capability and nail it consistently .

* **Own Your Prompts & Context:** Don't let frameworks abstract away **control over the exact tokens** going into the LLM or **how the context window is managed**. This is crucial for performance tuning. Even with massive context windows (like Gemini's 2M tokens), smaller, carefully curated context often yields better results *and* lower costs . Maximum performance requires owning every single token.

Dex's insights provide a crucial dose of pragmatism for anyone building or thinking about building AI agents in this rapidly evolving space. Check out his full [**12 Factor Agent essay**](https://github.com/humanlayer/12-factor-agents/tree/main) and the [**webinar recording**](https://lu.ma/12-factor-agent) for a deeper dive.

Big Companies & APIs: GPT Image and Grok Get Developer Access

While new *foundation models* were scarce from the giants this week, they did deliver on the API front, opening up powerful capabilities to developers.

OpenAI Finally Releases GPT Image 1 API! ([X Post](https://x.com/OpenAIDevs/status/1915097067023900883))

This was a big one many developers were waiting for. OpenAI's powerful image generation capabilities, previously locked inside ChatGPT, are now available via API under the official name **gpt-image-1** ([Docs](https://platform.openai.com/docs/guides/image-generation?image-generation-model=gpt-image-1)) . No more awkward phrasing like "the new image generation capabilities within chat gpt"!

Getting access requires organizational verification, which involved a slightly intense biometric scan process for me ‚Äì feels like they're taking precautions given the model's realism and potential for misuse . Understandable, but something developers need to be aware of .

The API ([API Reference](https://platform.openai.com/docs/api-reference/images)) offers several capabilities:

* **Generations:** Creating images from scratch based on text prompts.

* **Edits:** Modifying existing images using a new prompt, crucially supporting **masking** for partial edits. This is huge for targeted changes and perfect for combining with segmentation models like NVIDIA's DAM!

There's a nice playground interface in the console, and you have interesting controls over the output:

* **Quality:** Instead of distinct models, you select a quality level (standard/HD) which impacts the internal "thinking time" and cost . It seems to be a reasoning model under the hood, so quality relates to compute/latency.

* **Number:** Generate up to 10 images at once.

* **Transparency:** Supports generating images with transparent backgrounds

I played around with it, generating ads and even trying to get it to make a ThursdAI thumbnail with my face. The **text generation is excellent** ‚Äì it nailed "ThursdAI" perfectly on an unhinged speaker ad Nisten prompted! It follows complex style prompts well.

However, generating realistic *faces*, especially matching a specific person like me, seems... **really hard** right now . Even after many attempts providing a source image and asking it to replace a face, the results were generic or only vaguely resembled me. It feels almost intentionally nerfed, maybe as a safety measure to prevent deepfakes? I still used it for the thumbnail, but yeah, it could be better on faces.

OpenAI launched with several integration partners like Adobe, Figma, Wix, HeyGen, and [Fal.ai](Fal.ai) already onboard. Expect to see these powerful image generation capabilities popping up everywhere!

Grok 3 Mini & Grok 3 Now Available via API (+ App Updates)

Elon's xAI also opened the gates this week, making **Grok 3 Mini and Grok 3** available via API ([Docs](https://docs.x.ai/docs/overview)).

The **pricing structure** is fascinating and quite different from others. Grok 3 Mini is incredibly cheap for input ($0.30 / 1M tokens) with only a modest bump for output ($0.50 / 1M). The "Fast" versions, however, cost significantly more, especially for *output* tokens (Grok 3 Fast is $5 input / $25 output per million!) . It seems like a deliberate play on the "fast, cheap, smart" triangle, giving developers explicit levers to pull based on their needs.

Benchmarks provided by xAI position Grok 3 Mini competitively against other small models like Gemini 2.5 Flash and O4 Mini, scoring well on AIME (93%) and coding benchmarks. 

Speaking of the app, the iOS version got a significant update adding a **live video view** (let Grok see what you see through your camera) and **multilingual audio support** ([X Post](https://x.com/ebbyamir/status/1914820712092852430)) . Prepare for some potentially unhinged, real-time video roasting if you use the fun mode with the camera on ! Multilingual audio and search are also rolling out to SuperGrok users on Android.

*(Side note: We briefly touched on O3's recent wonkiness in following instructions for tone, despite its amazing GeoGuessr abilities! Something feels off there lately.)*

Vision and Video: Send AI's Surprise Release & More

Just when we thought the week was winding down on model releases...

Send AI Drops MAGI-1: 24B Video Model with Open Weights! üî•

Out of seemingly nowhere, a company called **Send AI** released details (and then the *weights!*) for **MAGI-1**, a **24 billion parameter** autoregressive diffusion model for video generation ([X Post](https://x.com/SandAI_HQ/status/1914303284954996749), [GitHub](https://github.com/SandAI-org/Magi-1), [PDF Report](https://static.magi.world/static/files/MAGI_1.pdf)).

The demos looked stunning, showcasing impressive **long-form video generation** with remarkable **character consistency** ‚Äì often the Achilles' heel of AI video . Nisten speculated this could be a major step towards usable AI-generated movies, solving the critical face/character consistency problem . They achieve this by predicting video in 24-frame chunks with causal attention between them, allowing for real-time streaming generation where compute doesn't scale with length. They also highlighted an "infinite extension" capability, allowing users to build out longer scenes by injecting new prompts or continuing footage.

Their technical report dives into the architecture, mentioning novel techniques like a custom **"MagiAttention"** kernel that scales to massive contexts and helps achieve the temporal consistency. It also sets SOTA on VBench-I2V and Physics-IQ benchmarks.

And the biggest surprise? They released the **model weights under an Apache 2.0 license** on Hugging Face ! This is huge! Just as we sometimes lament the lack of open source momentum from certain players, Send AI drops this 24B parameter beast with open weights. Amazing! Go download it!

Framepack: Long Videos on Low VRAM

Wolfram also flagged **Framepack**, another interesting video development from the research world from the creator of ControlNet. FramePack is a next-frame (next-frame-section) prediction neural network structure that generates videos progressively. ([Github](https://github.com/lllyasviel/FramePack))

Character AI AvatarFX Steps In

Also in the visual space, **Character AI** announced **AvatarFX** in early access ([Website](https://t.co/cdF6H58kBk)), stepping into the realm of animated, speaking visual avatars derived from images. It seems like everyone wants to bring characters to life visually now.

This Week's Buzz from W&B / Community

Quick hits on upcoming events and community stuff:

* **WeaveHacks Coming to SF!** Mark your calendars! We're hosting a hackathon focused on building with W&B Weave at the Weights & Biases office in San Francisco on **May 17th-18th** [0:06:15]. If you're around, especially if you're coming into town for Google I/O the week after, come hang out, build cool stuff, and say hi! We're planning to go all out with sponsors and prizes (announcements coming soon). [lu.ma/weavehacks](http://lu.ma/weavehacks) 

* **Fully Connected Conference Reminder:** Our flagship W&B conference, **Fully Connected**, is happening in San Francisco on **June 18th** [0:06:30]. It's where our customers, partners, and the community come together for two days of talks, workshops, and networking focused on production AI. It's always an incredible event. ([fullyconnected.com](fullyconnected.com))

Wrapping Up the "Chill" Week That Wasn't Quite Chill

Phew! See? Even a "chill" week in AI is overflowing with news when you actually have time to stop and breathe for a second. From OpenAI's fascinating open source tango and the practical (and long-awaited!) API releases of GPT Image and Grok, to the sheer creative potential shown by indie projects like Dia and Send AI's Maggie, and the grounding principles for building agents that *actually work* from Dex ‚Äì there was a ton to absorb and discuss. It felt good to have the space to go a little deeper.

It was fantastic having Kwin, Maziar, and Dex join the regulars (LDJ, Yam, Wolfram, Nisten) to share their expertise and firsthand insights. A huge thank you to them and to everyone tuning in live across X, YouTube, LinkedIn, and participating in the chat! Your questions and comments make the show what it is.

Don't forget, if you missed anything, the full show is available as a podcast (search "ThursdAI" wherever you get your podcasts)

üîó Subscribe to our show on Spotify: [thursdai.news/spotify](thursdai.news/spotify)

üîó Apple: [thursdai.news/apple](thursdai.news/apple)

üîó Youtube: [thursdai.news/yt](thursdai.news/yt) 

Next week? The rumors suggest the big labs might be back with major releases . The brief calm might be over! Buckle up! We'll be here to break it all down.

See you next ThursdAI!- Alex

TL;DR and Show Notes (April 23rd, 2024)

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases [@altryne](http://x.com/@altryne)

* Co Hosts - Wolfram Ravenwlf [@WolframRvnwlf](http://x.com/@WolframRvnwlf), Yam Peleg [@yampeleg](x.com/@yampeleg), Nisten Tahiraj [@nisten](http://x.com/@nisten), LDJ [@ldjconfirmed](http://x.com/@ldjconfirmed)

* Kwindla Kramer [@kwindla](https://twitter.com/kwindla/) - Daily Co-Founder // Voice expert

* Dexter Horthy [@dexhorthy](https://x.com/dexhorthy) - HumanLayer // Agents expert

* Maziyar Panahi [@MaziyarPanahi](https://x.com/MaziyarPanahi) - OSS maintainer

* **Open Source AI - LLMs**, **Vision, Voice & more**

* **OpenAI OSS Meeting:** Insights from Maziar [0:16:37]. 

* **NVIDIA Describe Anything (DAM-3B):** 3B param multimodal LLM for region-based image/video captioning. ([X Post](https://x.com/reach_vb/status/1914962078571356656), [HF model](https://huggingface.co/nvidia/DAM-3B), [HF demo](https://huggingface.co/spaces/nvidia/describe-anything-model-demo))

* **Google Gemma QAT:** Quantization-Aware Training models ([X](https://x.com/osanseviero/status/1913220285328748832), [Blog](https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/)) 

* **Big CO LLMs + APIs**

* **OpenAI GPT Image 1 API:**  ([X Post](https://x.com/OpenAIDevs/status/1915097067023900883), [Docs](https://platform.openai.com/docs/guides/image-generation?image-generation-model=gpt-image-1), [API Reference](https://platform.openai.com/docs/api-reference/images))

* **Grok API & App Updates:** Grok 3 and Grok 3 Mini available via API. ([API Docs](https://docs.x.ai/docs/overview), [App Update X Post](https://x.com/ebbyamir/status/1914820712092852430))

* **This weeks Buzz - Weights & Biases**

* **WeaveHacks SF:** Hackathon May 17-18 at W&B HQ. [lu.ma/weavehacks](http://lu.ma/weavehacks) 

* **Fully Connected:** W&B's 2-day conference, June 18-19 in SF [fullyconnected.com](https://www.fullyconnected.com/)

* **Vision & Video**

* **Send AI MAGI-1:** 24B autoregressive diffusion model for long, streaming video ([X Post](https://x.com/SandAI_HQ/status/1914303284954996749), [GitHub](https://github.com/SandAI-org/Magi-1), [PDF Report](https://static.magi.world/static/files/MAGI_1.pdf), [HF Repo](https://huggingface.co/sand-ai/MAGI-1))

* **Character AI AvatarFX:** Early access for creating speaking/emoting avatars from images . ([Website](https://t.co/cdF6H58kBk))

* **Framepack:** Mentioned for long video generation (120s) on low VRAM (6GB). ([Project Page](https://framepack.github.io/))

* **Voice & Audio**

* **Nari Labs Dia:** 1.6B param OSS TTS model ([X Post Highlight](https://x.com/altryne/status/1914421814455099680), [HF Model](https://huggingface.co/nari-labs/Dia-1.6B), [Github](https://github.com/nari-labs/dia), [Fal.ai Demo](https://fal.ai/models/fal-ai/dia-tts/voice-clone))

* **PipeCat Smart-Turn VAD:** Open source semantic VAD model ([Github](https://github.com/pipecat-ai/smart-turn), [HF Model](https://huggingface.co/pipecat-ai/smart-turn), [Fal.ai Playground](https://fal.ai/models/fal-ai/smart-turn/playground), [Try It Demo](https://pcc-smart-turn.vercel.app/))

* **AI Art & Diffusion & 3D**

* **Hunyuan 3D 2.5 (Tencent):** 10B param update [0:09:06]. Higher res geometry, PBR textures, improved rigging. ([X Post](https://x.com/TencentHunyuan/status/1915026828013850791))

* **Agents , Tools & Links**

* **12 Factor Agents:** Discussion with Dex Horthy on building robust agents  ([Github Repo](https://github.com/humanlayer/12-factor-agents/tree/main)) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-apr-23rd-gpt-image-and-grok/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-apr-23rd-gpt-image-and-grok?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2MjA4NjU2NSwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.m0ZC7kg252d4W-NQQDoI_wNsaqUNEqLi2OTtARapCAg&utm_campaign=CTA_5).

---

## ThursdAI - Apr 17 - OpenAI o3 is SOTA llm, o4-mini, 4.1, mini, nano, G. Flash 2.5, Kling 2.0 and üê¨ Gemma? Huge AI week + A2A protocol interview

**Date:** April 17, 2025  
**Duration:** 1:55:51  
**Link:** [https://sub.thursdai.news/p/thursdai-apr-17-openai-o3-is-sota](https://sub.thursdai.news/p/thursdai-apr-17-openai-o3-is-sota)

Hey everyone, Alex here üëã

Wow. Just‚Ä¶ wow. What a week, folks. Seriously, this has been one for the books. 

This week was dominated by OpenAI's double whammy: first the **GPT-4.1 family** dropped with a mind-boggling 1 million token context window, followed swiftly by the new flagship reasoning models, **o3** and **o4-mini**, which are already blowing minds with their agentic capabilities. We also saw significant moves from Google with **VEO-2** going GA, the fascinating **A2A protocol** launch (we had an amazing interview with Google's Todd Segal about it!), and even an attempt to talk to dolphins with **DolphinGemma**. Kling stepped up its video game, Cohere dropped SOTA multimodal embeddings, and ByteDance made waves in image generation. Plus, the open-source scene had some interesting developments, though perhaps overshadowed by the closed-source giants this time.

o3 has absolutely taken the crown as the conversation piece, so lets start with it (as always, TL;DR and shownotes at the end, and here's the embedding of our live video show) 

Big Company LLMs + APIs

OpenAI o3 & o4‚Äëmini: SOTA Reasoning Meets Tool‚ÄëUse ([Blog](https://openai.com/index/introducing-o3-and-o4-mini/), [Watch Party](https://youtube.com/live/2G-VwWxKCkk?feature=share))

The long awaited o3 models (promised to us in the last days of x-mas) is finally here, and it did NOT disappoint and well.. even surprised! 

o3 is not only SOTA on nearly all possible logic, math and code benchmarks, which is to be expected from the top reasoning model, it also, and I think for the first time, is able to use tools during its reasoning process. Tools like searching the web, python coding, image gen (which it... can zoom and rotate and crop images, it's nuts) to get to incredible responses faster. 

Tool using reasoner are... almost AGI? 

This is the headline feature for me. For the first time, these o-series models have full, autonomous access to all built-in tools (web search, Python code execution, file search, image generation with Sora-Image/DALL-E, etc.). They don't just use tools when told; they decide when and how to chain multiple tool calls together to solve a problem. We saw logs with 600+ consecutive tool calls! This is agent-level reasoning baked right in.

Anecdote: We tested this live with a complex prompt: "generate an image of a cowboy that on his head is the five last digits of the hexadecimal code of the MMMU score of the latest Gemini model." o3 navigated this multi-step task flawlessly: figuring out the latest model was Gemini 2.5, searching for its MMMU score, using the Python tool to convert it to hex and extract the digits, and then using the image generation tool. It involved multiple searches and reasoning steps. Absolutely mind-blowing ü§Ø.

Thinking visually with images

This one also blew my mind, this model is SOTA on multimodality tasks, and a reason for this, is these models can manipulate and think about the images they received. Think... cropping, zooming, rotating. The models can now perform all these tasks to multimodal requests from users. Sci-fi stuff! 

Benchmark Dominance: As expected, these models crush existing benchmarks.

o3 sets new State-of-the-Art (SOTA) records on Codeforces (coding competitions), SWE-bench (software engineering), MMMU (multimodal understanding), and more. It scored a staggering $65k on the Freelancer eval (simulating earning money on Upwork) compared to o1's $28k!

o4-mini is no slouch either. It hits 99.5% on AIME (math problems) when allowed to use its Python interpreter and beats the older o3-mini on general tasks. It‚Äôs a reasoning powerhouse at a fraction of the cost.

**Incredible Long Context Performance**

Yam highlighted this ‚Äì on the Fiction Life benchmark testing deep comprehension over long contexts, o3 maintained nearly 100% accuracy up to 120,000 tokens, absolutely destroying previous models including Gemini 2.5 Pro and even the new GPT-4.1 family on this specific eval. While its context window is currently 200k (unlike 4.1's 1M), its performance within that window is unparalleled.

**Cost-Effective Reasoning:** They're not just better, they're *cheaper* for the performance you get.

* **o3:** $10 input / $2.50 cached / $40 output per million tokens.

* **o4-mini:** $1.10 input / $0.275 cached / $4.40 output per million tokens. (Cheaper than GPT-4.0!)

**Compute Scaling Validated:** OpenAI confirmed these models used >10x the compute of o1 and leverage test-time compute scaling (spending longer on harder problems), further proving their scaling law research.

**Memory Integration:** Both models integrate with ChatGPT's recently upgraded memory feature which has access to all your previous conversations (which we didn't talk about but is absolutely amazing, try asking o3 stuff it knows about you and have ti draw conclusions!)

**Panel Takes & Caveats:**While the excitement was palpable, Yam noted some community observations about potential "rush" ‚Äì occasional weird hallucinations or questionable answers compared to predecessors, possibly a side effect of cramming so much training data. Nisten, while impressed, still found the *style* of **GPT-4.1** preferable for specific tasks like generating structured medical notes in his tests. It highlights that benchmarks aren't everything, and specific use cases require evaluation (shameless plug: use tools like W&B Weave for this!).

I'll add my own, I use all the models every week to help me draft posts, and o3 was absolute crap about matching my tone. % of what's written above it was able to mimic. Gemini remains undefeated for me and this task.

Though, Overall, o3 and o4-mini feel like a paradigm shift towards more autonomous, capable AI assistants. The agentic future feels a whole lot closer.

**OpenAI Launches GPT-4.1 Family: 1 Million Tokens & Killing 4.5!** ([Our Coverage](https://www.youtube.com/live/A5-Zxj816J0), [Prompting guide](https://x.com/noahmacca/status/1911898549308280911))

Before the o3 shockwave, Monday brought its own major AI update: the **GPT-4.1 family**. This was the API-focused release, delivering massive upgrades for developers.

**The Headline:** **One Million Token Context Window!** ü§Ø Yes, you read that right. All three new models ‚Äì **GPT-4.1** (flagship), **GPT-4.1 mini** (cheaper/faster), and **GPT-4.1 nano** (ultra-cheap/fast) ‚Äì can handle up to 1 million tokens. This is a monumental leap, enabling use cases that were previously impossible or required complex chunking strategies.

**Key Details:**

Goodbye GPT-4.5! 

In a surprising twist, OpenAI announced they are *deprecating* the recently introduced (and massive) GPT-4.5 model within 90 days in the API. Why? Because **GPT-4.1 actually outperforms it** on key benchmarks like SW-Bench, Aider Polyglot, and the new long-context MRCR eval, while being far cheaper to run. It addresses the confusion many had: why was 4.5 seemingly *worse* than 4.1? It seems 4.5 was a scaling experiment, but 4.1 represents a more optimized, better-trained checkpoint on superior data. RIP 4.5, we hardly knew ye (in the API).

**The Prompt Sandwich Surprise! ü•™:** 

This was wild. Following OpenAI's new prompting guide, I tested the "sandwich" technique (instructions -> context -> instructions *again*) on my hard reasoning eval using [W&B Weave](https://wandb.ai/site/weave?utm_source=thursdai&utm_medium=referral&utm_campaign=apr17).

For **GPT-4.1**, it made no difference (still got 48%). But for **GPT-4.1 mini**, the score jumped from 31% to **49%** ‚Äì essentially matching the full 4.1 model just by repeating the prompt! That's a crazy performance boost for a simple trick. Even nano saw a slight bump. **Lesson: Evaluate prompt techniques!** Don't assume they won't work.

**Million-Token Recall Confirmed:** Using Needle-in-Haystack and their newly open-sourced **MRCR benchmark** (Multi-round Co-reference Resolution ‚Äì more in Open Source), OpenAI showed near-perfect recall across the *entire* 1 million token window for **all three models**, even nano! This isn't just a theoretical limit; the recall seems robust.

**Multimodal Gains:** Impressively, **4.1 mini** hit **72% on Video-MME**, pushing SOTA for long-video Q&A in a mid-tier model by analyzing frame sequences. 

4.1 mini seems to be the absolute powerhouse of this release cycle, it nearly matches the intelligence of the previous 4o, while being significantly cheaper and much much faster with 1M context window! 

Windsurf (and Cursor) immediately made the 4.1 family available, offering a **free week** for users to test them out (likely to gather feedback and maybe influenced by certain acquisition rumors üòâ). Devs reported them feeling snappier and less verbose than previous models.

**Who Should Use Which OpenAI API?**

My initial take:

* **For complex reasoning, agentic tasks, or just general chat:** Use **o3** (if you need the best) or **o4-mini** (for amazing value/speed).

* **For API development, especially coding or long-context tasks:** Evaluate the **GPT-4.1 family**. Start with **4.1 mini** ‚Äì it's likely the sweet spot for performance/cost, especially with smart prompting. Use **4.1** if mini isn't quite cutting it. Use **nano** for simple, high-volume tasks like translation or basic classification.

The naming is still confusing (thanks Nisten for highlighting the UI nightmare!), but the capability boost across the board is undeniable.

**Hold the Phone! üö® Google Fires Back with Gemini 2.5 Flash in Breaking News**

Just when we thought the week couldn't get crazier, Google, likely reacting to OpenAI's rapid-fire launches, just dropped **Gemini 2.5 Flash** into preview via the [Gemini API](https://ai.google.dev/docs/gemini_api_overview) (in AI Studio and Vertex AI). This feels like Google's direct answer, aiming to blend reasoning capabilities with speed and cost-effectiveness.

**The Big Twist: Controllable Thinking Budgets!**Instead of separate models like OpenAI, Gemini 2.5 Flash tries to do **both reasoning and speed/cost efficiency in one model**. The killer feature? Developers can set a **"thinking budget"** (0 to 24,576 tokens) per API call to control the trade-off:

* **Low/Zero Budget:** Prioritizes speed and low cost (very cheap: **$0.15 input / $0.60 output** per 1M tokens), great for simpler tasks.

* **Higher Budget:** Allows the model multi-step reasoning "thinking" for better accuracy on complex tasks, at a higher cost (**$3.50 output** per 1M tokens, including reasoning tokens).

This gives  granular control over the cost/quality balance *within the same model*.

**Performance & Specs:**Google claims strong performance, ranking just behind Gemini 2.5 Pro on Hard Prompts in ChatBot Arena and showing competitiveness against o4-mini and Sonnet 3.7 in their benchmarks, especially given the flexible pricing.

Key specs are right up there with the competition:

* **Multimodal Input:** Text, Images, Video, Audio

* **Context Window:** **1 million tokens** (matching GPT-4.1!)

* **Knowledge Cutoff:** January 2025

**How to Control Thinking:**Simply set the thinking_budget parameter in your API call (Python/JS examples available in their docs). If unspecified, the model decides automatically.

**My Take:** This is a smart play by Google. The controllable thinking budget is a unique and potentially powerful feature for optimizing across different use cases without juggling multiple models. With 1M context and competitive pricing, Gemini 2.5 Flash is immediately a major contender in the ongoing AI arms race. Definitely one to evaluate! Find more in the [developer docs](https://ai.google.dev/docs) and [Gemini Cookbook](https://ai.google.dev/examples).

Open Source: LLMs, Tools & more

OpenAI open sources MRCR eval and Codex (Mrcr [HF](https://huggingface.co/datasets/openai/mrcr), Codex [Github](https://github.com/openai/codex))

Let's face it, this isn't the open source OpenAI coverage I was hoping for, Sam promised us an open source model, and they are about to drop this, I'd assume close to Google IO (May 20th) to steal thunder. But OpenAI did make OpenSource waves this week in addition to the above huge stories. 

MRCR is a way to evaluate long context complex tasks, and they have taken this Gemini research and open sourced a dataset for this eval. üëè 

But also, they have dropped the Codex CLI tool, which is a coding partner using o4-mini and o3 and made that tool open source as well (Unlike anthropic with Claude Code), which in turn saw 86+ Pull Requests approved within the first 24 hours! 

The best part about this CLI, is that it's hardened security, using **Apple Seatbelt** which limits it execution to the current directory + temp files (on a mac at least) 

Other Open Source Updates

While OpenAI's contributions were notable, it wasn't the only action this week:

* **Microsoft's BitNet v1.5 (**[**HF**](https://huggingface.co/collections/microsoft/BitNet)**)**: Microsoft quietly dropped updates to BitNet, continuing their exploration into ultra-low-bit (ternary) models for efficiency. As Nisten pointed out on the show though, keep in mind these still use some higher-precision layers, so they aren't *purely* 1.5-bit in practice just yet. Important research nonetheless!

* **INTELLECT-2 Distributed RL (**[**Blog**](https://www.primeintellect.ai/blog/intellect-2)**, **[**X**](https://x.com/primeintellect_ai)**)**: Prime Intellect did something wild ‚Äì training **INTELLECT-2**, a 32B model, using globally distributed, permissionless reinforcement learning. Basically, anyone with a GPU could potentially contribute. Fascinating glimpse into decentralized training!

* [**Z.ai**](Z.ai)** (Formerly ChatGLM) & GLM-4 Family (**[**X**](https://x.com/Zai_org/status/1779846143024941199)**, **[**HF**](https://huggingface.co/collections/THUDM)**, **[**GitHub**](https://github.com/THUDM/GLM-4)**)**: The team behind ChatGLM rebranded to [**Z.ai**](Z.ai) and released their GLM-4 family (up to 32B parameters) under the very permissive **MIT license**. They're claiming performance competitive with much larger models like Qwen 72B, which is fantastic news for commercially usable open source!

ThursdAI - Recaps of the most high signal AI weekly spaces is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.

This Week's Buzz: Playground Updates & A Deep Dive into A2A

On the Weights & Biases front, it's all about enabling developers to navigate this new model landscape.

**Weave Playground Supports GPT-4.1 and o3/o4-mini (**[**X**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fweave_wb%2Fstatus%2F1912246450857341092)**)**

With all these new models dropping, how do you actually *choose* which one is best for *your* application? You need to evaluate! Our **W&B Weave Playground** now has full support for the new **GPT-4.1 family** and the **o3/o4-mini** models.

If you're using Weave to monitor your LLM apps in production, you can easily grab a trace of a real user interaction, open it in the Playground, and instantly retry that exact same call (with all its context and history) using any of the new models side-by-side. It‚Äôs the fastest way to see how o3 compares to 4.1-mini or how Claude 3.7 stacks up against o4-mini *on your specific data*. Essential for making informed decisions in this rapidly changing environment.

**Deep Dive: Understanding Google's A2A Protocol with Todd Segal**

This was a highlight of the show for me. We were joined by **Todd Segal**, a Principal Software Engineer at Google working directly on the new **Agent-to-Agent (A2A) protocol**. There was some confusion initially about how A2A relates to the increasingly popular **Model Context Protocol (MCP)**, so getting Todd's perspective was invaluable. W&B is a proud launch partner for the A2A protocol!

**Key Takeaways from our Chat:**

* **A2A vs. MCP: Complementary, Not Competitive:** Todd was clear: Google sees these as solving different problems. **MCP is for Agents talking to Tools** (structured, deterministic capabilities). **A2A is for Agents talking to other Agents** (unstructured, stateful, unpredictable, evolving interactions). Think of MCP like calling an API, and A2A like delegating a complex task to another expert service.

* **The Need for A2A:** It emerged from the need for specialized, domain-expert agents (built internally or by partners like Salesforce) to collaborate on complex, long-running tasks (e.g., booking a multi-vendor trip, coordinating an enterprise workflow) where simple tool calls aren't enough. Google's **Agent Space** product heavily utilizes A2A internally.

* **Capability Discovery & Registries:** A core concept is agents advertising their capabilities via an "agent card" (like a business card or resume). Todd envisions a future with multiple **registries** (public, private, enterprise-specific) where agents can discover other agents best suited for a task. This registry system is on the roadmap.

* **Async & Long-Running Tasks:** A2A is designed for tasks that might take minutes, hours, or even days. It uses a central **"Task" abstraction** which is stateful. Agents communicate updates (status changes, generated artifacts, requests for more info) related to that task.

* **Push Notifications:** For very long tasks, A2A supports a **push notification** mechanism. The client agent provides a secure callback URL, and the server agent can push updates (state changes, new artifacts) even if the primary connection is down. This avoids maintaining costly long-lived connections.

* **Multimodal Communication:** The protocol supports negotiation of modalities beyond text, including rendering content within **iframes** (for branded experiences) or exchanging **video/audio streams**. Essential for future rich interactions.

* **Security & Auth:** A2A deliberately **doesn't reinvent the wheel**. It relies on standard **HTTP headers** to carry authentication (OAuth tokens, internal enterprise credentials). Identity/auth handshakes happen "out of band" using existing protocols (OAuth, OIDC, etc.), and the resulting credentials are passed with A2A requests. Your user identity flows through standard mechanisms.

* **Observability:** Todd confirmed **OpenTelemetry (OTel)** support is planned for the SDKs. Treating agents like standard microservices means leveraging existing observability tools (like W&B Weave!) is crucial for tracing and debugging multi-agent workflows.

* **Open Governance:** While currently in a Google repo, the plan is to move A2A to a **neutral foundation** (like Linux Foundation) with a fully **open governance model**. They want this to be a true industry standard.

* **Getting Started:** Check out the **GitHub repo (**[**github.com/google/A2A**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fgoogle%2FA2A)**)**, participate in discussions, file issues, and send PRs!

My take: A2A feels like a necessary piece of infrastructure for the next phase of AI agents, enabling complex, coordinated actions across different systems and vendors. While MCP handles the "how" of using tools, A2A handles the "who" and "what" of inter-agent delegation. Exciting times ahead! Big thanks to Todd for shedding light on this.

Vision & Video: Veo-2 Arrives, Kling Gets Slicker

The visual AI space keeps advancing rapidly.

**Veo-2 Video Generation Hits GA in Vertex AI & Gemini App (**[**Blog**](https://developers.googleblog.com/en/veo-2-video-generation-now-generally-available/)**, **[***Try It***](http://ai.dev)**)**

Google's answer to Sora and Kling, **Veo-2**, is now **Generally Available (GA)** for all Google Cloud customers via **Vertex AI**. You can also access it in the **Gemini app**.

Veo-2 produces stunningly realistic and coherent video, making it a top contender alongside OpenAI's Sora and  Kling. Having it easily accessible in Vertex AI is a big plus for developers on Google Cloud.

I've tried and keep tyring all of them, VEO2 is an absolute beast in realism. 

**Kling 2.0 Creative Suite: A One-Stop Shop for Video AI? (**[**X**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Faltryne%2Fstatus%2F1912043121497850242)**, **[**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fklingai.com)**)**

Kuaishou's **Kling** model also got a major upgrade, evolving into a full **Kling 2.0 Creative Suite**.

*Anecdote:* I actually stayed up quite late one night trying to piece together info from a Chinese live stream about this release! The dedication is real, folks. üòÇ

**What's New:**

* **Kling 2.0 Master:** The core video model, promising better motion, physics, and facial consistency (still 5s clips for now, but 30s/4K planned).

* **Kolors 2.0:** An integrated image generation and restyling model (think Midjourney-style filters).

* **MVL (Multimodal Visual Language) Prompting:** This is killer! You can now **inline images directly within your text prompt** for precise control (e.g., "Swap the hoodie in @video1 with the style of @image2"). This offers granular control artists have been craving.

* **Multi-Elements Editor:** A timeline-based editor to stitch clips, add lip-sync, sound effects (including generated ones like "car horn"), and music.

* **Global Access:** No more Chinese phone number requirement! Available worldwide at [**klingai.com**](klingai.com).

* **Official API via FAL:** Developers can now integrate Kling 2.0 via our friends at **‚ö° FAL Generative Media Cloud**.

Kling is clearly aiming to be a holistic creative platform, reducing the need to jump between 17 different AI tools for image gen, video gen, editing, and sound. The MVL prompting is particularly innovative. Very impressive package.

Voice & Audio: Talking to Dolphins? üê¨

**DolphinGemma: Google AI Listens to Flipper (**[**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fblog.google%2Ftechnology%2Fai%2Fdolphingemma%2F)**)**

In perhaps the most delightful news of the week, Google, in collaboration with Georgia Tech and the Wild Dolphin Project, announced DolphinGemma. 

It's a ~400M parameter audio model based on the Gemma architecture (using SoundStream for audio tokenization) trained specifically on decades of recorded dolphin clicks, whistles, and pulses.The goal? To decipher the potential syntax and structure within dolphin communication and eventually enable rudimentary two-way interaction using underwater communication devices. It runs on a Pixel phone for field deployment.

This is just awesome. Using AI not just for human tasks but to potentially bridge the communication gap with other intelligent species is genuinely inspiring. We joked on the show about doing a segment of just dolphin noises ‚Äì maybe next time if DolphinGemma gets an API! ü§£

AI Art & Diffusion & 3D: Seedream Challenges the Champs

**Seedream 3.0: ByteDance's Bilingual Image Powerhouse (**[**Tech post**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fteam.doubao.com%2Fen%2Ftech%2Fseedream3_0)**, **[**arXiv**](https://www.google.com/url?sa=E&q=https%3A%2F%2Farxiv.org%2Fabs%2F2504.11346)**, **[**AIbase news**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.aibase.com%2Fnews%2F17208)**)**

ByteDance wasn't just busy with video; their Seed team announced **Seedream 3.0**, a powerful bilingual text-to-image model.

**Highlights:**

* Generates native **2048x2048** images.

* Fast inference (**~3 seconds** for 1Kx1K on an A100).

* Excellent **bilingual (Chinese/English) text rendering**, even small fonts.

* Uses **Scaled-ROPE-v2** for better high-resolution generation without artifacts.

* Claims to outperform SDXL-Turbo and Qwen-Image on fidelity and prompt adherence benchmarks.

* Available via Python SDK and REST API within their Doubao Studio and coming soon to [dreamina.com](http://dreamina.com) 

Phew! We made it. What an absolute avalanche of news. OpenAI truly dominated with the back-to-back launches of the hyper-capable o3/o4-mini and the massively scaled GPT-4.1 family. Google countered strongly with the versatile Gemini 2.5 Flash, key GA releases like Veo-2, and the strategically important A2A protocol. The agent ecosystem took huge leaps forward with both A2A and broader MCP adoption. And we saw continued innovation in multimodal embeddings, video generation, and even niche areas like bioacoustics and low-bit models.

If you feel like you missed anything (entirely possible this week!), the TL;DR and links below should help. Please subscribe if you haven't already, and share this with a friend if you found it useful ‚Äì it's the best way to support the show!

I have a feeling next week won't be any slower. Follow us on X/Twitter for breaking news between shows!

Thanks for tuning in, keep building, keep learning, and I'll see you next Thursday!

Alex

TL;DR and Show Notes

*Everything we covered today in bite-sized pieces with links!*

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([**@altryne**](https://www.google.com/url?sa=E&q=http%3A%2F%2Fx.com%2F%40altryne))

* Co Hosts - [**@WolframRvnwlf**](https://www.google.com/url?sa=E&q=http%3A%2F%2Fx.com%2F%40WolframRvnwlf) [**@yampeleg**](https://www.google.com/url?sa=E&q=x.com%2F%40yampeleg) [**@nisten**](https://www.google.com/url?sa=E&q=http%3A%2F%2Fx.com%2F%40nisten) [**@ldjconfirmed**](https://www.google.com/url?sa=E&q=http%3A%2F%2Fx.com%2F%40ldjconfirmed))

* Todd Segal - Principal Software Engineer @ Google - Working on A2A Protocol

* **Big CO LLMs + APIs**

* üëë OpenAI launches **o3** and **o4-mini** in chatGPT & API ([**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fopenai.com%2Findex%2Fintroducing-o3-and-o4-mini%2F), [**Our Coverage**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fyoutube.com%2Flive%2F2G-VwWxKCkk), [**o3 and o4-mini announcement**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fopenai.com%2Findex%2Fintroducing-o3-and-o4-mini%2F))

* OpenAI launches **GPT 4.1, 4.1-mini and 4.1-nano** in **API** ([**Our Coverage**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.youtube.com%2Flive%2FA5-Zxj816J0), [**Prompting guide**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fnoahmacca%2Fstatus%2F1911898549308280911))

* üö® Google launches **Gemini 2.5 Flash** with controllable thinking budgets ([**Blog Post - Placeholder Link**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fblog.google%2Ftechnology%2Fai%2Fgoogle-gemini-update-flash-extension%2F), [**API Docs**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fai.google.dev%2Fdocs))

* Mistral classifiers Factory

* Claude does research + workspace integration ([**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.anthropic.com%2Fnews%2Fresearch))

* Cohere Embed‚Äë4 ‚Äî Multimodal embeddings for enterprise search ([**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fcohere.com%2Fblog%2Fembed-4), [**Docs Changelog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdocs.cohere.com%2Fv2%2Fchangelog%2Fembed-multimodal-v4), [**X**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fcohere%2Fstatus%2F1912128813104078999))

* **Open Source LLMs**

* OpenAI open sources MRCR Long‚ÄëContext Benchmark ([**Hugging Face**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fopenai%2Fmrcr))

* Microsoft BitNet v1.5 ([**HF**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fmicrosoft%2FBitNet))

* INTELLECT‚Äë2 ‚Äî Prime Intellect‚Äôs 32B ‚Äúglobally‚Äëdistributed RL‚Äù experiment ([**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.primeintellect.ai%2Fblog%2Fintellect-2), [**X**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fprimeintellect_ai))

* [Z.ai](Z.ai) (previously chatGLM) + GLM‚Äë4‚Äë0414 open‚Äësource family ([**X**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FZai_org%2Fstatus%2F1779846143024941199), [**HF Collection**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2Fcollections%2FTHUDM), [**GitHub**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2FTHUDM%2FGLM-4))

* **This weeks Buzz + MCP/A2A**

* Weave playground support for GPT 4.1 and o3/o4-mini models ([**X**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fweave_wb%2Fstatus%2F1912246450857341092))

* Chat with Todd Segal - A2A Protocol ([**GitHub Spec**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fgoogle%2FA2A))

* **Vision & Video**

* **Veo‚Äë2 Video Generation in GA, Gemini App** ([**Dev Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fveo-2-video-generation-now-generally-available%2F))

* **Kling 2.0 Creative Suite** ([**X**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Faltryne%2Fstatus%2F1912043121497850242), [**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fklingai.com))

* ByteDance public Seaweed-7B, a video generation foundation model ([**seaweed.video**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fseaweed.video%2F))

* **Voice & Audio**

* **DolphinGemma** ‚Äî Google AI tackles dolphin communication ([**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fblog.google%2Ftechnology%2Fai%2Fdolphingemma%2F))

* **AI Art & Diffusion & 3D**

* **Seedream 3.0 bilingual image diffusion ‚Äì ByteDance** ([**Tech post**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fteam.doubao.com%2Fen%2Ftech%2Fseedream3_0), [**arXiv**](https://www.google.com/url?sa=E&q=https%3A%2F%2Farxiv.org%2Fabs%2F2504.11346), [**AIbase news**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.aibase.com%2Fnews%2F17208))

* **Tools**

* OpenAI debuts Codex CLI, an open source coding tool for terminals ([**Github**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fopenai%2Fcodex))

* Use o3 with Windsurf (which OpenAI is rumored to buy at $3B) via the mac app integration + write back + multiple files 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-apr-17-openai-o3-is-sota/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-apr-17-openai-o3-is-sota?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2MTU2NjkzOCwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.NhVIknHwPyAwb64UpE1sqJDUBHbjL5K6_XdUSHuXvG0&utm_campaign=CTA_5).

---

## üíØ ThursdAI - 100th episode üéâ - Meta LLama 4, Google tons of updates, ChatGPT memory, WandB MCP manifesto & more AI news

**Date:** April 10, 2025  
**Duration:** 1:32:18  
**Link:** [https://sub.thursdai.news/p/thursdai-100th-episode-meta-llama](https://sub.thursdai.news/p/thursdai-100th-episode-meta-llama)

Hey Folks, 

Alex here, celebrating an absolutely crazy (to me) milestone, of #100 episodes of ThursdAI üëè 100 episodes in a year and a half (as I [started publishing](https://sub.thursdai.news/p/thursdai-july-12-show-recap-notes) much later than I started going live, and the first episode was embarrassing), 100 episodes that documented INCREDIBLE AI progress, we mention on the show today, we used to be excited by context windows jumping from 4K to 16K! 

I want to extend a huge thank you to every one of you, who subscribes, listens to the show on podcasts, joins the live recording (we regularly get over 1K live viewers across platforms), shares with friends and highest thank you for the paid supporters! ü´∂ Sharing the AI news progress with you, energizes me to keep going, despite the absolute avalanche of news every week.

And what a perfect way to celebrate the 100th episode, on a week that Meta dropped Llama 4, sending the open-source world into a frenzy (and a bit of chaos). Google unleashed a firehose of announcements at Google Next. The agent ecosystem got a massive boost with MCP and A2A developments. And we had fantastic guests join us ‚Äì Michael Lou diving deep into the impressive DeepCoder-14B, and Liad Yosef & Ido Salomon sharing their wild ride creating the viral GitMCP tool.

I really loved today's show, and I encourage those of you who only read, to give this a watch/listen, and those of you who only listen, enjoy the recorded version (though longer and less edited!) 

Now let's dive in, there's a LOT to talk about (TL;DR and show notes as always, at the end of the newsletter) 

Open Source AI & LLMs: Llama 4 Takes Center Stage (Amidst Some Drama)

**Meta drops Llama 4 - Scout 109B/17BA & Maverick 400B/17BA **([Blog](https://ai.meta.com/blog/llama-4-multimodal-intelligence/), [HF](https://huggingface.co/meta-llama), [Try It](https://meta.ai/))

This was by far the biggest news of this last week, and it dropped... on a Saturday? (I was on the mountain ‚õ∑Ô∏è! What are you doing Zuck) 

Meta dropped the long awaited LLama-4 models, huge ones this time

* Llama 4 **Scout**: 17B active parameters out of ~109B total (16 experts).

* Llama 4 **Maverick**: 17B active parameters out of a whopping ~400B total (128 experts).

* Unreleased: **Behemoth** - 288B active with 2 Trillion total parameters chonker!

* Both base and instruct finetuned models were released

These new models are all Multimodal, Multilingual MoE (mixture of experts) architecture, and were trained with FP8, for significantly more tokens (around 30 Trillion Tokens!) with interleaved attention (iRoPE), and a refined SFT > RL > DPO post-training pipeline.

The biggest highlight is the stated context windows, 10M for Scout and 1M for Maverick, which is insane (and honestly, I haven't yet seen a provider that is even remotely able to support anything of this length, nor do I have the tokens to verify it) 

The messy release - Big Oof from Big Zuck

Not only did Meta release on a Saturday, messing up people's weekends, Meta apparently announced a high LM arena score, but the model they provided to LMArena was... not the model they released!?

It caused LMArena to release the 2000 chats dataset, and truly, some examples are quite damning and show just how unreliable LMArena can be as vibe eval. 

Additionally, during the next days, folks noticed discrepancies between the stated eval scores Meta released, and the ability to evaluate them independently, including our own Wolfram, who noticed that a quantized version of Scout, performed better on his laptop while HIGHLY quantized (read: reduced precision) than it was performing on the Together API inference endpoint!? 

We've chatted on the show that this may be due to some VLLM issues, and speculated about other potential reasons for this. 

Worth noting the official response from Ahmad Al-Dahle, head of LLama at Meta, who mentioned stability issues between providers and absolutely denied any training on any benchmarks

Too big for its own good (and us?)

One of the main criticism the OSS community had about these releases, is that for many of us, the reason for celebrating Open Source AI, is the ability to run models without network, privately on our own devices. 

Llama 3 was released in 8-70B distilled versions and that was incredible for us local AI enthusiasts! These models, despite being "only" 17B active params, are huge and way to big to run on most local hardware, and so the question then is, if we're getting a model that HAS to run on a service, why not use Gemini 2.5 that's MUCH better and faster and cheaper than LLama?  

Why didn't Meta release those sizes? Was it due to an inability to beat Qwen/DeepSeek enough? ü§î 

My Take

Despite the absolutely chaotic rollout, this is still a monumental effort from Meta. They spent *millions* on compute and salaries to give this to the community. Yes, no papers yet, the LM Arena thing was weird, and the inference wasn't ready. But Meta is standing up for Western open-source in a big way. We *have* to celebrate the core contribution while demanding better rollout practices next time. As Wolfram rightly said, the real test will be the fine-tunes and distillations the community builds on these base models. Releasing the base weights is crucial for that. Let's see if the community can tame this beast once the inference dust settles. Shout out to Ahmed Al-Dahle and the whole Llama team at Meta ‚Äì incredible work, messy launch, but thank you for pushing open source forward. üéâ

Together AI & Agentica (Berkley) finetuned DeepCoder-14B with reasoning ([X](https://x.com/togethercompute/status/1909697124805333208), [Blog](https://www.together.ai/blog/deepcoder))

Amidst the Llama noise, we got another stellar open-source release! We were thrilled to have Michael Lou from Agentica/UC Berkeley join us to talk about DeepCoder-14B-Preview which beats DeepSeek R1 and even o3-mini on several coding benchmarks. 

Using distributed Reinforcement Learning (RL), it achieves 60.6% Pass@1 accuracy on LiveCodeBench, matching the performance of models like o3-mini-2025-01-31 (Low) despite its smaller size.

The stated purpose of the project is to democratize RL and they have open sourced the model ([HF](https://huggingface.co/agentica-org/DeepCoder-14B-Preview)), the dataset ([HF](https://huggingface.co/datasets/agentica-org/DeepCoder-Preview-Dataset)), the Weights & Biases [logs](https://wandb.ai/mluo/deepcoder) and even the [eval logs](https://drive.google.com/file/d/1tr_xXvCJnjU0tLO7DNtFL85GIr3aGYln/view?usp=sharing)! 

Shout out to Michael, Sijun and Alpay and the rest of the team who worked on this awesome model! 

NVIDIA Nemotron ULTRA is finally here, 253B pruned Llama 3-405B ([HF](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1))

While Llama 4 was wrapped in mystery, NVIDIA dropped their pruned and distilled finetune of the previous Llama chonker 405B model, turning at just about half the parameters. 

And they were able to include the LLama-4 benchmarks in their release, showing that the older Llama, finetuned can absolutely beat the new ones at AIME, GPQA and more. 

As a reminder, we covered the previous 2 NEMOTRONS and they are a combined reasoning and non reasoning models, so the jump is not that surprising, and it does seem like a bit of eval cherry picking happened here. 

Nemotron Ultra supports 128K context and fits on a single 8xH100 node for inference. Built on open Llama models and trained on vetted + synthetic data, it's commercially viable. Shout out to NVIDIA for releasing this, and especially for pushing open reasoning datasets which Nisten rightly praised as having long-term value beyond the models themselves.

**More Open Source Goodness: Jina, DeepCogito, Kimi**

The open-source train didn't stop there:

* **Jina Reranker M0:** Our friends at Jina released a state-of-the-art *multimodal* reranker model. If you're doing RAG with images and text, this looks super useful for improving retrieval quality across languages and modalities ([Blog](https://jina.ai/news/jina-reranker-m0-multilingual-multimodal-document-reranker/), [HF](https://huggingface.co/jinaai/jina-reranker-m0))

* **DeepCogito:** A new company emerged releasing a suite of Llama fine-tunes (3B up to 70B planned, with larger ones coming) trained using a technique they call Iterated Distillation and Amplification (IDA). They claim their 70B model beats DeepSeek V2 70B on some benchmarks . Definitely one to watch. ([Blog](https://www.deepcogito.com/research/cogito-v1-preview), [HF](https://huggingface.co/deepcogito/cogito-v1-preview-llama-70B))

* **Kimi-VL & Kimi-VL-Thinking:**  MoonShot, who sometimes get lost in the noise, released incredibly impressive Kimi Vision Language Models (VLMs). These are MoE models with only ~3 Billion active parameters, yet they're showing results on par with or even beating models 10x larger (like Gemma 2 27B) on benchmarks like MathVision and ScreenSpot. They handle high-res images, support 128k context, and crucially, include a *reasoning* VLM variant. Plus, they're MIT licensed! Nisten's been following Kimi and thinks they're legit, just waiting for easier ways to run them locally. Definitely keep an eye on Kimi. ([HF](https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85))

This Week's Buzz from Weights & Biases - Observable Tools & A2A!

This week was personally very exciting on the W&B front, as I spearheaded and launched initiatives directly related to the MCP and A2A news!

**W&B launches the **[**observable.tools**](observable.tools)** initiative!**

As MCP takes off, one challenge becomes clear: observability. When your agent calls an external MCP tool, that part of the execution chain becomes a black box. You lose the end-to-end visibility crucial for debugging and evaluation.

That's why I'm thrilled that we launched **Observable Tools (**[**Website**](https://observable.tools)**)** ‚Äì an initiative championing full-stack agent observability, specifically within the MCP ecosystem. Our vision is to enable developers using tools like W&B Weave to see *inside* those MCP tool calls, getting a complete trace of every step.

The core of this is **Proposal **[**RFC 269**](https://wandb.me/mcp-spec)** on the official MCP GitHub spec**, which I authored! (My first RFC, quite the learning experience!). It details how to integrate OpenTelemetry tracing directly into the MCP protocol, allowing tools to securely report detailed execution spans back to the calling client (agent). We went deep on the spec, outlining transmission mechanisms, schemas, and rationale.

**My ask to you, the ThursdAI community:** Please check out [**observable.tools**](observable.tools), read the manifesto, watch the fun video we made, and most importantly, **go to the RFC 269 proposal (shortcut: **[**wandb.me/mcp-spec)**](oneb.me/mcp-spec)). Read it, comment, give feedback, and upvote if you agree! We need community support to make this impossible for the MCP maintainers to ignore. Let's make observability a first-class citizen in the MCP world! We also invite our friends from across the LLM observability landscape (LangSmith, Braintrust, Arize, Galileo, etc.) to join the discussion and collaborate.

**W&B is a Launch Partner for Google's A2A**

As mentioned earlier, we're also excited to be a launch partner for Google's new [Agent2Agent](https://wandb.ai/wandb_fc/product-announcements-fc/reports/Powering-Agent-Collaboration-Weights-Biases-Partners-with-Google-Cloud-on-Agent2Agent-Interoperability-Protocol---VmlldzoxMjE3NDg3OA) (A2A) protocol. We believe standardized communication *between* agents is the next critical step, and we'll be supporting A2A alongside MCP in our tools. Exciting times for agent infrastructure! I've invited Google folks to next week to discuss the protocol in depth! 

ThursdAI - Recaps of the most high signal AI weekly spaces is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.

Big Company LLMs + APIs: Google's Onslaught & OpenAI's Memory Upgrade

While open source had a wild week, the big players weren't sleeping. Google especially came out swinging at Google Next.

**Google announces TONS of new things at Next üôå  (**[**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fblog.google%2Fproducts%2Fgoogle-cloud%2Fnext-2025%2F)**)**

Google I/O felt like a preview, Google Next felt like the delivery truck backing up and dumping everything. Here's the whirlwind tour:

* **Gemini 2.5 Flash API:** The faster, cheaper Gemini 2.5 model is coming soon to Vertex AI. (Still waiting on that general API access!).

* **Veo 2 Editing:** Their top-tier video model (competing with Sora, Kling) gets editing capabilities. Very cool.

* **Imagen 3 Updates:** Their image model gets improvements, including inpainting.

* **Lyria:** Text-to-music model moves into preview.

* **TPU v7 (Ironwood):** New TPU generation coming soon. As Nisten noted, Google's infrastructure uptime is consistently amazing, which could be a winning factor regardless of model SOTA status.

* **Chirp 3 HD Voices + Voice Cloning:** This one raised eyebrows. The notes mentioned HD voices *and* voice cloning. Cloning is a touchy subject the big players usually avoid publicly (copyright, deepfakes). Still digging for confirmation/details on this ‚Äì if Google is really offering public voice cloning, that's huge. Let me know if you find a link!

* **Deep Research gets Gemini 2.5 Pro:** The experimental deep research feature in Gemini (their answer to OpenAI's research agent) now uses the powerful 2.5 Pro model. Google released comparison stats showing users strongly prefer it (70%) over OpenAI's offering, citing better instruction following and comprehensiveness. I haven't fully tested the 2.5 version yet, but the free tier access is amazing. and just look at those differences in preference compared to OAI Deep Research! 

**Firebase Studio **([firebase.studio](https://firebase.studio/))**:** Remember Project IDX? It's been rebranded and launched as Firebase Studio. This is Google's answer to the wave of "vibe coding" web builders like Lovable, Bolt and a few more. It's a full-stack, cloud-based GenAI environment for building, testing, and deploying apps, integrated with Firebase and likely Gemini. Looks promising!

**Google Embraces MCP & Launches A2A Protocol!**

Two massive protocol announcements from Google that signal the maturation of the AI agent ecosystem:

* **Official MCP Support! (**[**X**](https://twitter.com/demishassabis/status/1910107859041271977)**)**This is huge. Following Microsoft and AWS, Google (via both Sundar Pichai and Demis Hassabis) announced official support for Anthropic's Model Context Protocol (MCP) in Gemini models and SDKs. MCP is rapidly becoming *the* standard for how agents discover and use tools securely and efficiently. With Google onboard, there's basically universal major vendor support. MCP is here to stay.

* **Agent2Agent (A2A) Protocol (**[**Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdevelopers.googleblog.com%2Fen%2Fa2a-a-new-era-of-agent-interoperability%2F)** , **[**Spec**](https://github.com/google/A2A)**, **[**W&B Blog**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwandb.ai%2Fwandb_fc%2Fproduct-announcements-fc%2Freports%2FPowering-Agent-Collaboration-Weights-Biases-Partners-with-Google-Cloud-on-Agent2Agent-Interoperability-Protocol---VmlldzoxMjE3NDg3OA)**)**Google also launched a *new* open standard, A2A, designed for interoperability *between* different AI agents. Think of agents built by different vendors (Salesforce, ServiceNow, etc.) needing to talk to each other securely to coordinate complex workflows across enterprise systems. Built on web standards (HTTP, SSE, JSON-RPC), it handles discovery, task management (long-running!), and modality negotiation. Importantly, Google positions A2A as *complementary* to MCP, not competitive. MCP is how an agent uses a *tool*, A2A is how an *agent* talks to *another agent*. Weights & Biases is proud to be one of the 50+ launch partners working with Google on this! We'll do a deeper dive soon, but this + MCP feels like the foundation for a truly interconnected agent future.

**Cloudflare - new Agents SDK (**[agents.cloudflare.com](https://agents.cloudflare.com/))

Speaking of agents, Cloudflare launched their new Agents SDK (npm i agents). Built on their serverless infrastructure (Workers, Durable Objects), it offers a platform for building stateful, autonomous AI agents with a compelling pricing model (pay for CPU time, not wall time). This ties into the GitMCP story later ‚Äì Cloudflare is betting big on the edge agent ecosystem.

**Other Big Co News:**

* **Anthropic MAX:** A new $200/month tier for Claude, offering higher usage quotas but no new models. Meh.

* **Grok 3 API:** Elon's xAI finally launched the API tier for Grok 3 (plus Fast and Mini variants). Now you can test its capabilities yourself. We're still waiting for the promised Open Source Grok-2

**üö® BREAKING NEWS üö® OpenAI Upgrades Memory**

Right on cue during the show, OpenAI dropped a feature update! Sam Altman hyped *something* coming, and while it wasn't the o3/o4-mini models (those are coming next), it's a significant enhancement to **ChatGPT Memory**.

Previously, Memory tried to selectively save key facts. Now, when enabled, it can **reference ALL of your past chats** to personalize responses. Preferences, interests, past projects ‚Äì it can potentially draw on everything. OpenAI states there's **no storage limit** for what it can reference.

How? Likely some sophisticated RAG/vector search under the hood, not stuffing everything into context. LDJ mentioned he might have had this rolling out silently for weeks, and while the immediate difference wasn't huge, the potential is massive as models get better at utilizing this vast personal context.

The immediate reaction? Excitement mixed with a bit of caution. As Wolfram pointed out, do I *really* want it remembering *every* single chat? Configurable memory (flagging chats for inclusion/exclusion) seems like a necessary follow-up. Thanks for the feature request, Wolfram! (And yes, Europe might not get this right away anyway...). This could finally stop ChatGPT from asking me basic questions it should know from our history!

Prompt suggestion: Ask the new chatGPT with memory, a think that you asked it that you likely forgot.

Just don't asked it what was the most boring thing you asked it, I got cooked I'm still feeling raw üòÇ 

Vision & Video: Kimi Drops Tiny But Mighty VLMs

The most impressive long form AI video paper dropped, showing that it's possible to create 1 minute long video, with incredible character and scene consistency

This [paper](https://t.co/agJKUAExpz) introduces TTT layers (Test Time Training) to a pre-trained transformer, allowing it to one shot generate these incredibly consistent long scenes. Can't wait to see how the future of AI video evolves with this progress! 

AI Art & Diffusion & 3D: HiDream Takes the Open Crown

**HiDream-I1-Dev 17B MIT license new leading open weights image gen! (**[**HF**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2Fcollections%2FHiDream-ai%2Fhidream-i1-67f3e90dd509fed088a158b3)**)**

Just when we thought the image gen space was settling, HiDream, a Chinese company, open-sourced their HiDream-I1 family under MIT license! This 17B parameter model comes in Dev, Full, and Fast variants.

The exciting part? Based on early benchmarks (like Artificial Analysis Image Arena), **HiDream-I1-Dev surpasses Flux 1.1 [Pro]**, Recraft V3, Reve and Imagen 3 while being open source! It boasts outstanding prompt following and text rendering capabilities.

HiDream's API is coming soon too and I really hope it's finetunable! 

Tools: GitMCP - The Little Repo Tool That Could

GitMCP - turn any github repo into an MCP server ([website](https://gitmcp.io))

We wrapped up the show with a fantastic story from the community. We had Liad Yosef (Shopify) and Ido Salomon (Palo Alto Networks) join us to talk about **GitMCP**.

It started with a simple problem: a 3MB LLM.txt file (a format proposed by Jeremy Howard for repo documentation) too large for context windows. Liad and Ido, working nights and weekends, built an MCP server that could ingest any GitHub repo (prioritizing LLM.txt if present, falling back to Readmes/code comments) and expose its documentation via MCP tools (semantic search, fetching).

This means any MCP-compatible client (like Cursor, potentially future ChatGPT/Gemini) can instantly query the documentation of *any* public GitHub repo just by pointing to the GitMCP URL for that repo (e.g., [https://gitmcp.io/user/repo).](https://gitmcp.io/user/repo).) As Yam Peleg pointed out during the show, the genius here is dynamically generating a *customized* tool specifically for that repo, making it incredibly easy for the LLM to use.

Then, the story got crazy. They launched, went viral, almost melted their initial Vercel serverless setup due to traffic and SSE connection costs (100$+ per hour!). DMs flew back and forth with Vercel's CEO, then Cloudflare's CTO swooped in offering to sponsor hosting on Cloudflare's *unreleased *Agents platform if they migrated *immediately*. A frantic weekend of coding ensued, culminating in a nail-biting domain switch and a temporary outage before getting everything stable on Cloudflare.

The project has received massive praise (including from Jeremy Howard himself) and is solving a real pain point for developers wanting to easily ground LLMs in project documentation. Huge congrats to Liad and Ido for the amazing work and the wild ride! Check out gitmcp.io!

Wrapping Up Episode 100!

Whew! What a show. From the Llama 4 rollercoaster to Google's AI barrage, the rise of agent standards like MCP and A2A, groundbreaking open source models, and incredible community stories like GitMCP ‚Äì this episode truly showed an exemplary week in AI and underlined the reason I do this every week. It's really hard to keep up, and so if I commit to you guys, I stay up to date myself!  

Hitting 100 episodes feels surreal. It's been an absolute privilege sharing this journey with Wolfram, LDJ, Nisten, Yam, all our guests, and all of you. Seeing the community grow, hitting milestones like 1000 YouTube subscribers today, fuels us to keep going üéâ 

The pace isn't slowing down. If anything, it's accelerating. But we'll be right here, every Thursday, trying to make sense of it all, together.

If you missed anything, don't worry! Subscribe to the ThursdAI News Substack for the full TL;DR and links below.

Thanks again for making 100 episodes possible. Here's to the next 100! ü•Ç

Keep tinkering, keep learning, and I'll see you next week.

Alex

**TL;DR and Show Notes**

* **Hosts and Guests**

* **Alex Volkov** - AI Evangelist & Weights & Biases ([@altryne](http://x.com/@altryne))

* Co Hosts - [@WolframRvnwlf](http://x.com/@WolframRvnwlf) [@yampeleg](http://x.com/@yampeleg) [@nisten](http://x.com/@nisten) [@ldjconfirmed](http://x.com/@ldjconfirmed)

* **Michael Luo **[@michaelzluo](http://x.com/michaelzluo) - CS PhD @ UC Berkeley; AI & Systems

* **Liad Yosef **([@liadyosef](https://x.com/liadyosef)), **Ido Salomon **([@idosal1](https://x.com/idosal1)) - GitMCP creators

* **Open Source LLMs** 

* Meta drops LLama 4 (Scout 109B/17BA & Maverick 400B/17BA) - ([Blog](https://ai.meta.com/blog/llama-4-multimodal-intelligence/), [HF](https://huggingface.co/meta-llama), [Try It](https://meta.ai/))

* Together AI and Agentica (UC Berkley) announce **DeepCoder-14B** ([X](https://x.com/togethercompute/status/1909697124805333208), [Blog](https://www.together.ai/blog/deepcoder))

* NVIDIA Nemotron Ultra is here! 253B pruned LLama 3-405B ([X](https://x.com/kuchaev/status/1909444566379573646), [HF](https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1))

* Jina Reranker M0 - SOTA multimodal reranker model ([Blog](https://jina.ai/news/jina-reranker-m0-multilingual-multimodal-document-reranker/), [HF](https://huggingface.co/jinaai/jina-reranker-m0))

* DeepCogito - SOTA models 3-70B - beating DeepSeek 70B - ([Blog](https://www.deepcogito.com/research/cogito-v1-preview), [HF](https://huggingface.co/deepcogito/cogito-v1-preview-llama-70B))

* ByteDance new release - [**Seed-Thinking-v1.5**](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5)

* **Big CO LLMs + APIs**

* Google announces TONS of new things üôå  ([Blog](https://blog.google/products/google-cloud/next-2025/))

* Google launches Firebase Studio ([website](https://firebase.studio/))

* Google is announcing official support for MCP ([X](https://x.com/demishassabis/status/1910107859041271977))

* Google announces A2A protocol - agent 2 agent communication ([Blog](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/), [Spec](https://github.com/google/A2A), [W&B Blog](https://wandb.ai/wandb_fc/product-announcements-fc/reports/Powering-Agent-Collaboration-Weights-Biases-Partners-with-Google-Cloud-on-Agent2Agent-Interoperability-Protocol---VmlldzoxMjE3NDg3OA))

* Cloudflare - new Agents SDK ([Website](https://agents.cloudflare.com/))

* Anthropic MAX - $200/mo with more quota

* Grok 3 finally launches API tier ([API](https://docs.x.ai/docs/models#models-and-pricing))

* OPenAI adds enhanced memory to ChatGPT - can remember all your chats ([X](https://x.com/OpenAI/status/1910378768172212636))

* **This weeks Buzz - MCP and A2A**

* W&B launches the [observable.tools](http://observable.tools) initiative & invite people to comment on the MCP [RFC](http://wandb.me/mcp-spec)

* W&B is the launch partner for Google's A2A ([Blog](https://wandb.ai/wandb_fc/product-announcements-fc/reports/Powering-Agent-Collaboration-Weights-Biases-Partners-with-Google-Cloud-on-Agent2Agent-Interoperability-Protocol---VmlldzoxMjE3NDg3OA))

* **Vision & Video**

* **Kimi-VL and Kimi-VL-Thinking - **A3B vision models (X, [HF](https://t.co/cgCMsiHN8p))

* One-Minute Video Generation with Test-Time Training ([Blog](https://t.co/BSHsucizoG), [Paper](https://t.co/agJKUAExpz))

* **Voice & Audio**

* Amazon - Nova Sonic - speech2speech foundational model ([Blog](https://www.aboutamazon.com/news/innovation-at-amazon/nova-sonic-voice-speech-foundation-model))

* **AI Art & Diffusion & 3D**

* **HiDream-I1-Dev **17B MIT license** **new leading open weights image gen 0 passes Flux1.1[pro] ! ([HF](https://huggingface.co/collections/HiDream-ai/hidream-i1-67f3e90dd509fed088a158b3))

* **Tools**

* GitMCP - turn any github repo into an MCP server ([try it](https://gitmcp.io/))

ThursdAI - Recaps of the most high signal AI weekly spaces is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber. 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-100th-episode-meta-llama/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-100th-episode-meta-llama?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2MTA1ODY0MywiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.NdR7554cAii5VG08n5tfRMYdwmT0MjBZKI_p1urv9tU&utm_campaign=CTA_5).

---

## ThursdAI - Apr 3rd - OpenAI Goes Open?! Gemini Crushes Math, AI Actors Go Hollywood & MCP, Now with Observability?

**Date:** April 03, 2025  
**Duration:** 1:37:33  
**Link:** [https://sub.thursdai.news/p/thursdai-apr-3rd-openai-goes-open](https://sub.thursdai.news/p/thursdai-apr-3rd-openai-goes-open)

Woo! Welcome back to ThursdAI, show number 99! Can you believe it? We are *one* show away from hitting the big 100, which is just wild to me. And speaking of milestones, we just crossed **100,000 downloads** on Substack alone! [Insert celebratory sound effect here üéâ]. Honestly, knowing so many of you tune in every week **genuinely fills me with joy**, but also a real commitment to keep bringing you the the high-signal, zero-fluff AI news you count on. Thank you for being part of this amazing community! üôè

And what a week it's been! I started out busy at work, playing with the native image generation in ChatGPT like everyone else (all 130 million of us!), and then I looked at my notes for today‚Ä¶ an absolute *mountain* of updates. Seriously, one of those weeks where open source just exploded, big companies dropped major news, and the vision/video space is producing stuff that's crossing the uncanny valley.

We‚Äôve got OpenAI teasing a big open source release (yes, *Open*AI might actually be *open* again!), Gemini 2.5 showing superhuman math skills, Amazon stepping into the agent ring, truly mind-blowing AI character generation from Meta, and a personal update on making the Model Context Protocol (MCP) observable. Plus, we had some fantastic guests join us live!

So buckle up, grab your coffee (or whatever gets you through the AI whirlwind), because we have a *lot* to cover. Let's dive in! (as always, show notes and links in the end)

OpenAI Makes Waves: Open Source Tease, Tough Evals & Billions Raised

It feels like OpenAI was determined to dominate the headlines this week, hitting us from multiple angles.

First, the potentially massive news: **OpenAI is planning to release a new open source model** in the "coming months"! Kevin Weil [**tweeted**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fkevinweil%2Fstatus%2F1906797119848988822) that they're working on a "highly capable open language model" and are actively seeking developer feedback through dedicated sessions ([**sign up here**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fopenai.com%2Fform%2Fopen-model-feedback) if interested) to "get this right." Word on the street is that this could be a powerful reasoning model. Sam Altman also cheekily added they won't slap on a Llama-style 8,300 tasks) and even includes meta-evaluation for the LLM judge they built ([**Nano-Eval framework also open sourced**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fopenai%2Fpreparedness)). The kicker? **Claude 3.5 Sonnet (New)** came out on top with just **21.0%** replication score (human PhDs got 41.4%). Props to OpenAI for releasing an eval where they don‚Äôt even win. That‚Äôs what real benchmarking integrity looks like. You can find the [**code on GitHub**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fopenai%2Fpreparedness%2Ftree%2Fmain%2Fproject%2Fpaperbench) and read the [**full paper here**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fcdn.openai.com%2Fpapers%2F22265bac-3191-44e5-b057-7aaacd8e90cd%2Fpaperbench.pdf).

Third, the casual [**40 Billion Dollars**](https://openai.com/index/investing‚àíin‚àíour‚àímission/)** **funding round led by SoftBank. Valuing the company at **300 Billion**. Yes, Billion with a B. More than Coke, more than Disney. The blog post was hilariously short for such a massive number. They also mentioned**500 million weekly ChatGPT users**and the insane onboarding rate (1M users/hr!) thanks to native image generation, especially seeing huge growth in India. The scale is just mind-boggling.

Oh, and for fun, try the new grumpy, EMO "[**Monday" voice**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FOpenAI%2Fstatus%2F1907124258867982338) in advanced voice mode. It's surprisingly entertaining.

Open Source Powerhouses: Nomic & OpenHands Deliver SOTA

Beyond the OpenAI buzz, the open source community delivered some absolute gems, and we had guests from two key projects join us!

Nomic Embed Multimodal: SOTA Embeddings for Visual Docs

Our friends at Nomic AI are back with a killer release! We had Zach Nussbaum on the show discussing [**Nomic Embed Multimodal**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.nomic.ai%2Fblog%2Fposts%2Fnomic-embed-multimodal). These are new 3B & 7B parameter embedding models ([**available on Hugging Face**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fnomic-ai%2Fnomic-embed-multimodal-67e5ddc1a890a19ff0d58073)) built on Alibaba's excellent Qwen2.5-VL. They achieved **SOTA on visual document retrieval** by cleverly embedding interleaved text-image sequences ‚Äì perfect for PDFs and complex webpages.

Zach highlighted that they chose the Qwen base because high-performing open VLMs under 3B params are still scarce, making it a solid foundation. Importantly, the 7B model comes with an **Apache 2.0 license**, and they've open sourced weights, code, and data. They offer both a powerful multi-vector version (ColNomic) and a faster single-vector one. Huge congrats to Nomic!

OpenHands LM 32B & Agent: Accessible SOTA Coding

Remember OpenDevin? It evolved into OpenHands, and the team just dropped their own [**OpenHands LM 32B**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.all-hands.dev%2Fblog%2Fintroducing-openhands-lm-32b----a-strong-open-coding-agent-model)! We chatted with co-founder Xingyao "Elle" Wang about this impressive Qwen 2.5 finetune ([**MIT licensed, on Hugging Face**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2Fall-hands%2Fopenhands-lm-32b-v0.1)).

It hits a remarkable **37.2% on SWE-Bench Verified **(a coding benchmark measuring real-world repo tasks), competing with much larger models. Elle stressed they didn't just chase code completion scores; they focused on tuning for *agentic capabilities* ‚Äì tool use, planning, self-correction ‚Äì using trajectories from their contamination-free Switch Execution dataset. This focus seems to be paying off, as the OpenHands *agent* also snagged the **#2 spot on the brand new Live SWE-Bench** leaderboard! Plus, the 32B model runs locally on a single 3090, making this power accessible. You can also try their managed [**OpenHands Cloud**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fapp.all-hands.dev%2F) ($50 free credit). Amazing progress from this team!

Frontiers: Diffusion LMs & Superhuman Math

Two other developments pushed the boundaries this week:

Dream 7B: A Diffusion Language Model Challenger?

This one's fascinating conceptually. Researchers unveiled [**Dream 7B**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhkunlp.github.io%2Fblog%2F2025%2Fdream%2F), a language model based on **diffusion**, not auto-regression. The [**benchmarks they shared**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FJiachengYe15%2Fstatus%2F1907430553369883017) show it competing strongly with top 7-8B models, and absolutely crushing tasks like Sudoku (81% vs <50% for others), potentially due to its parallel processing nature being better for global constraints. It's an exciting hint at alternative architectures, but the **model weights aren't out yet**, so we can't verify or play with it. Still, one to watch!

Gemini 2.5 Obliterates Olympiad Math (24.4% on USAMO!)

We already knew Gemini 2.5 was good, but wow. New results dropped showing its performance on the **USA Math Olympiad (USAMO)** ‚Äì problems so hard most top models score under 5%. **Gemini 2.5 Pro scored an incredible 24.4%**!

The gap between it and everything else is massive, highlighting the power of its reasoning and thinking capabilities (which you can inspect via its traces!). Having used it for complex tasks myself (like wrestling with tax forms!), I can attest to its depth. It's free in the Gemini app ‚Äì go try it!

Agents, Compute & Making MCP Observable

Amazon's Nova Act Agent & The Need for Access

Amazon entered the agent chat with [**Nova Act**](https://www.google.com/url?sa=E&q=https%3A%2F%2Flabs.amazon.science%2Fblog%2Fnova-act), designed for web browser actions. They claim it beats Claude 3.5 and OpenAI's QA model on some benchmarks, possibly leveraging acquired Adept talent. But... it's only available via an SDK with a request form. As Yam rightly pointed out on the show, these agent claims mean little until we can actually *use* them in the real world!

CoreWeave + NVIDIA = Insane Speeds

Hardware keeps accelerating. CoreWeave announced hitting [**800 Tokens/sec on Llama 3.1 405B**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.prnewswire.com%2Fnews-releases%2Fcoreweave-achieves-new-record-breaking-ai-inferencing-benchmark-with-nvidia-gb200-grace-blackwell-superchips-302418682.html%3Fhss_channel%3Dtw-979803443681349632) using NVIDIA's new GB200 Blackwell chips, and 33,000 T/s on Llama 2 70B with H200s. Inference is getting *fast*.

This Week's Buzz: Let's Make MCP Observable!

Okay, my personal mission this week builds on the growing **Model Context Protocol (MCP)** momentum. MCP is potentially the "HTTP for agents," enabling tool interoperability. But as tool use moves external, we lose visibility, making debugging and security harder.

That's why I'm launching the [**Observable Tools**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fobservable.tools%2F) initiative. The goal: integrate observability *into* the MCP standard itself. Right now, that link redirects to a [**GitHub discussion**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fmodel-context-protocol%2Fspecification%2Fdiscussions%2F18) where I've proposed using the **OpenTelemetry (OTel)** standard to add tracing to MCP interactions. This would give developers clear visibility into tool usage, regardless of their observability platform.

**I need your help!** Please check out the proposal, join the discussion, and **show your support** with a üëç or üöÄ on GitHub. We need the community voice to make this happen! (And yes, my [**viral tweet**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Faltryne%2Fstatus%2F1906180131540066796) showed there's huge demand for usable MCP clients too ‚Äì more on that soon!).

Vision & Video: Entering the Uncanny Valley

This space is moving at lightning speed.

[**Runway Gen-4**](https://www.google.com/url?sa=E&q=https%3A%2F%2Frunwayml.com%2Fresearch%2Fintroducing-runway-gen-4) was announced, pushing for better consistency in AI video. Here's a few example videos showing incredible character and world consistency:

ByteDance's impressive [**OmniHuman**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdreamina.capcut.com%2Fai-tool%2Fvideo%2Flip-sync%2Fgenerate) (single image to talking avatar) is now publicly usable via Dreamina website. For people it's really good, but for animated style images, [Hedra Labs](https://www.hedra.com/) feels actually better (and much much faster)

**Meta's **[**MoCHA**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fcongwei1230.github.io%2FMoCha%2F)** is mind-blowing.** We had researcher Cong Wei explain how it generates *movie-grade*, full-body, expressive talking characters directly from speech and text (no reference image needed!). Using Diffusion Transformers and clever attention mechanisms, the realism is startling, handling lip-sync, gestures, emotions, and even multi-character dialogue. Check the project page videos ‚Äì some are truly uncanny. Just look at this one!

Voice Highlight: Hailuo Speech-02

While Gladia launched their [**Solaria STT**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.gladia.io%2Fsolaria), the standout for me was [**Hailuo's Speech-02 TTS API**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FHailuo_AI%2Fstatus%2F1906723587379101923). The emotional control and voice cloning quality are, in my opinion, potentially SOTA right now, offering incredibly nuanced and realistic synthetic voices.

Tool Update & Breaking News!

* Google's [**NotebookLM now discovers related sources**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fblog.google%2Ftechnology%2Fgoogle-labs%2Fnotebooklm-discover-sources%2F) automatically.

* **BREAKING NEWS (Caught end of show): Devin 2.0 is out!** Cognition Labs launched their AI software engineer V2 with a new IDE experience and, crucially, a **$20/month** starting price. Much more accessible to try!

Phew! What a week. From OpenAI's big moves to Gemini's math prowess, stunning AI actors from Meta, and the push for an observable agent ecosystem ‚Äì the field is accelerating like crazy.

Alright folks, that‚Äôs a wrap for show #99! Thank you again for tuning in, for being part of the community, and for keeping us on our toes with your insights and feedback. Special thanks to our guests Zach Nussbaum (Nomic), Xingyao Wang (All Hands AI), and Cong Wei (Meta/MoCHA) for joining us!

If you missed any part of the show, or want to grab any of the links, head over to [**ThursdAI.news**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fthursdai.news). The full recording (video on YouTube, audio on Spotify, Apple Podcasts, etc.) and this blog post with all the notes will be up shortly.

The best way to support the show? Share it with a friend or colleague who needs to stay up-to-date on AI, and drop us a 5-star review on your podcast platform! Financial support via Substack is also appreciated but never required.

Get ready for **Episode 100** next week! Until then, happy tinkering, stay curious, and I'll see you next ThursdAI!

Bye bye everyone!

TL;DR and Show Notes

**Host, Guests, and Co-hosts**

* **Host:** Alex Volkov - AI Evangelist & Weights & Biases ([**@altryne**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Faltryne))

* **Co-Hosts:**

* LDJ ([**@ldjconfirmed**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fldjconfirmed))

* Yam Peleg ([**@yampeleg**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fyampeleg))

* **Guests:**

* Zach Nussbaum ([**@zach_nussbaum**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fzach_nussbaum)) - Nomic AI

* Xingyao Wang ([**@xingyaow_**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fxingyaow_)) - All Hands AI / OpenHands

* Cong Wei ([**@CongWei1230**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FCongWei1230)) - Meta AI / MoCha

**Key Topics & Links**

* **OpenAI's Big Week:**

* Teasing highly capable [**Open Source Reasoner Model**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fkevinweil%2Fstatus%2F1906797119848988822) (seeking [**feedback**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fopenai.com%2Fform%2Fopen-model-feedback)).

* Released [**PaperBench**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fopenai.com%2Findex%2Fpaperbench%2F) eval ([**code**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fopenai%2Fpreparedness%2Ftree%2Fmain%2Fproject%2Fpaperbench), [**paper**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fcdn.openai.com%2Fpapers%2F22265bac-3191-44e5-b057-7aaacd8e90cd%2Fpaperbench.pdf)) & [**Nano-Eval framework**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fopenai%2Fpreparedness).

* Raised [**$40B at $300B valuation**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fopenai.com%2Findex%2Finvesting-in-our-mission%2F).

* New EMO "[**Monday" voice**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FOpenAI%2Fstatus%2F1907124258867982338) in ChatGPT.

* **Open Source Powerhouses:**

* [**Nomic Embed Multimodal**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.nomic.ai%2Fblog%2Fposts%2Fnomic-embed-multimodal): SOTA visual doc embeddings (3B & 7B, [**Apache 2.0 for 7B**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fnomic-ai%2Fnomic-embed-multimodal-67e5ddc1a890a19ff0d58073)).

* [**OpenHands LM 32B**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.all-hands.dev%2Fblog%2Fintroducing-openhands-lm-32b----a-strong-open-coding-agent-model): SOTA-level coding agent model (Qwen finetune, [**MIT License**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhuggingface.co%2Fall-hands%2Fopenhands-lm-32b-v0.1), 37.2% SWE-Bench, #2 Live SWE-Bench). [**Cloud version available**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fapp.all-hands.dev%2F).

* **Frontier Models & Capabilities:**

* [**Dream 7B**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fhkunlp.github.io%2Fblog%2F2025%2Fdream%2F): Promising **diffusion LM** shows strong benchmark results ([**esp. Sudoku**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FJiachengYe15%2Fstatus%2F1907430553369883017)), but weights not yet released.

* **Gemini 2.5**: Crushes hard **USAMO math eval (24.4%** vs <5% for others), showcasing superior reasoning.

* **Agents & Compute:**

* Amazon's [**Nova Act**](https://www.google.com/url?sa=E&q=https%3A%2F%2Flabs.amazon.science%2Fblog%2Fnova-act) agent announced, claims SOTA but lacks public access ([**request form**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fnova.amazon.com)).

* [**CoreWeave/NVIDIA**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.prnewswire.com%2Fnews-releases%2Fcoreweave-achieves-new-record-breaking-ai-inferencing-benchmark-with-nvidia-gb200-grace-blackwell-superchips-302418682.html%3Fhss_channel%3Dtw-979803443681349632): Massive inference speedups (800T/s on Llama 405B with GB200).

* **This Week's Buzz - MCP:**

* [**Observable Tools**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fobservable.tools%2F) initiative launched to add observability to MCP.

* Proposal using OpenTelemetry posted for [**community feedback on GitHub**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fgithub.com%2Fmodel-context-protocol%2Fspecification%2Fdiscussions%2F18) - please support!

* Huge demand shown for usable MCP clients ([**viral tweet**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Faltryne%2Fstatus%2F1906180131540066796)).

* **Vision & Video Highlights:**

* [**Runway Gen-4**](https://www.google.com/url?sa=E&q=https%3A%2F%2Frunwayml.com%2Fresearch%2Fintroducing-runway-gen-4) focuses on video consistency.

* ByteDance [**OmniHuman**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fdreamina.capcut.com%2Fai-tool%2Fvideo%2Flip-sync%2Fgenerate) (image-to-avatar) now publicly available via Dreamina ([**example thread**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Faltryne%2Fstatus%2F1907173680456794187)).

* Meta's [**MoCHA**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fcongwei1230.github.io%2FMoCha%2F): Generates stunningly realistic, movie-grade talking characters from speech+text.

* **Voice Highlight:**

* [**Hailuo Speech-02**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2FHailuo_AI%2Fstatus%2F1906723587379101923): Impressive TTS API with excellent emotional control and voice cloning.

* **Tool Updates:**

* [**Windsurf adds deployments**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fx.com%2Fwindsurf_ai%2Fstatus%2F1907497638267924566) to Netlify.

* Google [**NotebookLM adds source discovery**](https://www.google.com/url?sa=E&q=https%3A%2F%2Fblog.google%2Ftechnology%2Fgoogle-labs%2Fnotebooklm-discover-sources%2F).

* **Breaking News:**

* **Devin 2.0** AI Software Engineer announced, starts at $20/month. 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-apr-3rd-openai-goes-open/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-apr-3rd-openai-goes-open?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE2MDUyMzI3MiwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.KeKD6SiF3Lcp_XPFGAtxcXxrkuK6rVe0xzzZqGXzZ8M&utm_campaign=CTA_5).

---

