# üìÜ ThursdAI - Aug 21 - DeepSeek V3.1‚Äôs hybrid upset, ByteDance‚Äôs 512K Seed-OSS, Nano Banana wizardry, Agents.md standardizes agents, and more AI

**Date:** August 21, 2025  
**Duration:** 1:06:24  
**Link:** [https://sub.thursdai.news/p/thursdai-aug-21-deepseek-v31s-hybrid](https://sub.thursdai.news/p/thursdai-aug-21-deepseek-v31s-hybrid)

---

## Description

Hey everyone, Alex here üëã

This week looked quiet‚Ä¶ until about 15 hours before we went live. Then the floodgates opened: DeepSeek dropped a hybrid V3.1 that beats their own R1 with fewer thinking tokens, ByteDance quietly shipped a 36B Apache-2.0 long-context family with a ‚Äúthinking budget‚Äù knob, NVIDIA pushed a faster mixed-architecture 9B with open training data, and a stealth image editor dubbed ‚ÄúNano Banana‚Äù started doing mind-bending scene edits that feel like a new tier of 3D-aware control. 

On the big-co side, a mystery ‚ÄúSonic‚Äù model appeared in Cursor and Cline (spoiler: the function call paths say a lot), and OpenAI introduced [Agents.md](Agents.md) to stop the config-file explosion in agentic dev tools. We also got a new open desktop-agent RL framework that 4x‚Äôd OSWorld SOTA, an IBM + NASA model for solar weather, and Qwen‚Äôs fully open 20B image editor that‚Äôs shockingly capable and runnable on your own GPU.

Our show today was one of the shortest yet, as I had to drop early to prepare for Burning Man üî•üï∫ Speaking of which, Wolfram and the team will host the next episode! 

Ok, let's dive in! 

DeepSeek V3.1: a faster hybrid that thinks less, scores more ([X](https://x.com/deepseek_ai/status/1958417062008918312), [HF](https://huggingface.co/deepseek-ai/DeepSeek-V3.1))

DeepSeek does this thing where they let a base artifact ‚Äúleak‚Äù onto Hugging Face, and the rumor mill goes into overdrive. Then, hours before we went live, the full V3.1 model card and an instruct variant dropped. The headline: it‚Äôs a hybrid reasoner that combines the strengths of their V3 (fast, non-thinking) and R1 (deep, RL-trained thinking), and on many tasks it hits R1-level scores with fewer thinking tokens. In human terms: you get similar or better quality, faster.

A few things I want to call out from the release and early testing:

* Hybrid reasoning mode done right. The model can plan with thinking tokens and then switch to non-thinking execution, so you don‚Äôt have to orchestrate two separate models. This alone simplifies agent frameworks: plan with thinking on, execute with thinking off.

* Thinking efficiency is real. DeepSeek shows curves where V3.1 reaches or surpasses R1 with significantly fewer thinking tokens. On AIME‚Äô25, for example, R1 clocks 87.5% with ~22k thinking tokens; V3.1 hits ~88.4 with ~15k. On GPQA Diamond, V3.1 basically matches R1 with roughly half the thinking budget.

* Tool-use and search-agent improvements. V3.1 puts tool calls inside the thinking process, instead of doing a monologue and only then calling tools. That‚Äôs the pattern you want for multi-turn research agents that iteratively query the web or your internal search.

* Long-context training was scaled up hard. DeepSeek says they increased the 32K extension phase to ~630B tokens, and the 128K phase to ~209B tokens. That‚Äôs a big bet on long-context quality at train time, not just inference-time RoPE tricks. The config shows a max position in the 160K range, with folks consistently running it in the 128K class.

* Benchmarks show the coding and terminal agent work got a big push. TerminalBench jumps from a painful 5.7 (R1) to 31 with V3.1. Codeforces ratings are up. On SweBench Verified (non-thinking), V3.1 posts  66 vs R1‚Äôs ~44. And you feel it: it‚Äôs faster to ‚Äúget to it‚Äù without noodling forever.

* API parity you‚Äôll actually use. Their API now supports the Anthropic-style interface as well, which means a bunch of editor integrations ‚Äújust work‚Äù with minimal glue. If you‚Äôre in a Claude-first workflow, you won‚Äôt have to rewire the world to try V3.1.

* License and availability. This release is MIT-licensed, and you can grab the base model on Hugging Face. If you prefer hosted, keep an eye on our inference‚Äîwe‚Äôre working to get V3.1 live so you can benchmark without burning your weekend assembling a serving stack.

Hugging Face: [https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base)

Quick personal note: I‚Äôm seeing a lot of small, pragmatic improvements add up here. If you‚Äôre building agents, the hybrid mode plus tighter tool integration is a gift. DeepSeek V3.1 is going to be deployed to W&B Inference service soon! Take a look here to see when it's ready [wandb.me/inference](http://wandb.me/inference) 

ByteDance Seed-OSS 36B: Apache-2.0, 512K context, and a ‚Äúthinking budget‚Äù knob ([X](https://x.com/yshan783399/status/1958225093915779256), [HF](https://huggingface.co/collections/ByteDance-Seed/seed-oss-68a609f4201e788db05b5dcd), [Github](https://github.com/ByteDance-Seed/seed-oss))

I didn‚Äôt see much chatter about this one, which is a shame because this seems like a serious release. ByteDance‚Äôs Seed team open-sourced a trio of 36B dense models‚Äîtwo Base variants (with and without synthetic data) and an Instruct model‚Äîunder Apache-2.0, trained on 12T tokens and built for long-context and agentic use. The context window is a native half-million tokens, and they include a ‚Äúthinking budget‚Äù control you can set in 512-token increments so you can trade depth for speed.

They report strong general performance, long-context RULER scores, and solid code/math numbers for a sub-40B model, with the Instruct variant posting very competitive MMLU/MMLU-Pro and LiveCodeBench results. The architecture is a straightforward dense stack (not MoE), and the models ship with Transformers/vLLM support and 4/8-bit quantization ready to go. If you‚Äôve been hunting for a commercial-friendly, long-context 30-something‚ÄëB with an explicit reasoning-control dial, this should be on your shortlist.

A neat detail for the training nerds: two Base releases‚Äîone trained with synthetic data, one without‚Äîmake for a rare apples-to-apples study in how synthetic data shapes base capability. Also worth noting: they previously shipped a Seed-Prover specialized for Lean; it looks like the team is interested in tight domain models and generalists.

NVIDIA Nemotron Nano 9B V2: mixed architecture, open data, and long-context throughput ([X](https://x.com/llm_wizard/status/1957516422520996020), [Blog](https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/), [HF](https://huggingface.co/collections/nvidia/nvidia-nemotron-689f6d6e6ead8e77dd641615), [Dataset](https://huggingface.co/collections/nvidia/nemotron-pre-training-dataset-689d9de36f84279d83786b35), [Try It](https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2)) 

NVIDIA shipped a fully open release of Nemotron Nano 9B V2‚Äîbase, base-before-alignment/pruning, and a realigned reasoning model‚Äîand, crucially, they published most of the pretraining dataset details (~6.6T tokens across premium web, math, code, and SFT). That level of data transparency is rare and makes this a great base for fine-tuners who want reproducibility.

Under the hood, this is a mixed Mamba+Transformer architecture. NVIDIA is claiming up to 6x higher throughput versus a pure-Transformer peer (they compare to Qwen3-8B) and specifically highlight that they pruned a 12B down to 9B while preserving quality. They also note a single A10 can handle 128K context after compression and distillation passes, which is the kind of practical systems work that matters when you‚Äôre running fleets.

A couple of caveats. The license is NVIDIA Open Model License‚Äînot Apache-2.0‚Äîso read it; it includes restrictions around illegal surveillance and safety bypasses and has revocation clauses. Personally, I appreciate the data openness and the long-context engineering; as always, just make sure the license fits your use case.

If you‚Äôre into longer-context math/coding with small models, the numbers (AIME‚Äô25, RULER-128K, GPQA) are impressive for 9B. And if you fine-tune: the availability of both pruned and pre-pruned bases plus the dataset recipe is a rare treat.

Cohere‚Äôs Command-A Reasoning: dense, multilingual, and research-only licensing ([X](https://x.com/cohere/status/1958542682890047511), [Blog](https://cohere.com/blog/command-a-reasoning), [HF](https://huggingface.co/CohereLabs/command-a-reasoning-08-2025?ref=cohere-ai.ghost.io))

Cohere Dropped a new reasoning model focused on enterprise deployment patterns. It‚Äôs dense 111B model, supports a 256K context, and includes very strong multilingual coverage (23 languages is what they called out). What caught my eye: on the BFCL (Berkeley Function-Calling Leaderboard) they show 70%‚Äîabove DeepSeek R1‚Äôs ~63% and GPT-OSS‚Äôs ~61%‚Äîand they plot the now-familiar test-time compute curves where more thinking tokens yield higher scores.

This release uses Cohere‚Äôs non-commercial research license; if you want commercial usage you‚Äôll need to go through them. That said, for teams who need privately deployable, on-prem reasoning and can work under a research license for prototyping, it‚Äôs a serious option. A meta observation from the show: there‚Äôs accumulating evidence that more active parameters help multi-hop tool-use chains compared to very sparse MoE at similar effective capacity. This model nudges in that direction.

Desktop agents leap: ComputerRL hits 48% on OSWorld ([Paper](https://arxiv.org/abs/2508.14040))

A new framework dubbed ComputerRL from [Z.ai](http://Z.ai) and folks at Tsingua Uni, unified API calls with GUI actions and scaled RL across fleets of virtual desktops, posting a 48.1% success rate on OSWorld versus ~12% for earlier open models. The training system spins up thousands of qemu-in-docker VMs via gRPC; the learning loop alternates RL with supervised fine-tuning and uses a clean step-level binary reward to simplify credit assignment. If you care about practical desktop automation across Ubuntu/Windows/macOS, this is a big jump.

IBM + NASA‚Äôs Surya: open model for solar weather ([HF](https://huggingface.co/nasa-ibm-ai4science/Surya-1.0))

Scientists get some love: IBM and NASA open-sourced Surya, a transformer trained on nine years of multi-instrument observations (nearly 200 TB) to forecast solar dynamics and space weather‚Äîthe stuff that can knock satellites and power grids sideways. It‚Äôs on Hugging Face, it‚Äôs actually runnable, and it‚Äôs a fantastic example of open models delivering real-world scientific utility.

Smaller but notable: InternLM and OpenCUA, plus Intel‚Äôs quants

Two quick flags from the ‚Äúworth your time‚Äù pile. InternLM shipped [S1 Mini](https://x.com/intern_lm/status/1958479430361461008), an 8B vision+language model (ViT on top) that‚Äôs multimodal and lightweight; if you need on-device omni-ish behavior on a laptop or tablet, give it a look. And [OpenCUA](https://x.com/xywang626/status/1956400403911962757) 32B (Qwen-based) is a specialized computer-usage agent with strong scores; if you‚Äôre building automations that need native OS control, it‚Äôs worth benchmarking.

Also, if you‚Äôre running 4-bit: the Intel quantization work is excellent right now. Their 4-bit quants have been extremely high precision in my testing, especially for large coders and reasoners like DeepSeek V3.1. It‚Äôs an easy win if you‚Äôre trying to squeeze a 30B+ onto a workstation without hemorrhaging quality.

Big-co updates and platform shifts

Sonic appears in Cursor and Cline

If you open Cursor or fire up Cline, you may see a new ‚ÄúSonic‚Äù model toggle. It‚Äôs labeled as a reasoning model and, when you poke the function-calling internals, the call paths include ‚Äúxai/‚Ä¶‚Äù strings. Folks report it‚Äôs fast and solid for coding. No official docs yet, but I‚Äôd be surprised if this isn‚Äôt Grok Code in pre-release clothes.

Agents.md: one file to rule your agents

Agentic dev stacks have multiplied config files like gremlins: Cursor‚Äôs rules.json, Windsurf‚Äôs prompts, MCP server manifests, tool schemas, install scripts‚Ä¶ and every tool wants a different filename and format. OpenAI‚Äôs [Agents.md](Agents.md) is a strong attempt at standardization. It‚Äôs just Markdown at repo root that says, ‚Äúhere‚Äôs how to set up, build, test, and run this project,‚Äù plus any agent-specific caveats. Tools then auto-detect and follow your instructions instead of guessing.

It‚Äôs already supported by OpenAI Codex, Amp, Jules, Cursor, RooCode, and more, with tens of thousands of public repos adopting the pattern. In monorepos, the nearest [Agents.md](Agents.md) wins, so you can override at the package level. And human chat instructions still override the file‚Äôs guidance, which is the right default.

GPT‚Äë5 context truncation in the web UI ([reports](https://x.com/pvncher/status/1958289479283650741))

There‚Äôs been a wave of reports that GPT‚Äë5 in the web UI is truncating long prompts even when you‚Äôre under the documented context limit. The folks at Repo Prompt reproduced this multiple times and got confirmation from OpenAI that it‚Äôs a bug (not a deliberate nerf). If you saw GPT‚Äë5 suddenly forget the bottom half of your carefully structured system prompt in the web app, this likely explains it. The API doesn‚Äôt seem affected. Fingers crossed for a quick fix‚ÄîGPT‚Äë5 is still the best model I‚Äôve used for 300k‚Äëtoken ‚Äúread the whole repo and propose a plan‚Äù tasks.

Image and 3D: Nano Banana and Qwen‚Äôs open image editor

Nano Banana: 3D-consistent scene editing from thin air

A stealth model nicknamed ‚ÄúNano Banana‚Äù surfaced in a web demo and started doing the kind of edits you‚Äôd normally expect from a 3D suite with a modeler at the controls. Take two photos‚Äîyour living room and a product shot‚Äîand it composites the object into the scene with shockingly consistent lighting and geometry. Ask for a 3D mesh ‚Äúfive inches off the skin,‚Äù and the mesh really does offset. Ask for a new camera angle on a single still, and it renders the scene from above with plausible structure. People have been calling it a game-changer and, for once, it doesn‚Äôt feel like hyperbole.

There‚Äôs a strong whiff of 3D world modeling under the hood‚Äîsome volumetric representation or neural field that enables true view synthesis‚Äîand Logan Kilpatrick posted a banana emoji that set speculation on fire. We‚Äôll see where it lands and under what license, but for now the demo has been doing the rounds and it is‚Ä¶ wow.

If you‚Äôre wondering where to try it: LMarena is the currently only way to give it a try, but it's supossedly dropping soon! 

Qwen Image Edit (20B): fully open and already practical ([X](https://twitter.com/Alibaba_Qwen/status/1957500569029079083), [HF](https://huggingface.co/Qwen/Qwen-Image-Edit))

Qwen shipped a 20B image-editing model layered on their existing vision stack, and it‚Äôs properly open (permissive license) with strong bilingual text editing in Chinese and English. It handles high-level semantic edits (pose adjustments, rotations, style/IP creation) and low-level touch-ups (add/remove/insert). You can swap objects, expand aspect ratios, keep character identity consistent across panels, and do clean style transfer. It runs locally if you‚Äôve got a decent GPU.

What I appreciate here is the precision. Product placement tasks like ‚Äúput this book in this person‚Äôs hand at this angle,‚Äù or ‚Äúmake the shoes match the dress‚Äù come out with the kind of control that used to require hand masking and a dozen passes. And yes, the capybara mascot is back in the release materials, which made my day! üëè

If Nano Banana is the closed-world demo of what‚Äôs ‚Äúbeyond SOTA,‚Äù Qwen Image Edit is the open tool you can actually ship with today.

This week‚Äôs buzz from Weights & Biases

Two quick updates from our side. First, we‚Äôre working to bring DeepSeek V3.1 to our inference as fast as we can so you can run serious benchmarks without fussing over serving stacks. Keep an eye on our channels; we‚Äôll shout when it‚Äôs live and we‚Äôll have some credits for early feedback.

Second, our cofounder Chris Van Pelt released Catnip, a containerized multi‚Äëagent coding workspace that runs multiple Claude Code sessions (or other agents) in isolated sandboxes, each with its own context and notification stream. If you‚Äôve been juggling parallel coding agents that step on each other‚Äôs toes, this is catnip indeed.

Catnip Github: [https://github.com/wandb/catnip](https://github.com/wandb/catnip)

Closing thoughts

A year ago, ‚Äúthinking tokens‚Äù weren't even a curiosity; We only got the first whiff of "reasoning" back in September, and now we‚Äôre watching hybrid models that do more with less thinking, tool calls woven inside the reasoning loop, and long-context training regimes scaled up by an order of magnitude. The agent stack is maturing fast‚Äîdesktop RL is finally clearing real tasks; editor ecosystems are converging on a single config file; and even the stealth drops are clearly building toward world-model‚Äëaware editing and control.

If you only try two things this week: run DeepSeek V3.1 in both modes (planning with thinking on, execution with thinking off) and throw a complex multi-step tool workflow at it; then take Qwen Image Edit for a spin on a real product-placement or character-consistency task. You‚Äôll feel the future in your hands.

I‚Äôm off to the desert next week for a bit (no internet where I‚Äôm going), but Wolfram and the crew will keep the ship on course. If you‚Äôre at Burning Man, DM me‚Äîwould love to say hi out there. As always, thank you for tuning in and nerding out with us every week.

‚Äî Alex

TL;DR and show notes

ThursdAI - Aug 21, 2025 - TL;DR

ThursdAI - Aug 21, 2024 - TL;DR

TL;DR of all topics covered:

* **Hosts and Guests**

* [**Alex Volkov**](http://x.com/@altryne) - AI Evangelist & Weights & Biases

* **Co Hosts** - [@WolframRvnwlf](http://x.com/@WolframRvnwlf), [@yampeleg](http://x.com/@yampeleg), [@nisten](http://x.com/@nisten), [@ldjconfirmed](http://x.com/@ldjconfirmed)

* **Open Source LLMs // Papers**

* **ByteDance Seed-OSS** - 36B open-source LLM family ([X](https://x.com/gm8xx8/status/1958258474154143923), [HF](https://huggingface.co/collections/ByteDance-Seed/seed-oss-68a609f4201e788db05b5dcd), [GitHub](https://github.com/ByteDance-Seed/seed-oss))

* **DeepSeek V3.1** - Updated Hybrid model ([HF](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base))

* **Cohere CMD-a Reasoning - **([X](https://x.com/cohere/status/1958542682890047511), [Blog](https://cohere.com/blog/command-a-reasoning), [HF](https://huggingface.co/CohereLabs/command-a-reasoning-08-2025?ref=cohere-ai.ghost.io))

* **Zai/Tsinghua ComputerRL** - Framework for desktop agents ([X](https://x.com/Zai_org/status/1958175133706891613), [Paper](https://arxiv.org/abs/2508.14040), [Benchmark](https://os-world.github.io))

* **IBM & NASA Surya** - Solar weather prediction ([HF](https://huggingface.co/nasa-ibm-ai4science/Surya-1.0))

* **NVIDIA Nemotron Nano 9B V2 - **([X](https://x.com/llm_wizard/status/1957516422520996020), [Blog](https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/), [HF](https://huggingface.co/collections/nvidia/nvidia-nemotron-689f6d6e6ead8e77dd641615), [Dataset](https://huggingface.co/collections/nvidia/nemotron-pre-training-dataset-689d9de36f84279d83786b35), [Try It](https://build.nvidia.com/nvidia/nvidia-nemotron-nano-9b-v2)) 

* **Alibaba Quark Med**

* **Big CO LLMs + APIs**

* **Sonic Stealth Model** - Likely Grok Code

* **OpenAI **[**agents.md**](agents.md) - Unified agent files ([agents.md](https://agents.md))

* **GPT-5 Nerf**

* **AI Art & Diffusion & 3D**

* **Nano Banana** - Image model (rumored Google)

* **Qwen-Image-Edit** - 20B Image editing ([X](https://x.com/Alibaba_Qwen/status/1957500569029079083), [HF](https://huggingface.co/Qwen/Qwen-Image-Edit))

* **This weeks Buzz**

* **Catnip** - Containerized AI agent runner ([GitHub](https://github.com/wandb/catnip)) 

Thank you for subscribing. [Leave a comment](https://sub.thursdai.news/p/thursdai-aug-21-deepseek-v31s-hybrid/comments?utm_medium=podcast&utm_campaign=CTA_5) or [share this episode](https://sub.thursdai.news/p/thursdai-aug-21-deepseek-v31s-hybrid?utm_source=substack&utm_medium=podcast&utm_content=share&action=share&token=eyJ1c2VyX2lkIjoxNTIyMTYxMTAsInBvc3RfaWQiOjE3MTU5MTYyNSwiaWF0IjoxNzY1MjQyMjg2LCJleHAiOjE3Njc4MzQyODYsImlzcyI6InB1Yi0xODAxMjI4Iiwic3ViIjoicG9zdC1yZWFjdGlvbiJ9.GzhggMq7eqrzUXFXtvXqPZNhj2eE7s-nKFKsMRdX-58&utm_campaign=CTA_5).
